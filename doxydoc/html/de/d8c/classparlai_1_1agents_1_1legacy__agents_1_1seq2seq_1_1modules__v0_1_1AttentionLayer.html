<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer Class Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d5/d22/namespaceparlai.html">parlai</a></li><li class="navelem"><a class="el" href="../../dd/d61/namespaceparlai_1_1agents.html">agents</a></li><li class="navelem"><a class="el" href="../../d1/dae/namespaceparlai_1_1agents_1_1legacy__agents.html">legacy_agents</a></li><li class="navelem"><a class="el" href="../../d9/dd0/namespaceparlai_1_1agents_1_1legacy__agents_1_1seq2seq.html">seq2seq</a></li><li class="navelem"><a class="el" href="../../d0/dcb/namespaceparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0.html">modules_v0</a></li><li class="navelem"><a class="el" href="../../de/d8c/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer.html">AttentionLayer</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="../../df/d7d/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer:</div>
<div class="dyncontent">
<div class="center"><img src="../../df/d1a/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer__inherit__graph.png" border="0" usemap="#parlai_8agents_8legacy__agents_8seq2seq_8modules__v0_8AttentionLayer_inherit__map" alt="Inheritance graph"/></div>
<map name="parlai_8agents_8legacy__agents_8seq2seq_8modules__v0_8AttentionLayer_inherit__map" id="parlai_8agents_8legacy__agents_8seq2seq_8modules__v0_8AttentionLayer_inherit__map">
</map>
<center><span class="legend">[<a href="../../graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer:</div>
<div class="dyncontent">
<div class="center"><img src="../../dd/df8/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer__coll__graph.png" border="0" usemap="#parlai_8agents_8legacy__agents_8seq2seq_8modules__v0_8AttentionLayer_coll__map" alt="Collaboration graph"/></div>
<map name="parlai_8agents_8legacy__agents_8seq2seq_8modules__v0_8AttentionLayer_coll__map" id="parlai_8agents_8legacy__agents_8seq2seq_8modules__v0_8AttentionLayer_coll__map">
</map>
<center><span class="legend">[<a href="../../graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a454f7b72defa9f2dc7e8efec1ab336eb"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../de/d8c/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer.html#a454f7b72defa9f2dc7e8efec1ab336eb">__init__</a> (self, attn_type, hidden_size, emb_size, bidirectional=False, attn_length=-1, attn_time='pre')</td></tr>
<tr class="separator:a454f7b72defa9f2dc7e8efec1ab336eb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6b5727eb84f3cab62adc1c647e23b98a"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../de/d8c/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer.html#a6b5727eb84f3cab62adc1c647e23b98a">forward</a> (self, xes, hidden, enc_out, attn_mask=None)</td></tr>
<tr class="separator:a6b5727eb84f3cab62adc1c647e23b98a"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a0faa53163c14800b0deda68be978cd76"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../de/d8c/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer.html#a0faa53163c14800b0deda68be978cd76">attention</a></td></tr>
<tr class="separator:a0faa53163c14800b0deda68be978cd76"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acd4f27aecfab6e05a026061bdabc42aa"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../de/d8c/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer.html#acd4f27aecfab6e05a026061bdabc42aa">attn_combine</a></td></tr>
<tr class="separator:acd4f27aecfab6e05a026061bdabc42aa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae7aed4fe106be5b39f7cc9027c5673f7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../de/d8c/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer.html#ae7aed4fe106be5b39f7cc9027c5673f7">max_length</a></td></tr>
<tr class="separator:ae7aed4fe106be5b39f7cc9027c5673f7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aedd5ce11eba4a46b40bc61a8bdad7e3d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../de/d8c/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer.html#aedd5ce11eba4a46b40bc61a8bdad7e3d">attn</a></td></tr>
<tr class="separator:aedd5ce11eba4a46b40bc61a8bdad7e3d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4659f062cd7e92b2015f2cdf8558449d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../de/d8c/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer.html#a4659f062cd7e92b2015f2cdf8558449d">attn_v</a></td></tr>
<tr class="separator:a4659f062cd7e92b2015f2cdf8558449d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock">
<p class="definition">Definition at line <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html#l00724">724</a> of file <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a454f7b72defa9f2dc7e8efec1ab336eb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a454f7b72defa9f2dc7e8efec1ab336eb">&#9670;&nbsp;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>emb_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>bidirectional</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_length</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_time</em> = <code>'pre'</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html#l00733">733</a> of file <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a>.</p>
<div class="fragment"><div class="line"><a name="l00733"></a><span class="lineno">  733</span>&#160;    ):</div><div class="line"><a name="l00734"></a><span class="lineno">  734</span>&#160;        super().__init__()</div><div class="line"><a name="l00735"></a><span class="lineno">  735</span>&#160;        self.attention = attn_type</div><div class="line"><a name="l00736"></a><span class="lineno">  736</span>&#160;</div><div class="line"><a name="l00737"></a><span class="lineno">  737</span>&#160;        <span class="keywordflow">if</span> self.attention != <span class="stringliteral">&#39;none&#39;</span>:</div><div class="line"><a name="l00738"></a><span class="lineno">  738</span>&#160;            hsz = hidden_size</div><div class="line"><a name="l00739"></a><span class="lineno">  739</span>&#160;            hszXdirs = hsz * (2 <span class="keywordflow">if</span> bidirectional <span class="keywordflow">else</span> 1)</div><div class="line"><a name="l00740"></a><span class="lineno">  740</span>&#160;            <span class="keywordflow">if</span> attn_time == <span class="stringliteral">&#39;pre&#39;</span>:</div><div class="line"><a name="l00741"></a><span class="lineno">  741</span>&#160;                <span class="comment"># attention happens on the input embeddings</span></div><div class="line"><a name="l00742"></a><span class="lineno">  742</span>&#160;                input_dim = emb_size</div><div class="line"><a name="l00743"></a><span class="lineno">  743</span>&#160;            <span class="keywordflow">elif</span> attn_time == <span class="stringliteral">&#39;post&#39;</span>:</div><div class="line"><a name="l00744"></a><span class="lineno">  744</span>&#160;                <span class="comment"># attention happens on the output of the rnn</span></div><div class="line"><a name="l00745"></a><span class="lineno">  745</span>&#160;                input_dim = hsz</div><div class="line"><a name="l00746"></a><span class="lineno">  746</span>&#160;            <span class="keywordflow">else</span>:</div><div class="line"><a name="l00747"></a><span class="lineno">  747</span>&#160;                <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&#39;unsupported attention time&#39;</span>)</div><div class="line"><a name="l00748"></a><span class="lineno">  748</span>&#160;            self.attn_combine = nn.Linear(hszXdirs + input_dim, input_dim, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00749"></a><span class="lineno">  749</span>&#160;</div><div class="line"><a name="l00750"></a><span class="lineno">  750</span>&#160;            <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;local&#39;</span>:</div><div class="line"><a name="l00751"></a><span class="lineno">  751</span>&#160;                <span class="comment"># local attention over fixed set of output states</span></div><div class="line"><a name="l00752"></a><span class="lineno">  752</span>&#160;                <span class="keywordflow">if</span> attn_length &lt; 0:</div><div class="line"><a name="l00753"></a><span class="lineno">  753</span>&#160;                    <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&#39;Set attention length to &gt; 0.&#39;</span>)</div><div class="line"><a name="l00754"></a><span class="lineno">  754</span>&#160;                self.max_length = attn_length</div><div class="line"><a name="l00755"></a><span class="lineno">  755</span>&#160;                <span class="comment"># combines input and previous hidden output layer</span></div><div class="line"><a name="l00756"></a><span class="lineno">  756</span>&#160;                self.attn = nn.Linear(hsz + input_dim, attn_length, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00757"></a><span class="lineno">  757</span>&#160;                <span class="comment"># combines attention weights with encoder outputs</span></div><div class="line"><a name="l00758"></a><span class="lineno">  758</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;concat&#39;</span>:</div><div class="line"><a name="l00759"></a><span class="lineno">  759</span>&#160;                self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00760"></a><span class="lineno">  760</span>&#160;                self.attn_v = nn.Linear(hsz, 1, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00761"></a><span class="lineno">  761</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;general&#39;</span>:</div><div class="line"><a name="l00762"></a><span class="lineno">  762</span>&#160;                <span class="comment"># equivalent to dot if attn is identity</span></div><div class="line"><a name="l00763"></a><span class="lineno">  763</span>&#160;                self.attn = nn.Linear(hsz, hszXdirs, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00764"></a><span class="lineno">  764</span>&#160;</div></div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a6b5727eb84f3cab62adc1c647e23b98a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6b5727eb84f3cab62adc1c647e23b98a">&#9670;&nbsp;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer.forward </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>xes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hidden</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>enc_out</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_mask</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html#l00765">765</a> of file <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a>.</p>
<div class="fragment"><div class="line"><a name="l00765"></a><span class="lineno">  765</span>&#160;    <span class="keyword">def </span>forward(self, xes, hidden, enc_out, attn_mask=None):</div><div class="line"><a name="l00766"></a><span class="lineno">  766</span>&#160;        <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;none&#39;</span>:</div><div class="line"><a name="l00767"></a><span class="lineno">  767</span>&#160;            <span class="keywordflow">return</span> xes</div><div class="line"><a name="l00768"></a><span class="lineno">  768</span>&#160;</div><div class="line"><a name="l00769"></a><span class="lineno">  769</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="../../d4/da3/namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf.html#ad5dfae268e23f506da084a9efb72f619">type</a>(hidden) == tuple:</div><div class="line"><a name="l00770"></a><span class="lineno">  770</span>&#160;            <span class="comment"># for lstms use the &quot;hidden&quot; state not the cell state</span></div><div class="line"><a name="l00771"></a><span class="lineno">  771</span>&#160;            hidden = hidden[0]</div><div class="line"><a name="l00772"></a><span class="lineno">  772</span>&#160;        last_hidden = hidden[-1]  <span class="comment"># select hidden state from last RNN layer</span></div><div class="line"><a name="l00773"></a><span class="lineno">  773</span>&#160;</div><div class="line"><a name="l00774"></a><span class="lineno">  774</span>&#160;        <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;local&#39;</span>:</div><div class="line"><a name="l00775"></a><span class="lineno">  775</span>&#160;            <span class="keywordflow">if</span> enc_out.size(1) &gt; self.max_length:</div><div class="line"><a name="l00776"></a><span class="lineno">  776</span>&#160;                offset = enc_out.size(1) - self.max_length</div><div class="line"><a name="l00777"></a><span class="lineno">  777</span>&#160;                enc_out = enc_out.narrow(1, offset, self.max_length)</div><div class="line"><a name="l00778"></a><span class="lineno">  778</span>&#160;            h_merged = torch.cat((xes.squeeze(1), last_hidden), 1)</div><div class="line"><a name="l00779"></a><span class="lineno">  779</span>&#160;            attn_weights = F.softmax(self.attn(h_merged), dim=1)</div><div class="line"><a name="l00780"></a><span class="lineno">  780</span>&#160;            <span class="keywordflow">if</span> attn_weights.size(1) &gt; enc_out.size(1):</div><div class="line"><a name="l00781"></a><span class="lineno">  781</span>&#160;                attn_weights = attn_weights.narrow(1, 0, enc_out.size(1))</div><div class="line"><a name="l00782"></a><span class="lineno">  782</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00783"></a><span class="lineno">  783</span>&#160;            hid = last_hidden.unsqueeze(1)</div><div class="line"><a name="l00784"></a><span class="lineno">  784</span>&#160;            <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;concat&#39;</span>:</div><div class="line"><a name="l00785"></a><span class="lineno">  785</span>&#160;                hid = hid.expand(</div><div class="line"><a name="l00786"></a><span class="lineno">  786</span>&#160;                    last_hidden.size(0), enc_out.size(1), last_hidden.size(1)</div><div class="line"><a name="l00787"></a><span class="lineno">  787</span>&#160;                )</div><div class="line"><a name="l00788"></a><span class="lineno">  788</span>&#160;                h_merged = torch.cat((enc_out, hid), 2)</div><div class="line"><a name="l00789"></a><span class="lineno">  789</span>&#160;                active = F.tanh(self.attn(h_merged))</div><div class="line"><a name="l00790"></a><span class="lineno">  790</span>&#160;                attn_w_premask = self.attn_v(active).squeeze(2)</div><div class="line"><a name="l00791"></a><span class="lineno">  791</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;dot&#39;</span>:</div><div class="line"><a name="l00792"></a><span class="lineno">  792</span>&#160;                <span class="keywordflow">if</span> hid.size(2) != enc_out.size(2):</div><div class="line"><a name="l00793"></a><span class="lineno">  793</span>&#160;                    <span class="comment"># enc_out has two directions, so double hid</span></div><div class="line"><a name="l00794"></a><span class="lineno">  794</span>&#160;                    hid = torch.cat([hid, hid], 2)</div><div class="line"><a name="l00795"></a><span class="lineno">  795</span>&#160;                attn_w_premask = torch.bmm(hid, enc_out.transpose(1, 2)).squeeze(1)</div><div class="line"><a name="l00796"></a><span class="lineno">  796</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;general&#39;</span>:</div><div class="line"><a name="l00797"></a><span class="lineno">  797</span>&#160;                hid = self.attn(hid)</div><div class="line"><a name="l00798"></a><span class="lineno">  798</span>&#160;                attn_w_premask = torch.bmm(hid, enc_out.transpose(1, 2)).squeeze(1)</div><div class="line"><a name="l00799"></a><span class="lineno">  799</span>&#160;            <span class="comment"># calculate activation scores</span></div><div class="line"><a name="l00800"></a><span class="lineno">  800</span>&#160;            <span class="keywordflow">if</span> attn_mask <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00801"></a><span class="lineno">  801</span>&#160;                <span class="comment"># remove activation from NULL symbols</span></div><div class="line"><a name="l00802"></a><span class="lineno">  802</span>&#160;                attn_w_premask -= (1 - attn_mask) * 1e20</div><div class="line"><a name="l00803"></a><span class="lineno">  803</span>&#160;            attn_weights = F.softmax(attn_w_premask, dim=1)</div><div class="line"><a name="l00804"></a><span class="lineno">  804</span>&#160;</div><div class="line"><a name="l00805"></a><span class="lineno">  805</span>&#160;        attn_applied = torch.bmm(attn_weights.unsqueeze(1), enc_out)</div><div class="line"><a name="l00806"></a><span class="lineno">  806</span>&#160;        merged = torch.cat((xes.squeeze(1), attn_applied.squeeze(1)), 1)</div><div class="line"><a name="l00807"></a><span class="lineno">  807</span>&#160;        output = F.tanh(self.attn_combine(merged).unsqueeze(1))</div><div class="line"><a name="l00808"></a><span class="lineno">  808</span>&#160;        <span class="keywordflow">return</span> output</div><div class="line"><a name="l00809"></a><span class="lineno">  809</span>&#160;<div class="ttc" id="namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf_html_ad5dfae268e23f506da084a9efb72f619"><div class="ttname"><a href="../../d4/da3/namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf.html#ad5dfae268e23f506da084a9efb72f619">parlai.agents.tfidf_retriever.build_tfidf.type</a></div><div class="ttdeci">type</div><div class="ttdef"><b>Definition:</b> <a href="../../df/de8/build__tfidf_8py_source.html#l00339">build_tfidf.py:339</a></div></div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a0faa53163c14800b0deda68be978cd76"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0faa53163c14800b0deda68be978cd76">&#9670;&nbsp;</a></span>attention</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer.attention</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html#l00735">735</a> of file <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a>.</p>

</div>
</div>
<a id="aedd5ce11eba4a46b40bc61a8bdad7e3d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aedd5ce11eba4a46b40bc61a8bdad7e3d">&#9670;&nbsp;</a></span>attn</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer.attn</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html#l00756">756</a> of file <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a>.</p>

</div>
</div>
<a id="acd4f27aecfab6e05a026061bdabc42aa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acd4f27aecfab6e05a026061bdabc42aa">&#9670;&nbsp;</a></span>attn_combine</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer.attn_combine</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html#l00748">748</a> of file <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a>.</p>

</div>
</div>
<a id="a4659f062cd7e92b2015f2cdf8558449d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4659f062cd7e92b2015f2cdf8558449d">&#9670;&nbsp;</a></span>attn_v</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer.attn_v</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html#l00760">760</a> of file <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a>.</p>

</div>
</div>
<a id="ae7aed4fe106be5b39f7cc9027c5673f7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae7aed4fe106be5b39f7cc9027c5673f7">&#9670;&nbsp;</a></span>max_length</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v0.AttentionLayer.max_length</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html#l00754">754</a> of file <a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>parlai/agents/legacy_agents/seq2seq/<a class="el" href="../../d3/da6/seq2seq_2modules__v0_8py_source.html">modules_v0.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
