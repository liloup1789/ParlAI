<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer Class Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d5/d22/namespaceparlai.html">parlai</a></li><li class="navelem"><a class="el" href="../../dd/d61/namespaceparlai_1_1agents.html">agents</a></li><li class="navelem"><a class="el" href="../../d1/dae/namespaceparlai_1_1agents_1_1legacy__agents.html">legacy_agents</a></li><li class="navelem"><a class="el" href="../../d9/dd0/namespaceparlai_1_1agents_1_1legacy__agents_1_1seq2seq.html">seq2seq</a></li><li class="navelem"><a class="el" href="../../d0/deb/namespaceparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1.html">modules_v1</a></li><li class="navelem"><a class="el" href="../../df/d83/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer.html">AttentionLayer</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="../../d3/db7/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer:</div>
<div class="dyncontent">
<div class="center"><img src="../../df/de7/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer__inherit__graph.png" border="0" usemap="#parlai_8agents_8legacy__agents_8seq2seq_8modules__v1_8AttentionLayer_inherit__map" alt="Inheritance graph"/></div>
<center><span class="legend">[<a href="../../graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer:</div>
<div class="dyncontent">
<div class="center"><img src="../../d6/d91/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer__coll__graph.png" border="0" usemap="#parlai_8agents_8legacy__agents_8seq2seq_8modules__v1_8AttentionLayer_coll__map" alt="Collaboration graph"/></div>
<center><span class="legend">[<a href="../../graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aed77232760e470a547c8c4ec68e529c7"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../df/d83/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer.html#aed77232760e470a547c8c4ec68e529c7">__init__</a> (self, attn_type, hiddensize, embeddingsize, bidirectional=False, attn_length=-1, attn_time='pre')</td></tr>
<tr class="separator:aed77232760e470a547c8c4ec68e529c7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad6041bd52e148a4dfe894b71f79937c0"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../df/d83/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer.html#ad6041bd52e148a4dfe894b71f79937c0">forward</a> (self, xes, hidden, attn_params)</td></tr>
<tr class="separator:ad6041bd52e148a4dfe894b71f79937c0"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a6c95b5fa079109f57c305954f1f1465a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../df/d83/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer.html#a6c95b5fa079109f57c305954f1f1465a">attention</a></td></tr>
<tr class="separator:a6c95b5fa079109f57c305954f1f1465a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ead8a9fdb5a6ad88b1613961a7eff99"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../df/d83/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer.html#a1ead8a9fdb5a6ad88b1613961a7eff99">attn_combine</a></td></tr>
<tr class="separator:a1ead8a9fdb5a6ad88b1613961a7eff99"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afb020d5934ba3cee17968fe893417ce8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../df/d83/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer.html#afb020d5934ba3cee17968fe893417ce8">max_length</a></td></tr>
<tr class="separator:afb020d5934ba3cee17968fe893417ce8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa9eaecc56fcf6d7816fe23d0c27dd481"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../df/d83/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer.html#aa9eaecc56fcf6d7816fe23d0c27dd481">attn</a></td></tr>
<tr class="separator:aa9eaecc56fcf6d7816fe23d0c27dd481"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a034298f5224687c4fe53f401e5a13ec2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../df/d83/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v1_1_1AttentionLayer.html#a034298f5224687c4fe53f401e5a13ec2">attn_v</a></td></tr>
<tr class="separator:a034298f5224687c4fe53f401e5a13ec2"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Computes attention between hidden and encoder states.

See arxiv.org/abs/1508.04025 for more info on each attention type.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d8/d53/modules__v1_8py_source.html#l00722">722</a> of file <a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="aed77232760e470a547c8c4ec68e529c7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed77232760e470a547c8c4ec68e529c7">&#9670;&nbsp;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hiddensize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>embeddingsize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>bidirectional</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_length</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_time</em> = <code>'pre'</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Initialize attention layer.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d8/d53/modules__v1_8py_source.html#l00737">737</a> of file <a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a>.</p>
<div class="fragment"><div class="line"><a name="l00737"></a><span class="lineno">  737</span>&#160;    ):</div><div class="line"><a name="l00738"></a><span class="lineno">  738</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00739"></a><span class="lineno">  739</span>&#160;<span class="stringliteral">        Initialize attention layer.</span></div><div class="line"><a name="l00740"></a><span class="lineno">  740</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00741"></a><span class="lineno">  741</span>&#160;        super().__init__()</div><div class="line"><a name="l00742"></a><span class="lineno">  742</span>&#160;        self.attention = attn_type</div><div class="line"><a name="l00743"></a><span class="lineno">  743</span>&#160;</div><div class="line"><a name="l00744"></a><span class="lineno">  744</span>&#160;        <span class="keywordflow">if</span> self.attention != <span class="stringliteral">&#39;none&#39;</span>:</div><div class="line"><a name="l00745"></a><span class="lineno">  745</span>&#160;            hsz = hiddensize</div><div class="line"><a name="l00746"></a><span class="lineno">  746</span>&#160;            hszXdirs = hsz * (2 <span class="keywordflow">if</span> bidirectional <span class="keywordflow">else</span> 1)</div><div class="line"><a name="l00747"></a><span class="lineno">  747</span>&#160;            <span class="keywordflow">if</span> attn_time == <span class="stringliteral">&#39;pre&#39;</span>:</div><div class="line"><a name="l00748"></a><span class="lineno">  748</span>&#160;                <span class="comment"># attention happens on the input embeddings</span></div><div class="line"><a name="l00749"></a><span class="lineno">  749</span>&#160;                input_dim = embeddingsize</div><div class="line"><a name="l00750"></a><span class="lineno">  750</span>&#160;            <span class="keywordflow">elif</span> attn_time == <span class="stringliteral">&#39;post&#39;</span>:</div><div class="line"><a name="l00751"></a><span class="lineno">  751</span>&#160;                <span class="comment"># attention happens on the output of the rnn</span></div><div class="line"><a name="l00752"></a><span class="lineno">  752</span>&#160;                input_dim = hsz</div><div class="line"><a name="l00753"></a><span class="lineno">  753</span>&#160;            <span class="keywordflow">else</span>:</div><div class="line"><a name="l00754"></a><span class="lineno">  754</span>&#160;                <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&#39;unsupported attention time&#39;</span>)</div><div class="line"><a name="l00755"></a><span class="lineno">  755</span>&#160;</div><div class="line"><a name="l00756"></a><span class="lineno">  756</span>&#160;            <span class="comment"># linear layer for combining applied attention weights with input</span></div><div class="line"><a name="l00757"></a><span class="lineno">  757</span>&#160;            self.attn_combine = nn.Linear(hszXdirs + input_dim, input_dim, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00758"></a><span class="lineno">  758</span>&#160;</div><div class="line"><a name="l00759"></a><span class="lineno">  759</span>&#160;            <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;local&#39;</span>:</div><div class="line"><a name="l00760"></a><span class="lineno">  760</span>&#160;                <span class="comment"># local attention over fixed set of output states</span></div><div class="line"><a name="l00761"></a><span class="lineno">  761</span>&#160;                <span class="keywordflow">if</span> attn_length &lt; 0:</div><div class="line"><a name="l00762"></a><span class="lineno">  762</span>&#160;                    <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&#39;Set attention length to &gt; 0.&#39;</span>)</div><div class="line"><a name="l00763"></a><span class="lineno">  763</span>&#160;                self.max_length = attn_length</div><div class="line"><a name="l00764"></a><span class="lineno">  764</span>&#160;                <span class="comment"># combines input and previous hidden output layer</span></div><div class="line"><a name="l00765"></a><span class="lineno">  765</span>&#160;                self.attn = nn.Linear(hsz + input_dim, attn_length, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00766"></a><span class="lineno">  766</span>&#160;                <span class="comment"># combines attention weights with encoder outputs</span></div><div class="line"><a name="l00767"></a><span class="lineno">  767</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;concat&#39;</span>:</div><div class="line"><a name="l00768"></a><span class="lineno">  768</span>&#160;                self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00769"></a><span class="lineno">  769</span>&#160;                self.attn_v = nn.Linear(hsz, 1, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00770"></a><span class="lineno">  770</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;general&#39;</span>:</div><div class="line"><a name="l00771"></a><span class="lineno">  771</span>&#160;                <span class="comment"># equivalent to dot if attn is identity</span></div><div class="line"><a name="l00772"></a><span class="lineno">  772</span>&#160;                self.attn = nn.Linear(hsz, hszXdirs, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00773"></a><span class="lineno">  773</span>&#160;</div></div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="ad6041bd52e148a4dfe894b71f79937c0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad6041bd52e148a4dfe894b71f79937c0">&#9670;&nbsp;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer.forward </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>xes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hidden</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_params</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute attention over attn_params given input and hidden states.

:param xes:         input state. will be combined with applied
            attention.
:param hidden:      hidden state from model. will be used to select
            states to attend to in from the attn_params.
:param attn_params: tuple of encoder output states and a mask showing
            which input indices are nonzero.

:returns: output, attn_weights
  output is a new state of same size as input state `xes`.
  attn_weights are the weights given to each state in the
  encoder outputs.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d8/d53/modules__v1_8py_source.html#l00774">774</a> of file <a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a>.</p>
<div class="fragment"><div class="line"><a name="l00774"></a><span class="lineno">  774</span>&#160;    <span class="keyword">def </span>forward(self, xes, hidden, attn_params):</div><div class="line"><a name="l00775"></a><span class="lineno">  775</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00776"></a><span class="lineno">  776</span>&#160;<span class="stringliteral">        Compute attention over attn_params given input and hidden states.</span></div><div class="line"><a name="l00777"></a><span class="lineno">  777</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00778"></a><span class="lineno">  778</span>&#160;<span class="stringliteral">        :param xes:         input state. will be combined with applied</span></div><div class="line"><a name="l00779"></a><span class="lineno">  779</span>&#160;<span class="stringliteral">                            attention.</span></div><div class="line"><a name="l00780"></a><span class="lineno">  780</span>&#160;<span class="stringliteral">        :param hidden:      hidden state from model. will be used to select</span></div><div class="line"><a name="l00781"></a><span class="lineno">  781</span>&#160;<span class="stringliteral">                            states to attend to in from the attn_params.</span></div><div class="line"><a name="l00782"></a><span class="lineno">  782</span>&#160;<span class="stringliteral">        :param attn_params: tuple of encoder output states and a mask showing</span></div><div class="line"><a name="l00783"></a><span class="lineno">  783</span>&#160;<span class="stringliteral">                            which input indices are nonzero.</span></div><div class="line"><a name="l00784"></a><span class="lineno">  784</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00785"></a><span class="lineno">  785</span>&#160;<span class="stringliteral">        :returns: output, attn_weights</span></div><div class="line"><a name="l00786"></a><span class="lineno">  786</span>&#160;<span class="stringliteral">                  output is a new state of same size as input state `xes`.</span></div><div class="line"><a name="l00787"></a><span class="lineno">  787</span>&#160;<span class="stringliteral">                  attn_weights are the weights given to each state in the</span></div><div class="line"><a name="l00788"></a><span class="lineno">  788</span>&#160;<span class="stringliteral">                  encoder outputs.</span></div><div class="line"><a name="l00789"></a><span class="lineno">  789</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00790"></a><span class="lineno">  790</span>&#160;        <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;none&#39;</span>:</div><div class="line"><a name="l00791"></a><span class="lineno">  791</span>&#160;            <span class="comment"># do nothing, no attention</span></div><div class="line"><a name="l00792"></a><span class="lineno">  792</span>&#160;            <span class="keywordflow">return</span> xes, <span class="keywordtype">None</span></div><div class="line"><a name="l00793"></a><span class="lineno">  793</span>&#160;</div><div class="line"><a name="l00794"></a><span class="lineno">  794</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="../../d4/da3/namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf.html#ad5dfae268e23f506da084a9efb72f619">type</a>(hidden) == tuple:</div><div class="line"><a name="l00795"></a><span class="lineno">  795</span>&#160;            <span class="comment"># for lstms use the &quot;hidden&quot; state not the cell state</span></div><div class="line"><a name="l00796"></a><span class="lineno">  796</span>&#160;            hidden = hidden[0]</div><div class="line"><a name="l00797"></a><span class="lineno">  797</span>&#160;        last_hidden = hidden[-1]  <span class="comment"># select hidden state from last RNN layer</span></div><div class="line"><a name="l00798"></a><span class="lineno">  798</span>&#160;</div><div class="line"><a name="l00799"></a><span class="lineno">  799</span>&#160;        enc_out, attn_mask = attn_params</div><div class="line"><a name="l00800"></a><span class="lineno">  800</span>&#160;        bsz, seqlen, hszXnumdir = enc_out.size()</div><div class="line"><a name="l00801"></a><span class="lineno">  801</span>&#160;        numlayersXnumdir = last_hidden.size(1)</div><div class="line"><a name="l00802"></a><span class="lineno">  802</span>&#160;</div><div class="line"><a name="l00803"></a><span class="lineno">  803</span>&#160;        <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;local&#39;</span>:</div><div class="line"><a name="l00804"></a><span class="lineno">  804</span>&#160;            <span class="comment"># local attention weights aren&#39;t based on encoder states</span></div><div class="line"><a name="l00805"></a><span class="lineno">  805</span>&#160;            h_merged = torch.cat((xes.squeeze(1), last_hidden), 1)</div><div class="line"><a name="l00806"></a><span class="lineno">  806</span>&#160;            attn_weights = F.softmax(self.attn(h_merged), dim=1)</div><div class="line"><a name="l00807"></a><span class="lineno">  807</span>&#160;</div><div class="line"><a name="l00808"></a><span class="lineno">  808</span>&#160;            <span class="comment"># adjust state sizes to the fixed window size</span></div><div class="line"><a name="l00809"></a><span class="lineno">  809</span>&#160;            <span class="keywordflow">if</span> seqlen &gt; self.max_length:</div><div class="line"><a name="l00810"></a><span class="lineno">  810</span>&#160;                offset = seqlen - self.max_length</div><div class="line"><a name="l00811"></a><span class="lineno">  811</span>&#160;                enc_out = enc_out.narrow(1, offset, self.max_length)</div><div class="line"><a name="l00812"></a><span class="lineno">  812</span>&#160;                seqlen = self.max_length</div><div class="line"><a name="l00813"></a><span class="lineno">  813</span>&#160;            <span class="keywordflow">if</span> attn_weights.size(1) &gt; seqlen:</div><div class="line"><a name="l00814"></a><span class="lineno">  814</span>&#160;                attn_weights = attn_weights.narrow(1, 0, seqlen)</div><div class="line"><a name="l00815"></a><span class="lineno">  815</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00816"></a><span class="lineno">  816</span>&#160;            hid = last_hidden.unsqueeze(1)</div><div class="line"><a name="l00817"></a><span class="lineno">  817</span>&#160;            <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;concat&#39;</span>:</div><div class="line"><a name="l00818"></a><span class="lineno">  818</span>&#160;                <span class="comment"># concat hidden state and encoder outputs</span></div><div class="line"><a name="l00819"></a><span class="lineno">  819</span>&#160;                hid = hid.expand(bsz, seqlen, numlayersXnumdir)</div><div class="line"><a name="l00820"></a><span class="lineno">  820</span>&#160;                h_merged = torch.cat((enc_out, hid), 2)</div><div class="line"><a name="l00821"></a><span class="lineno">  821</span>&#160;                <span class="comment"># then do linear combination of them with activation</span></div><div class="line"><a name="l00822"></a><span class="lineno">  822</span>&#160;                active = F.tanh(self.attn(h_merged))</div><div class="line"><a name="l00823"></a><span class="lineno">  823</span>&#160;                attn_w_premask = self.attn_v(active).squeeze(2)</div><div class="line"><a name="l00824"></a><span class="lineno">  824</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;dot&#39;</span>:</div><div class="line"><a name="l00825"></a><span class="lineno">  825</span>&#160;                <span class="comment"># dot product between hidden and encoder outputs</span></div><div class="line"><a name="l00826"></a><span class="lineno">  826</span>&#160;                <span class="keywordflow">if</span> numlayersXnumdir != hszXnumdir:</div><div class="line"><a name="l00827"></a><span class="lineno">  827</span>&#160;                    <span class="comment"># enc_out has two directions, so double hid</span></div><div class="line"><a name="l00828"></a><span class="lineno">  828</span>&#160;                    hid = torch.cat([hid, hid], 2)</div><div class="line"><a name="l00829"></a><span class="lineno">  829</span>&#160;                enc_t = enc_out.transpose(1, 2)</div><div class="line"><a name="l00830"></a><span class="lineno">  830</span>&#160;                attn_w_premask = torch.bmm(hid, enc_t).squeeze(1)</div><div class="line"><a name="l00831"></a><span class="lineno">  831</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;general&#39;</span>:</div><div class="line"><a name="l00832"></a><span class="lineno">  832</span>&#160;                <span class="comment"># before doing dot product, transform hidden state with linear</span></div><div class="line"><a name="l00833"></a><span class="lineno">  833</span>&#160;                <span class="comment"># same as dot if linear is identity</span></div><div class="line"><a name="l00834"></a><span class="lineno">  834</span>&#160;                hid = self.attn(hid)</div><div class="line"><a name="l00835"></a><span class="lineno">  835</span>&#160;                enc_t = enc_out.transpose(1, 2)</div><div class="line"><a name="l00836"></a><span class="lineno">  836</span>&#160;                attn_w_premask = torch.bmm(hid, enc_t).squeeze(1)</div><div class="line"><a name="l00837"></a><span class="lineno">  837</span>&#160;</div><div class="line"><a name="l00838"></a><span class="lineno">  838</span>&#160;            <span class="comment"># calculate activation scores, apply mask if needed</span></div><div class="line"><a name="l00839"></a><span class="lineno">  839</span>&#160;            <span class="keywordflow">if</span> attn_mask <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00840"></a><span class="lineno">  840</span>&#160;                <span class="comment"># remove activation from NULL symbols</span></div><div class="line"><a name="l00841"></a><span class="lineno">  841</span>&#160;                attn_w_premask.masked_fill_((1 - attn_mask), -NEAR_INF)</div><div class="line"><a name="l00842"></a><span class="lineno">  842</span>&#160;            attn_weights = F.softmax(attn_w_premask, dim=1)</div><div class="line"><a name="l00843"></a><span class="lineno">  843</span>&#160;</div><div class="line"><a name="l00844"></a><span class="lineno">  844</span>&#160;        <span class="comment"># apply the attention weights to the encoder states</span></div><div class="line"><a name="l00845"></a><span class="lineno">  845</span>&#160;        attn_applied = torch.bmm(attn_weights.unsqueeze(1), enc_out)</div><div class="line"><a name="l00846"></a><span class="lineno">  846</span>&#160;        <span class="comment"># concatenate the input and encoder states</span></div><div class="line"><a name="l00847"></a><span class="lineno">  847</span>&#160;        merged = torch.cat((xes.squeeze(1), attn_applied.squeeze(1)), 1)</div><div class="line"><a name="l00848"></a><span class="lineno">  848</span>&#160;        <span class="comment"># combine them with a linear layer and tanh activation</span></div><div class="line"><a name="l00849"></a><span class="lineno">  849</span>&#160;        output = torch.tanh(self.attn_combine(merged).unsqueeze(1))</div><div class="line"><a name="l00850"></a><span class="lineno">  850</span>&#160;</div><div class="line"><a name="l00851"></a><span class="lineno">  851</span>&#160;        <span class="keywordflow">return</span> output, attn_weights</div><div class="line"><a name="l00852"></a><span class="lineno">  852</span>&#160;<div class="ttc" id="namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf_html_ad5dfae268e23f506da084a9efb72f619"><div class="ttname"><a href="../../d4/da3/namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf.html#ad5dfae268e23f506da084a9efb72f619">parlai.agents.tfidf_retriever.build_tfidf.type</a></div><div class="ttdeci">type</div><div class="ttdef"><b>Definition:</b> <a href="../../df/de8/build__tfidf_8py_source.html#l00339">build_tfidf.py:339</a></div></div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a6c95b5fa079109f57c305954f1f1465a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6c95b5fa079109f57c305954f1f1465a">&#9670;&nbsp;</a></span>attention</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer.attention</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d8/d53/modules__v1_8py_source.html#l00742">742</a> of file <a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a>.</p>

</div>
</div>
<a id="aa9eaecc56fcf6d7816fe23d0c27dd481"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa9eaecc56fcf6d7816fe23d0c27dd481">&#9670;&nbsp;</a></span>attn</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer.attn</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d8/d53/modules__v1_8py_source.html#l00765">765</a> of file <a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a>.</p>

</div>
</div>
<a id="a1ead8a9fdb5a6ad88b1613961a7eff99"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1ead8a9fdb5a6ad88b1613961a7eff99">&#9670;&nbsp;</a></span>attn_combine</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer.attn_combine</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d8/d53/modules__v1_8py_source.html#l00757">757</a> of file <a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a>.</p>

</div>
</div>
<a id="a034298f5224687c4fe53f401e5a13ec2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a034298f5224687c4fe53f401e5a13ec2">&#9670;&nbsp;</a></span>attn_v</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer.attn_v</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d8/d53/modules__v1_8py_source.html#l00769">769</a> of file <a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a>.</p>

</div>
</div>
<a id="afb020d5934ba3cee17968fe893417ce8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afb020d5934ba3cee17968fe893417ce8">&#9670;&nbsp;</a></span>max_length</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.legacy_agents.seq2seq.modules_v1.AttentionLayer.max_length</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d8/d53/modules__v1_8py_source.html#l00763">763</a> of file <a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>parlai/agents/legacy_agents/seq2seq/<a class="el" href="../../d8/d53/modules__v1_8py_source.html">modules_v1.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
