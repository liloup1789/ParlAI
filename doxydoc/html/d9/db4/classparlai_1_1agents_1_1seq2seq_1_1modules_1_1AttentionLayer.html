<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: parlai.agents.seq2seq.modules.AttentionLayer Class Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d5/d22/namespaceparlai.html">parlai</a></li><li class="navelem"><a class="el" href="../../dd/d61/namespaceparlai_1_1agents.html">agents</a></li><li class="navelem"><a class="el" href="../../de/d3e/namespaceparlai_1_1agents_1_1seq2seq.html">seq2seq</a></li><li class="navelem"><a class="el" href="../../d9/ddf/namespaceparlai_1_1agents_1_1seq2seq_1_1modules.html">modules</a></li><li class="navelem"><a class="el" href="../../d9/db4/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer.html">AttentionLayer</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="../../db/d76/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">parlai.agents.seq2seq.modules.AttentionLayer Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for parlai.agents.seq2seq.modules.AttentionLayer:</div>
<div class="dyncontent">
<div class="center"><img src="../../d6/d96/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer__inherit__graph.png" border="0" usemap="#parlai_8agents_8seq2seq_8modules_8AttentionLayer_inherit__map" alt="Inheritance graph"/></div>
<center><span class="legend">[<a href="../../graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for parlai.agents.seq2seq.modules.AttentionLayer:</div>
<div class="dyncontent">
<div class="center"><img src="../../d9/d2b/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer__coll__graph.png" border="0" usemap="#parlai_8agents_8seq2seq_8modules_8AttentionLayer_coll__map" alt="Collaboration graph"/></div>
<center><span class="legend">[<a href="../../graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aca18372fe25dbf5132ebcb2e63de31e3"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d9/db4/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer.html#aca18372fe25dbf5132ebcb2e63de31e3">__init__</a> (self, attn_type, hiddensize, embeddingsize, bidirectional=False, attn_length=-1, attn_time='pre')</td></tr>
<tr class="separator:aca18372fe25dbf5132ebcb2e63de31e3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1c840562c4d3d0a715b4d5ddb1b8bafe"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d9/db4/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer.html#a1c840562c4d3d0a715b4d5ddb1b8bafe">forward</a> (self, xes, hidden, attn_params)</td></tr>
<tr class="separator:a1c840562c4d3d0a715b4d5ddb1b8bafe"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a8582ed1bb09a75d9a54e812a1b4ae648"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d9/db4/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer.html#a8582ed1bb09a75d9a54e812a1b4ae648">attention</a></td></tr>
<tr class="separator:a8582ed1bb09a75d9a54e812a1b4ae648"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38e130fe29ef52f03f78ba3c9a2ce4b9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d9/db4/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer.html#a38e130fe29ef52f03f78ba3c9a2ce4b9">attn_combine</a></td></tr>
<tr class="separator:a38e130fe29ef52f03f78ba3c9a2ce4b9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac886c3ee00cb9163fed352af8aa53e0c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d9/db4/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer.html#ac886c3ee00cb9163fed352af8aa53e0c">max_length</a></td></tr>
<tr class="separator:ac886c3ee00cb9163fed352af8aa53e0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5bbb3d93ee56f0a41562141beb302da7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d9/db4/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer.html#a5bbb3d93ee56f0a41562141beb302da7">attn</a></td></tr>
<tr class="separator:a5bbb3d93ee56f0a41562141beb302da7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2b5f04f72da768b45739b7e9f1dd6705"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d9/db4/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer.html#a2b5f04f72da768b45739b7e9f1dd6705">attn_v</a></td></tr>
<tr class="separator:a2b5f04f72da768b45739b7e9f1dd6705"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Computes attention between hidden and encoder states.

See arxiv.org/abs/1508.04025 for more info on each attention type.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html#l00576">576</a> of file <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="aca18372fe25dbf5132ebcb2e63de31e3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aca18372fe25dbf5132ebcb2e63de31e3">&#9670;&nbsp;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def parlai.agents.seq2seq.modules.AttentionLayer.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hiddensize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>embeddingsize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>bidirectional</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_length</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_time</em> = <code>'pre'</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Initialize attention layer.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html#l00591">591</a> of file <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a>.</p>
<div class="fragment"><div class="line"><a name="l00591"></a><span class="lineno">  591</span>&#160;    ):</div><div class="line"><a name="l00592"></a><span class="lineno">  592</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00593"></a><span class="lineno">  593</span>&#160;<span class="stringliteral">        Initialize attention layer.</span></div><div class="line"><a name="l00594"></a><span class="lineno">  594</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00595"></a><span class="lineno">  595</span>&#160;        super().__init__()</div><div class="line"><a name="l00596"></a><span class="lineno">  596</span>&#160;        self.attention = attn_type</div><div class="line"><a name="l00597"></a><span class="lineno">  597</span>&#160;</div><div class="line"><a name="l00598"></a><span class="lineno">  598</span>&#160;        <span class="keywordflow">if</span> self.attention != <span class="stringliteral">&#39;none&#39;</span>:</div><div class="line"><a name="l00599"></a><span class="lineno">  599</span>&#160;            hsz = hiddensize</div><div class="line"><a name="l00600"></a><span class="lineno">  600</span>&#160;            hszXdirs = hsz * (2 <span class="keywordflow">if</span> bidirectional <span class="keywordflow">else</span> 1)</div><div class="line"><a name="l00601"></a><span class="lineno">  601</span>&#160;            <span class="keywordflow">if</span> attn_time == <span class="stringliteral">&#39;pre&#39;</span>:</div><div class="line"><a name="l00602"></a><span class="lineno">  602</span>&#160;                <span class="comment"># attention happens on the input embeddings</span></div><div class="line"><a name="l00603"></a><span class="lineno">  603</span>&#160;                input_dim = embeddingsize</div><div class="line"><a name="l00604"></a><span class="lineno">  604</span>&#160;            <span class="keywordflow">elif</span> attn_time == <span class="stringliteral">&#39;post&#39;</span>:</div><div class="line"><a name="l00605"></a><span class="lineno">  605</span>&#160;                <span class="comment"># attention happens on the output of the rnn</span></div><div class="line"><a name="l00606"></a><span class="lineno">  606</span>&#160;                input_dim = hsz</div><div class="line"><a name="l00607"></a><span class="lineno">  607</span>&#160;            <span class="keywordflow">else</span>:</div><div class="line"><a name="l00608"></a><span class="lineno">  608</span>&#160;                <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&#39;unsupported attention time&#39;</span>)</div><div class="line"><a name="l00609"></a><span class="lineno">  609</span>&#160;</div><div class="line"><a name="l00610"></a><span class="lineno">  610</span>&#160;            <span class="comment"># linear layer for combining applied attention weights with input</span></div><div class="line"><a name="l00611"></a><span class="lineno">  611</span>&#160;            self.attn_combine = nn.Linear(hszXdirs + input_dim, input_dim, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00612"></a><span class="lineno">  612</span>&#160;</div><div class="line"><a name="l00613"></a><span class="lineno">  613</span>&#160;            <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;local&#39;</span>:</div><div class="line"><a name="l00614"></a><span class="lineno">  614</span>&#160;                <span class="comment"># local attention over fixed set of output states</span></div><div class="line"><a name="l00615"></a><span class="lineno">  615</span>&#160;                <span class="keywordflow">if</span> attn_length &lt; 0:</div><div class="line"><a name="l00616"></a><span class="lineno">  616</span>&#160;                    <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&#39;Set attention length to &gt; 0.&#39;</span>)</div><div class="line"><a name="l00617"></a><span class="lineno">  617</span>&#160;                self.max_length = attn_length</div><div class="line"><a name="l00618"></a><span class="lineno">  618</span>&#160;                <span class="comment"># combines input and previous hidden output layer</span></div><div class="line"><a name="l00619"></a><span class="lineno">  619</span>&#160;                self.attn = nn.Linear(hsz + input_dim, attn_length, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00620"></a><span class="lineno">  620</span>&#160;                <span class="comment"># combines attention weights with encoder outputs</span></div><div class="line"><a name="l00621"></a><span class="lineno">  621</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;concat&#39;</span>:</div><div class="line"><a name="l00622"></a><span class="lineno">  622</span>&#160;                self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00623"></a><span class="lineno">  623</span>&#160;                self.attn_v = nn.Linear(hsz, 1, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00624"></a><span class="lineno">  624</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;general&#39;</span>:</div><div class="line"><a name="l00625"></a><span class="lineno">  625</span>&#160;                <span class="comment"># equivalent to dot if attn is identity</span></div><div class="line"><a name="l00626"></a><span class="lineno">  626</span>&#160;                self.attn = nn.Linear(hsz, hszXdirs, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00627"></a><span class="lineno">  627</span>&#160;</div></div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a1c840562c4d3d0a715b4d5ddb1b8bafe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1c840562c4d3d0a715b4d5ddb1b8bafe">&#9670;&nbsp;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def parlai.agents.seq2seq.modules.AttentionLayer.forward </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>xes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hidden</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_params</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute attention over attn_params given input and hidden states.

:param xes:         input state. will be combined with applied
            attention.
:param hidden:      hidden state from model. will be used to select
            states to attend to in from the attn_params.
:param attn_params: tuple of encoder output states and a mask showing
            which input indices are nonzero.

:returns: output, attn_weights
  output is a new state of same size as input state `xes`.
  attn_weights are the weights given to each state in the
  encoder outputs.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html#l00628">628</a> of file <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a>.</p>
<div class="fragment"><div class="line"><a name="l00628"></a><span class="lineno">  628</span>&#160;    <span class="keyword">def </span>forward(self, xes, hidden, attn_params):</div><div class="line"><a name="l00629"></a><span class="lineno">  629</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00630"></a><span class="lineno">  630</span>&#160;<span class="stringliteral">        Compute attention over attn_params given input and hidden states.</span></div><div class="line"><a name="l00631"></a><span class="lineno">  631</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00632"></a><span class="lineno">  632</span>&#160;<span class="stringliteral">        :param xes:         input state. will be combined with applied</span></div><div class="line"><a name="l00633"></a><span class="lineno">  633</span>&#160;<span class="stringliteral">                            attention.</span></div><div class="line"><a name="l00634"></a><span class="lineno">  634</span>&#160;<span class="stringliteral">        :param hidden:      hidden state from model. will be used to select</span></div><div class="line"><a name="l00635"></a><span class="lineno">  635</span>&#160;<span class="stringliteral">                            states to attend to in from the attn_params.</span></div><div class="line"><a name="l00636"></a><span class="lineno">  636</span>&#160;<span class="stringliteral">        :param attn_params: tuple of encoder output states and a mask showing</span></div><div class="line"><a name="l00637"></a><span class="lineno">  637</span>&#160;<span class="stringliteral">                            which input indices are nonzero.</span></div><div class="line"><a name="l00638"></a><span class="lineno">  638</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00639"></a><span class="lineno">  639</span>&#160;<span class="stringliteral">        :returns: output, attn_weights</span></div><div class="line"><a name="l00640"></a><span class="lineno">  640</span>&#160;<span class="stringliteral">                  output is a new state of same size as input state `xes`.</span></div><div class="line"><a name="l00641"></a><span class="lineno">  641</span>&#160;<span class="stringliteral">                  attn_weights are the weights given to each state in the</span></div><div class="line"><a name="l00642"></a><span class="lineno">  642</span>&#160;<span class="stringliteral">                  encoder outputs.</span></div><div class="line"><a name="l00643"></a><span class="lineno">  643</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00644"></a><span class="lineno">  644</span>&#160;        <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;none&#39;</span>:</div><div class="line"><a name="l00645"></a><span class="lineno">  645</span>&#160;            <span class="comment"># do nothing, no attention</span></div><div class="line"><a name="l00646"></a><span class="lineno">  646</span>&#160;            <span class="keywordflow">return</span> xes, <span class="keywordtype">None</span></div><div class="line"><a name="l00647"></a><span class="lineno">  647</span>&#160;</div><div class="line"><a name="l00648"></a><span class="lineno">  648</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="../../d4/da3/namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf.html#ad5dfae268e23f506da084a9efb72f619">type</a>(hidden) == tuple:</div><div class="line"><a name="l00649"></a><span class="lineno">  649</span>&#160;            <span class="comment"># for lstms use the &quot;hidden&quot; state not the cell state</span></div><div class="line"><a name="l00650"></a><span class="lineno">  650</span>&#160;            hidden = hidden[0]</div><div class="line"><a name="l00651"></a><span class="lineno">  651</span>&#160;        last_hidden = hidden[-1]  <span class="comment"># select hidden state from last RNN layer</span></div><div class="line"><a name="l00652"></a><span class="lineno">  652</span>&#160;</div><div class="line"><a name="l00653"></a><span class="lineno">  653</span>&#160;        enc_out, attn_mask = attn_params</div><div class="line"><a name="l00654"></a><span class="lineno">  654</span>&#160;        bsz, seqlen, hszXnumdir = enc_out.size()</div><div class="line"><a name="l00655"></a><span class="lineno">  655</span>&#160;        numlayersXnumdir = last_hidden.size(1)</div><div class="line"><a name="l00656"></a><span class="lineno">  656</span>&#160;</div><div class="line"><a name="l00657"></a><span class="lineno">  657</span>&#160;        <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;local&#39;</span>:</div><div class="line"><a name="l00658"></a><span class="lineno">  658</span>&#160;            <span class="comment"># local attention weights aren&#39;t based on encoder states</span></div><div class="line"><a name="l00659"></a><span class="lineno">  659</span>&#160;            h_merged = torch.cat((xes.squeeze(1), last_hidden), 1)</div><div class="line"><a name="l00660"></a><span class="lineno">  660</span>&#160;            attn_weights = F.softmax(self.attn(h_merged), dim=1)</div><div class="line"><a name="l00661"></a><span class="lineno">  661</span>&#160;</div><div class="line"><a name="l00662"></a><span class="lineno">  662</span>&#160;            <span class="comment"># adjust state sizes to the fixed window size</span></div><div class="line"><a name="l00663"></a><span class="lineno">  663</span>&#160;            <span class="keywordflow">if</span> seqlen &gt; self.max_length:</div><div class="line"><a name="l00664"></a><span class="lineno">  664</span>&#160;                offset = seqlen - self.max_length</div><div class="line"><a name="l00665"></a><span class="lineno">  665</span>&#160;                enc_out = enc_out.narrow(1, offset, self.max_length)</div><div class="line"><a name="l00666"></a><span class="lineno">  666</span>&#160;                seqlen = self.max_length</div><div class="line"><a name="l00667"></a><span class="lineno">  667</span>&#160;            <span class="keywordflow">if</span> attn_weights.size(1) &gt; seqlen:</div><div class="line"><a name="l00668"></a><span class="lineno">  668</span>&#160;                attn_weights = attn_weights.narrow(1, 0, seqlen)</div><div class="line"><a name="l00669"></a><span class="lineno">  669</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00670"></a><span class="lineno">  670</span>&#160;            hid = last_hidden.unsqueeze(1)</div><div class="line"><a name="l00671"></a><span class="lineno">  671</span>&#160;            <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;concat&#39;</span>:</div><div class="line"><a name="l00672"></a><span class="lineno">  672</span>&#160;                <span class="comment"># concat hidden state and encoder outputs</span></div><div class="line"><a name="l00673"></a><span class="lineno">  673</span>&#160;                hid = hid.expand(bsz, seqlen, numlayersXnumdir)</div><div class="line"><a name="l00674"></a><span class="lineno">  674</span>&#160;                h_merged = torch.cat((enc_out, hid), 2)</div><div class="line"><a name="l00675"></a><span class="lineno">  675</span>&#160;                <span class="comment"># then do linear combination of them with activation</span></div><div class="line"><a name="l00676"></a><span class="lineno">  676</span>&#160;                active = F.tanh(self.attn(h_merged))</div><div class="line"><a name="l00677"></a><span class="lineno">  677</span>&#160;                attn_w_premask = self.attn_v(active).squeeze(2)</div><div class="line"><a name="l00678"></a><span class="lineno">  678</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;dot&#39;</span>:</div><div class="line"><a name="l00679"></a><span class="lineno">  679</span>&#160;                <span class="comment"># dot product between hidden and encoder outputs</span></div><div class="line"><a name="l00680"></a><span class="lineno">  680</span>&#160;                <span class="keywordflow">if</span> numlayersXnumdir != hszXnumdir:</div><div class="line"><a name="l00681"></a><span class="lineno">  681</span>&#160;                    <span class="comment"># enc_out has two directions, so double hid</span></div><div class="line"><a name="l00682"></a><span class="lineno">  682</span>&#160;                    hid = torch.cat([hid, hid], 2)</div><div class="line"><a name="l00683"></a><span class="lineno">  683</span>&#160;                enc_t = enc_out.transpose(1, 2)</div><div class="line"><a name="l00684"></a><span class="lineno">  684</span>&#160;                attn_w_premask = torch.bmm(hid, enc_t).squeeze(1)</div><div class="line"><a name="l00685"></a><span class="lineno">  685</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;general&#39;</span>:</div><div class="line"><a name="l00686"></a><span class="lineno">  686</span>&#160;                <span class="comment"># before doing dot product, transform hidden state with linear</span></div><div class="line"><a name="l00687"></a><span class="lineno">  687</span>&#160;                <span class="comment"># same as dot if linear is identity</span></div><div class="line"><a name="l00688"></a><span class="lineno">  688</span>&#160;                hid = self.attn(hid)</div><div class="line"><a name="l00689"></a><span class="lineno">  689</span>&#160;                enc_t = enc_out.transpose(1, 2)</div><div class="line"><a name="l00690"></a><span class="lineno">  690</span>&#160;                attn_w_premask = torch.bmm(hid, enc_t).squeeze(1)</div><div class="line"><a name="l00691"></a><span class="lineno">  691</span>&#160;</div><div class="line"><a name="l00692"></a><span class="lineno">  692</span>&#160;            <span class="comment"># calculate activation scores, apply mask if needed</span></div><div class="line"><a name="l00693"></a><span class="lineno">  693</span>&#160;            <span class="keywordflow">if</span> attn_mask <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00694"></a><span class="lineno">  694</span>&#160;                <span class="comment"># remove activation from NULL symbols</span></div><div class="line"><a name="l00695"></a><span class="lineno">  695</span>&#160;                attn_w_premask.masked_fill_((~attn_mask), -NEAR_INF)</div><div class="line"><a name="l00696"></a><span class="lineno">  696</span>&#160;            attn_weights = F.softmax(attn_w_premask, dim=1)</div><div class="line"><a name="l00697"></a><span class="lineno">  697</span>&#160;</div><div class="line"><a name="l00698"></a><span class="lineno">  698</span>&#160;        <span class="comment"># apply the attention weights to the encoder states</span></div><div class="line"><a name="l00699"></a><span class="lineno">  699</span>&#160;        attn_applied = torch.bmm(attn_weights.unsqueeze(1), enc_out)</div><div class="line"><a name="l00700"></a><span class="lineno">  700</span>&#160;        <span class="comment"># concatenate the input and encoder states</span></div><div class="line"><a name="l00701"></a><span class="lineno">  701</span>&#160;        merged = torch.cat((xes.squeeze(1), attn_applied.squeeze(1)), 1)</div><div class="line"><a name="l00702"></a><span class="lineno">  702</span>&#160;        <span class="comment"># combine them with a linear layer and tanh activation</span></div><div class="line"><a name="l00703"></a><span class="lineno">  703</span>&#160;        output = torch.tanh(self.attn_combine(merged).unsqueeze(1))</div><div class="line"><a name="l00704"></a><span class="lineno">  704</span>&#160;</div><div class="line"><a name="l00705"></a><span class="lineno">  705</span>&#160;        <span class="keywordflow">return</span> output, attn_weights</div><div class="line"><a name="l00706"></a><span class="lineno">  706</span>&#160;<div class="ttc" id="namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf_html_ad5dfae268e23f506da084a9efb72f619"><div class="ttname"><a href="../../d4/da3/namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf.html#ad5dfae268e23f506da084a9efb72f619">parlai.agents.tfidf_retriever.build_tfidf.type</a></div><div class="ttdeci">type</div><div class="ttdef"><b>Definition:</b> <a href="../../df/de8/build__tfidf_8py_source.html#l00339">build_tfidf.py:339</a></div></div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a8582ed1bb09a75d9a54e812a1b4ae648"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8582ed1bb09a75d9a54e812a1b4ae648">&#9670;&nbsp;</a></span>attention</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.seq2seq.modules.AttentionLayer.attention</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html#l00596">596</a> of file <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<a id="a5bbb3d93ee56f0a41562141beb302da7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5bbb3d93ee56f0a41562141beb302da7">&#9670;&nbsp;</a></span>attn</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.seq2seq.modules.AttentionLayer.attn</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html#l00619">619</a> of file <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<a id="a38e130fe29ef52f03f78ba3c9a2ce4b9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a38e130fe29ef52f03f78ba3c9a2ce4b9">&#9670;&nbsp;</a></span>attn_combine</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.seq2seq.modules.AttentionLayer.attn_combine</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html#l00611">611</a> of file <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<a id="a2b5f04f72da768b45739b7e9f1dd6705"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2b5f04f72da768b45739b7e9f1dd6705">&#9670;&nbsp;</a></span>attn_v</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.seq2seq.modules.AttentionLayer.attn_v</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html#l00623">623</a> of file <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<a id="ac886c3ee00cb9163fed352af8aa53e0c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac886c3ee00cb9163fed352af8aa53e0c">&#9670;&nbsp;</a></span>max_length</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">parlai.agents.seq2seq.modules.AttentionLayer.max_length</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html#l00617">617</a> of file <a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>parlai/agents/seq2seq/<a class="el" href="../../d5/dc5/parlai_2agents_2seq2seq_2modules_8py_source.html">modules.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
