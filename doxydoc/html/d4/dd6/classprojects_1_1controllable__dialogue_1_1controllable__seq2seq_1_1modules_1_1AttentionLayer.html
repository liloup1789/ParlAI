<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer Class Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d9/df1/namespaceprojects.html">projects</a></li><li class="navelem"><a class="el" href="../../db/da4/namespaceprojects_1_1controllable__dialogue.html">controllable_dialogue</a></li><li class="navelem"><a class="el" href="../../dd/dd3/namespaceprojects_1_1controllable__dialogue_1_1controllable__seq2seq.html">controllable_seq2seq</a></li><li class="navelem"><a class="el" href="../../df/dd2/namespaceprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules.html">modules</a></li><li class="navelem"><a class="el" href="../../d4/dd6/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer.html">AttentionLayer</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="../../d8/d4b/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer:</div>
<div class="dyncontent">
<div class="center"><img src="../../d7/dc9/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer__inherit__graph.png" border="0" usemap="#projects_8controllable__dialogue_8controllable__seq2seq_8modules_8AttentionLayer_inherit__map" alt="Inheritance graph"/></div>
<map name="projects_8controllable__dialogue_8controllable__seq2seq_8modules_8AttentionLayer_inherit__map" id="projects_8controllable__dialogue_8controllable__seq2seq_8modules_8AttentionLayer_inherit__map">
</map>
<center><span class="legend">[<a href="../../graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer:</div>
<div class="dyncontent">
<div class="center"><img src="../../d7/dcf/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer__coll__graph.png" border="0" usemap="#projects_8controllable__dialogue_8controllable__seq2seq_8modules_8AttentionLayer_coll__map" alt="Collaboration graph"/></div>
<map name="projects_8controllable__dialogue_8controllable__seq2seq_8modules_8AttentionLayer_coll__map" id="projects_8controllable__dialogue_8controllable__seq2seq_8modules_8AttentionLayer_coll__map">
</map>
<center><span class="legend">[<a href="../../graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:ae4cb3ab12da870aa7fda064ea2f9a082"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d4/dd6/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer.html#ae4cb3ab12da870aa7fda064ea2f9a082">__init__</a> (self, attn_type, hiddensize, embeddingsize, bidirectional=False, attn_length=-1, attn_time='pre')</td></tr>
<tr class="separator:ae4cb3ab12da870aa7fda064ea2f9a082"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18edcb2fb56e109d42db47ba411811b9"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d4/dd6/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer.html#a18edcb2fb56e109d42db47ba411811b9">forward</a> (self, xes, hidden, attn_params)</td></tr>
<tr class="separator:a18edcb2fb56e109d42db47ba411811b9"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:afe3902f442288aaef94b75b15b6547b7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d4/dd6/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer.html#afe3902f442288aaef94b75b15b6547b7">attention</a></td></tr>
<tr class="separator:afe3902f442288aaef94b75b15b6547b7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93efcb1437c5bbf7e8f99c843a6ed086"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d4/dd6/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer.html#a93efcb1437c5bbf7e8f99c843a6ed086">attn_combine</a></td></tr>
<tr class="separator:a93efcb1437c5bbf7e8f99c843a6ed086"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4e117b37868e62d4348fdead83f6b5cc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d4/dd6/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer.html#a4e117b37868e62d4348fdead83f6b5cc">max_length</a></td></tr>
<tr class="separator:a4e117b37868e62d4348fdead83f6b5cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0cedce597de9cf52d627f566998f6f85"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d4/dd6/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer.html#a0cedce597de9cf52d627f566998f6f85">attn</a></td></tr>
<tr class="separator:a0cedce597de9cf52d627f566998f6f85"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1fefce0c25d815745a8d6c5bfd98b18"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d4/dd6/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer.html#ab1fefce0c25d815745a8d6c5bfd98b18">attn_v</a></td></tr>
<tr class="separator:ab1fefce0c25d815745a8d6c5bfd98b18"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Computes attention between hidden and encoder states.

See arxiv.org/abs/1508.04025 for more info on each attention type.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html#l00815">815</a> of file <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="ae4cb3ab12da870aa7fda064ea2f9a082"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae4cb3ab12da870aa7fda064ea2f9a082">&#9670;&nbsp;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hiddensize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>embeddingsize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>bidirectional</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_length</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_time</em> = <code>'pre'</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Initialize attention layer.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html#l00830">830</a> of file <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a>.</p>
<div class="fragment"><div class="line"><a name="l00830"></a><span class="lineno">  830</span>&#160;    ):</div><div class="line"><a name="l00831"></a><span class="lineno">  831</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00832"></a><span class="lineno">  832</span>&#160;<span class="stringliteral">        Initialize attention layer.</span></div><div class="line"><a name="l00833"></a><span class="lineno">  833</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00834"></a><span class="lineno">  834</span>&#160;        super().__init__()</div><div class="line"><a name="l00835"></a><span class="lineno">  835</span>&#160;        self.attention = attn_type</div><div class="line"><a name="l00836"></a><span class="lineno">  836</span>&#160;</div><div class="line"><a name="l00837"></a><span class="lineno">  837</span>&#160;        <span class="keywordflow">if</span> self.attention != <span class="stringliteral">&#39;none&#39;</span>:</div><div class="line"><a name="l00838"></a><span class="lineno">  838</span>&#160;            hsz = hiddensize</div><div class="line"><a name="l00839"></a><span class="lineno">  839</span>&#160;            hszXdirs = hsz * (2 <span class="keywordflow">if</span> bidirectional <span class="keywordflow">else</span> 1)</div><div class="line"><a name="l00840"></a><span class="lineno">  840</span>&#160;            <span class="keywordflow">if</span> attn_time == <span class="stringliteral">&#39;pre&#39;</span>:</div><div class="line"><a name="l00841"></a><span class="lineno">  841</span>&#160;                <span class="comment"># attention happens on the input embeddings</span></div><div class="line"><a name="l00842"></a><span class="lineno">  842</span>&#160;                input_dim = embeddingsize</div><div class="line"><a name="l00843"></a><span class="lineno">  843</span>&#160;            <span class="keywordflow">elif</span> attn_time == <span class="stringliteral">&#39;post&#39;</span>:</div><div class="line"><a name="l00844"></a><span class="lineno">  844</span>&#160;                <span class="comment"># attention happens on the output of the rnn</span></div><div class="line"><a name="l00845"></a><span class="lineno">  845</span>&#160;                input_dim = hsz</div><div class="line"><a name="l00846"></a><span class="lineno">  846</span>&#160;            <span class="keywordflow">else</span>:</div><div class="line"><a name="l00847"></a><span class="lineno">  847</span>&#160;                <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&#39;unsupported attention time&#39;</span>)</div><div class="line"><a name="l00848"></a><span class="lineno">  848</span>&#160;</div><div class="line"><a name="l00849"></a><span class="lineno">  849</span>&#160;            <span class="comment"># linear layer for combining applied attention weights with input</span></div><div class="line"><a name="l00850"></a><span class="lineno">  850</span>&#160;            self.attn_combine = nn.Linear(hszXdirs + input_dim, input_dim, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00851"></a><span class="lineno">  851</span>&#160;</div><div class="line"><a name="l00852"></a><span class="lineno">  852</span>&#160;            <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;local&#39;</span>:</div><div class="line"><a name="l00853"></a><span class="lineno">  853</span>&#160;                <span class="comment"># local attention over fixed set of output states</span></div><div class="line"><a name="l00854"></a><span class="lineno">  854</span>&#160;                <span class="keywordflow">if</span> attn_length &lt; 0:</div><div class="line"><a name="l00855"></a><span class="lineno">  855</span>&#160;                    <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&#39;Set attention length to &gt; 0.&#39;</span>)</div><div class="line"><a name="l00856"></a><span class="lineno">  856</span>&#160;                self.max_length = attn_length</div><div class="line"><a name="l00857"></a><span class="lineno">  857</span>&#160;                <span class="comment"># combines input and previous hidden output layer</span></div><div class="line"><a name="l00858"></a><span class="lineno">  858</span>&#160;                self.attn = nn.Linear(hsz + input_dim, attn_length, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00859"></a><span class="lineno">  859</span>&#160;                <span class="comment"># combines attention weights with encoder outputs</span></div><div class="line"><a name="l00860"></a><span class="lineno">  860</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;concat&#39;</span>:</div><div class="line"><a name="l00861"></a><span class="lineno">  861</span>&#160;                self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00862"></a><span class="lineno">  862</span>&#160;                self.attn_v = nn.Linear(hsz, 1, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00863"></a><span class="lineno">  863</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;general&#39;</span>:</div><div class="line"><a name="l00864"></a><span class="lineno">  864</span>&#160;                <span class="comment"># equivalent to dot if attn is identity</span></div><div class="line"><a name="l00865"></a><span class="lineno">  865</span>&#160;                self.attn = nn.Linear(hsz, hszXdirs, bias=<span class="keyword">False</span>)</div><div class="line"><a name="l00866"></a><span class="lineno">  866</span>&#160;</div></div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a18edcb2fb56e109d42db47ba411811b9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a18edcb2fb56e109d42db47ba411811b9">&#9670;&nbsp;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer.forward </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>xes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hidden</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>attn_params</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute attention over attn_params given input and hidden states.

:param xes:         input state. will be combined with applied
            attention.
:param hidden:      hidden state from model. will be used to select
            states to attend to in from the attn_params.
:param attn_params: tuple of encoder output states and a mask showing
            which input indices are nonzero.

:returns: output, attn_weights
  output is a new state of same size as input state `xes`.
  attn_weights are the weights given to each state in the
  encoder outputs.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html#l00867">867</a> of file <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a>.</p>
<div class="fragment"><div class="line"><a name="l00867"></a><span class="lineno">  867</span>&#160;    <span class="keyword">def </span>forward(self, xes, hidden, attn_params):</div><div class="line"><a name="l00868"></a><span class="lineno">  868</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00869"></a><span class="lineno">  869</span>&#160;<span class="stringliteral">        Compute attention over attn_params given input and hidden states.</span></div><div class="line"><a name="l00870"></a><span class="lineno">  870</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00871"></a><span class="lineno">  871</span>&#160;<span class="stringliteral">        :param xes:         input state. will be combined with applied</span></div><div class="line"><a name="l00872"></a><span class="lineno">  872</span>&#160;<span class="stringliteral">                            attention.</span></div><div class="line"><a name="l00873"></a><span class="lineno">  873</span>&#160;<span class="stringliteral">        :param hidden:      hidden state from model. will be used to select</span></div><div class="line"><a name="l00874"></a><span class="lineno">  874</span>&#160;<span class="stringliteral">                            states to attend to in from the attn_params.</span></div><div class="line"><a name="l00875"></a><span class="lineno">  875</span>&#160;<span class="stringliteral">        :param attn_params: tuple of encoder output states and a mask showing</span></div><div class="line"><a name="l00876"></a><span class="lineno">  876</span>&#160;<span class="stringliteral">                            which input indices are nonzero.</span></div><div class="line"><a name="l00877"></a><span class="lineno">  877</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00878"></a><span class="lineno">  878</span>&#160;<span class="stringliteral">        :returns: output, attn_weights</span></div><div class="line"><a name="l00879"></a><span class="lineno">  879</span>&#160;<span class="stringliteral">                  output is a new state of same size as input state `xes`.</span></div><div class="line"><a name="l00880"></a><span class="lineno">  880</span>&#160;<span class="stringliteral">                  attn_weights are the weights given to each state in the</span></div><div class="line"><a name="l00881"></a><span class="lineno">  881</span>&#160;<span class="stringliteral">                  encoder outputs.</span></div><div class="line"><a name="l00882"></a><span class="lineno">  882</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00883"></a><span class="lineno">  883</span>&#160;        <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;none&#39;</span>:</div><div class="line"><a name="l00884"></a><span class="lineno">  884</span>&#160;            <span class="comment"># do nothing, no attention</span></div><div class="line"><a name="l00885"></a><span class="lineno">  885</span>&#160;            <span class="keywordflow">return</span> xes, <span class="keywordtype">None</span></div><div class="line"><a name="l00886"></a><span class="lineno">  886</span>&#160;</div><div class="line"><a name="l00887"></a><span class="lineno">  887</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="../../d4/da3/namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf.html#ad5dfae268e23f506da084a9efb72f619">type</a>(hidden) == tuple:</div><div class="line"><a name="l00888"></a><span class="lineno">  888</span>&#160;            <span class="comment"># for lstms use the &quot;hidden&quot; state not the cell state</span></div><div class="line"><a name="l00889"></a><span class="lineno">  889</span>&#160;            hidden = hidden[0]</div><div class="line"><a name="l00890"></a><span class="lineno">  890</span>&#160;        last_hidden = hidden[-1]  <span class="comment"># select hidden state from last RNN layer</span></div><div class="line"><a name="l00891"></a><span class="lineno">  891</span>&#160;</div><div class="line"><a name="l00892"></a><span class="lineno">  892</span>&#160;        enc_out, attn_mask = attn_params</div><div class="line"><a name="l00893"></a><span class="lineno">  893</span>&#160;        bsz, seqlen, hszXnumdir = enc_out.size()</div><div class="line"><a name="l00894"></a><span class="lineno">  894</span>&#160;        numlayersXnumdir = last_hidden.size(1)</div><div class="line"><a name="l00895"></a><span class="lineno">  895</span>&#160;</div><div class="line"><a name="l00896"></a><span class="lineno">  896</span>&#160;        <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;local&#39;</span>:</div><div class="line"><a name="l00897"></a><span class="lineno">  897</span>&#160;            <span class="comment"># local attention weights aren&#39;t based on encoder states</span></div><div class="line"><a name="l00898"></a><span class="lineno">  898</span>&#160;            h_merged = torch.cat((xes.squeeze(1), last_hidden), 1)</div><div class="line"><a name="l00899"></a><span class="lineno">  899</span>&#160;            attn_weights = F.softmax(self.attn(h_merged), dim=1)</div><div class="line"><a name="l00900"></a><span class="lineno">  900</span>&#160;</div><div class="line"><a name="l00901"></a><span class="lineno">  901</span>&#160;            <span class="comment"># adjust state sizes to the fixed window size</span></div><div class="line"><a name="l00902"></a><span class="lineno">  902</span>&#160;            <span class="keywordflow">if</span> seqlen &gt; self.max_length:</div><div class="line"><a name="l00903"></a><span class="lineno">  903</span>&#160;                offset = seqlen - self.max_length</div><div class="line"><a name="l00904"></a><span class="lineno">  904</span>&#160;                enc_out = enc_out.narrow(1, offset, self.max_length)</div><div class="line"><a name="l00905"></a><span class="lineno">  905</span>&#160;                seqlen = self.max_length</div><div class="line"><a name="l00906"></a><span class="lineno">  906</span>&#160;            <span class="keywordflow">if</span> attn_weights.size(1) &gt; seqlen:</div><div class="line"><a name="l00907"></a><span class="lineno">  907</span>&#160;                attn_weights = attn_weights.narrow(1, 0, seqlen)</div><div class="line"><a name="l00908"></a><span class="lineno">  908</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00909"></a><span class="lineno">  909</span>&#160;            hid = last_hidden.unsqueeze(1)</div><div class="line"><a name="l00910"></a><span class="lineno">  910</span>&#160;            <span class="keywordflow">if</span> self.attention == <span class="stringliteral">&#39;concat&#39;</span>:</div><div class="line"><a name="l00911"></a><span class="lineno">  911</span>&#160;                <span class="comment"># concat hidden state and encoder outputs</span></div><div class="line"><a name="l00912"></a><span class="lineno">  912</span>&#160;                hid = hid.expand(bsz, seqlen, numlayersXnumdir)</div><div class="line"><a name="l00913"></a><span class="lineno">  913</span>&#160;                h_merged = torch.cat((enc_out, hid), 2)</div><div class="line"><a name="l00914"></a><span class="lineno">  914</span>&#160;                <span class="comment"># then do linear combination of them with activation</span></div><div class="line"><a name="l00915"></a><span class="lineno">  915</span>&#160;                active = F.tanh(self.attn(h_merged))</div><div class="line"><a name="l00916"></a><span class="lineno">  916</span>&#160;                attn_w_premask = self.attn_v(active).squeeze(2)</div><div class="line"><a name="l00917"></a><span class="lineno">  917</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;dot&#39;</span>:</div><div class="line"><a name="l00918"></a><span class="lineno">  918</span>&#160;                <span class="comment"># dot product between hidden and encoder outputs</span></div><div class="line"><a name="l00919"></a><span class="lineno">  919</span>&#160;                <span class="keywordflow">if</span> numlayersXnumdir != hszXnumdir:</div><div class="line"><a name="l00920"></a><span class="lineno">  920</span>&#160;                    <span class="comment"># enc_out has two directions, so double hid</span></div><div class="line"><a name="l00921"></a><span class="lineno">  921</span>&#160;                    hid = torch.cat([hid, hid], 2)</div><div class="line"><a name="l00922"></a><span class="lineno">  922</span>&#160;                enc_t = enc_out.transpose(1, 2)</div><div class="line"><a name="l00923"></a><span class="lineno">  923</span>&#160;                attn_w_premask = torch.bmm(hid, enc_t).squeeze(1)</div><div class="line"><a name="l00924"></a><span class="lineno">  924</span>&#160;            <span class="keywordflow">elif</span> self.attention == <span class="stringliteral">&#39;general&#39;</span>:</div><div class="line"><a name="l00925"></a><span class="lineno">  925</span>&#160;                <span class="comment"># before doing dot product, transform hidden state with linear</span></div><div class="line"><a name="l00926"></a><span class="lineno">  926</span>&#160;                <span class="comment"># same as dot if linear is identity</span></div><div class="line"><a name="l00927"></a><span class="lineno">  927</span>&#160;                hid = self.attn(hid)</div><div class="line"><a name="l00928"></a><span class="lineno">  928</span>&#160;                enc_t = enc_out.transpose(1, 2)</div><div class="line"><a name="l00929"></a><span class="lineno">  929</span>&#160;                attn_w_premask = torch.bmm(hid, enc_t).squeeze(1)</div><div class="line"><a name="l00930"></a><span class="lineno">  930</span>&#160;</div><div class="line"><a name="l00931"></a><span class="lineno">  931</span>&#160;            <span class="comment"># calculate activation scores, apply mask if needed</span></div><div class="line"><a name="l00932"></a><span class="lineno">  932</span>&#160;            <span class="keywordflow">if</span> attn_mask <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00933"></a><span class="lineno">  933</span>&#160;                <span class="comment"># remove activation from NULL symbols</span></div><div class="line"><a name="l00934"></a><span class="lineno">  934</span>&#160;                attn_w_premask.masked_fill_(~attn_mask, -NEAR_INF)</div><div class="line"><a name="l00935"></a><span class="lineno">  935</span>&#160;            attn_weights = F.softmax(attn_w_premask, dim=1)</div><div class="line"><a name="l00936"></a><span class="lineno">  936</span>&#160;</div><div class="line"><a name="l00937"></a><span class="lineno">  937</span>&#160;        <span class="comment"># apply the attention weights to the encoder states</span></div><div class="line"><a name="l00938"></a><span class="lineno">  938</span>&#160;        attn_applied = torch.bmm(attn_weights.unsqueeze(1), enc_out)</div><div class="line"><a name="l00939"></a><span class="lineno">  939</span>&#160;        <span class="comment"># concatenate the input and encoder states</span></div><div class="line"><a name="l00940"></a><span class="lineno">  940</span>&#160;        merged = torch.cat((xes.squeeze(1), attn_applied.squeeze(1)), 1)</div><div class="line"><a name="l00941"></a><span class="lineno">  941</span>&#160;        <span class="comment"># combine them with a linear layer and tanh activation</span></div><div class="line"><a name="l00942"></a><span class="lineno">  942</span>&#160;        output = torch.tanh(self.attn_combine(merged).unsqueeze(1))</div><div class="line"><a name="l00943"></a><span class="lineno">  943</span>&#160;</div><div class="line"><a name="l00944"></a><span class="lineno">  944</span>&#160;        <span class="keywordflow">return</span> output, attn_weights</div><div class="line"><a name="l00945"></a><span class="lineno">  945</span>&#160;<div class="ttc" id="namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf_html_ad5dfae268e23f506da084a9efb72f619"><div class="ttname"><a href="../../d4/da3/namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf.html#ad5dfae268e23f506da084a9efb72f619">parlai.agents.tfidf_retriever.build_tfidf.type</a></div><div class="ttdeci">type</div><div class="ttdef"><b>Definition:</b> <a href="../../df/de8/build__tfidf_8py_source.html#l00339">build_tfidf.py:339</a></div></div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="afe3902f442288aaef94b75b15b6547b7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afe3902f442288aaef94b75b15b6547b7">&#9670;&nbsp;</a></span>attention</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer.attention</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html#l00835">835</a> of file <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<a id="a0cedce597de9cf52d627f566998f6f85"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0cedce597de9cf52d627f566998f6f85">&#9670;&nbsp;</a></span>attn</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer.attn</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html#l00858">858</a> of file <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<a id="a93efcb1437c5bbf7e8f99c843a6ed086"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93efcb1437c5bbf7e8f99c843a6ed086">&#9670;&nbsp;</a></span>attn_combine</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer.attn_combine</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html#l00850">850</a> of file <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<a id="ab1fefce0c25d815745a8d6c5bfd98b18"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab1fefce0c25d815745a8d6c5bfd98b18">&#9670;&nbsp;</a></span>attn_v</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer.attn_v</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html#l00862">862</a> of file <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<a id="a4e117b37868e62d4348fdead83f6b5cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4e117b37868e62d4348fdead83f6b5cc">&#9670;&nbsp;</a></span>max_length</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">projects.controllable_dialogue.controllable_seq2seq.modules.AttentionLayer.max_length</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html#l00856">856</a> of file <a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>projects/controllable_dialogue/controllable_seq2seq/<a class="el" href="../../d6/d33/projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py_source.html">modules.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
