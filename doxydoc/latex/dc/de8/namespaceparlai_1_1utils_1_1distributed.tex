\hypertarget{namespaceparlai_1_1utils_1_1distributed}{}\section{parlai.\+utils.\+distributed Namespace Reference}
\label{namespaceparlai_1_1utils_1_1distributed}\index{parlai.\+utils.\+distributed@{parlai.\+utils.\+distributed}}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{namespaceparlai_1_1utils_1_1distributed_afd854992e4cc6571b120b9e179cd4a7a}{validate\+\_\+params} (opt)
\item 
def \hyperlink{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}{is\+\_\+distributed} ()
\item 
def \hyperlink{namespaceparlai_1_1utils_1_1distributed_a99b61b4756577c6542039c238d670dba}{num\+\_\+workers} ()
\item 
def \hyperlink{namespaceparlai_1_1utils_1_1distributed_a9bb1dac198180590ef8c6b6c6f9fc2c4}{is\+\_\+primary\+\_\+worker} ()
\item 
def \hyperlink{namespaceparlai_1_1utils_1_1distributed_a601345e55fc3fc3845c43efa1a04db56}{override\+\_\+print} (suppress=False, prefix=None)
\item 
def \hyperlink{namespaceparlai_1_1utils_1_1distributed_a9ef5bf0debf512ddbeb045327bfd87ef}{all\+\_\+gather\+\_\+list} (data, max\+\_\+size=16384)
\item 
def \hyperlink{namespaceparlai_1_1utils_1_1distributed_a269d3c7284127d9b287b9b61e9161de6}{sync\+\_\+object} (data, max\+\_\+size=16384)
\item 
def \hyperlink{namespaceparlai_1_1utils_1_1distributed_a47e6d93d849e487d7d3f6e1a83468073}{sync\+\_\+parameters}
\end{DoxyCompactItemize}
\subsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\hyperlink{namespaceparlai_1_1utils_1_1distributed_af99c57e9c67ff88f4802625436fc2c6c}{T\+O\+R\+C\+H\+\_\+\+A\+V\+A\+I\+L\+A\+B\+LE}
\end{DoxyCompactItemize}


\subsection{Function Documentation}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_a9ef5bf0debf512ddbeb045327bfd87ef}\label{namespaceparlai_1_1utils_1_1distributed_a9ef5bf0debf512ddbeb045327bfd87ef}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!all\+\_\+gather\+\_\+list@{all\+\_\+gather\+\_\+list}}
\index{all\+\_\+gather\+\_\+list@{all\+\_\+gather\+\_\+list}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{all\+\_\+gather\+\_\+list()}{all\_gather\_list()}}
{\footnotesize\ttfamily def parlai.\+utils.\+distributed.\+all\+\_\+gather\+\_\+list (\begin{DoxyParamCaption}\item[{}]{data,  }\item[{}]{max\+\_\+size = {\ttfamily 16384} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Gather arbitrary data from all nodes into a list.

Similar to `~torch.distributed.all_gather` but for arbitrary Python
data. Note that *data* must be picklable.

:param data:
    data from the local worker to be gathered on other workers
:param int max_size:
    maximum size of the data to be gathered across workers

:returns:
    a list containing [data1, data2, ...] of all workers
\end{DoxyVerb}
 

Definition at line 115 of file distributed.\+py.


\begin{DoxyCode}
115 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1utils_1_1distributed_a9ef5bf0debf512ddbeb045327bfd87ef}{all\_gather\_list}(data, max\_size=16384):
116     \textcolor{stringliteral}{"""}
117 \textcolor{stringliteral}{    Gather arbitrary data from all nodes into a list.}
118 \textcolor{stringliteral}{}
119 \textcolor{stringliteral}{    Similar to `~torch.distributed.all\_gather` but for arbitrary Python}
120 \textcolor{stringliteral}{    data. Note that *data* must be picklable.}
121 \textcolor{stringliteral}{}
122 \textcolor{stringliteral}{    :param data:}
123 \textcolor{stringliteral}{        data from the local worker to be gathered on other workers}
124 \textcolor{stringliteral}{    :param int max\_size:}
125 \textcolor{stringliteral}{        maximum size of the data to be gathered across workers}
126 \textcolor{stringliteral}{}
127 \textcolor{stringliteral}{    :returns:}
128 \textcolor{stringliteral}{        a list containing [data1, data2, ...] of all workers}
129 \textcolor{stringliteral}{    """}
130     \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \hyperlink{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}{is\_distributed}():
131         \textcolor{comment}{# fall back to just keeping things basic if we're not distributed}
132         \textcolor{keywordflow}{return} [data]
133 
134     \textcolor{comment}{# stolen shamelessly from fairseq}
135     \textcolor{comment}{#
       https://github.com/pytorch/fairseq/blob/c37250ab1c845919af721cd3f5c4cec2993aefe1/fairseq/distributed\_utils.py#L116-L170}
136     rank = dist.get\_rank()
137     world\_size = dist.get\_world\_size()
138 
139     buffer\_size = max\_size * world\_size
140     \textcolor{keywordflow}{if} (
141         \textcolor{keywordflow}{not} hasattr(all\_gather\_list, \textcolor{stringliteral}{'\_buffer'})
142         \textcolor{keywordflow}{or} all\_gather\_list.\_buffer.numel() < buffer\_size
143     ):
144         all\_gather\_list.\_buffer = torch.cuda.ByteTensor(buffer\_size)
145 
146     buffer = all\_gather\_list.\_buffer
147     buffer.zero\_()
148 
149     enc = pickle.dumps(data)
150     enc\_size = len(enc)
151     \textcolor{keywordflow}{if} enc\_size + 2 > max\_size:
152         \textcolor{keywordflow}{raise} ValueError(\textcolor{stringliteral}{'encoded data exceeds max\_size: \{\}'}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(enc\_size + 2))
153     \textcolor{keyword}{assert} max\_size < 255 * 256
154 
155     buffer\_rank = buffer[rank * max\_size : (rank + 1) * max\_size]
156     buffer\_rank[0] = enc\_size // 255  \textcolor{comment}{# this encoding works for max\_size < 65k}
157     buffer\_rank[1] = enc\_size % 255
158     buffer\_rank[2 : enc\_size + 2] = torch.ByteTensor(list(enc))
159 
160     dist.all\_reduce(buffer)
161 
162     result = []
163     \textcolor{keywordflow}{for} i \textcolor{keywordflow}{in} range(world\_size):
164         out\_buffer = buffer[i * max\_size : (i + 1) * max\_size]
165         size = (255 * out\_buffer[0].item()) + out\_buffer[1].item()
166         \textcolor{keywordflow}{if} size > 0:
167             \textcolor{keywordflow}{try}:
168                 result.append(pickle.loads(bytes(out\_buffer[2 : size + 2].tolist())))
169             \textcolor{keywordflow}{except} pickle.UnpicklingError:
170                 \textcolor{keywordflow}{raise} RuntimeError(
171                     \textcolor{stringliteral}{'There was an unpickling error in all\_gather\_list. This likely '}
172                     \textcolor{stringliteral}{'means your workers got out of syncronization (e.g. one is '}
173                     \textcolor{stringliteral}{'expecting to sync and another is not.)'}
174                 )
175 
176     \textcolor{keywordflow}{return} result
177 
178 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}\label{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!is\+\_\+distributed@{is\+\_\+distributed}}
\index{is\+\_\+distributed@{is\+\_\+distributed}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{is\+\_\+distributed()}{is\_distributed()}}
{\footnotesize\ttfamily def parlai.\+utils.\+distributed.\+is\+\_\+distributed (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}

\begin{DoxyVerb}Return if we are in distributed mode.
\end{DoxyVerb}
 

Definition at line 55 of file distributed.\+py.


\begin{DoxyCode}
55 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}{is\_distributed}():
56     \textcolor{stringliteral}{"""}
57 \textcolor{stringliteral}{    Return if we are in distributed mode.}
58 \textcolor{stringliteral}{    """}
59     \textcolor{keywordflow}{return} TORCH\_AVAILABLE \textcolor{keywordflow}{and} dist.is\_available() \textcolor{keywordflow}{and} dist.is\_initialized()
60 
61 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_a9bb1dac198180590ef8c6b6c6f9fc2c4}\label{namespaceparlai_1_1utils_1_1distributed_a9bb1dac198180590ef8c6b6c6f9fc2c4}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!is\+\_\+primary\+\_\+worker@{is\+\_\+primary\+\_\+worker}}
\index{is\+\_\+primary\+\_\+worker@{is\+\_\+primary\+\_\+worker}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{is\+\_\+primary\+\_\+worker()}{is\_primary\_worker()}}
{\footnotesize\ttfamily def parlai.\+utils.\+distributed.\+is\+\_\+primary\+\_\+worker (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}

\begin{DoxyVerb}Determine if we are the primary (master) worker.

Returns False if we are a secondary worker. Returns True if we are either (1) not in
distributed mode (2) or are the primary (rank 0) worker.
\end{DoxyVerb}
 

Definition at line 72 of file distributed.\+py.


\begin{DoxyCode}
72 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1utils_1_1distributed_a9bb1dac198180590ef8c6b6c6f9fc2c4}{is\_primary\_worker}():
73     \textcolor{stringliteral}{"""}
74 \textcolor{stringliteral}{    Determine if we are the primary (master) worker.}
75 \textcolor{stringliteral}{}
76 \textcolor{stringliteral}{    Returns False if we are a secondary worker. Returns True if we are either (1) not in}
77 \textcolor{stringliteral}{    distributed mode (2) or are the primary (rank 0) worker.}
78 \textcolor{stringliteral}{    """}
79     \textcolor{keywordflow}{return} \textcolor{keywordflow}{not} \hyperlink{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}{is\_distributed}() \textcolor{keywordflow}{or} dist.get\_rank() == 0
80 
81 
82 @contextlib.contextmanager
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_a99b61b4756577c6542039c238d670dba}\label{namespaceparlai_1_1utils_1_1distributed_a99b61b4756577c6542039c238d670dba}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!num\+\_\+workers@{num\+\_\+workers}}
\index{num\+\_\+workers@{num\+\_\+workers}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{num\+\_\+workers()}{num\_workers()}}
{\footnotesize\ttfamily def parlai.\+utils.\+distributed.\+num\+\_\+workers (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}

\begin{DoxyVerb}Get the total number of workers.
\end{DoxyVerb}
 

Definition at line 62 of file distributed.\+py.


\begin{DoxyCode}
62 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1utils_1_1distributed_a99b61b4756577c6542039c238d670dba}{num\_workers}():
63     \textcolor{stringliteral}{"""}
64 \textcolor{stringliteral}{    Get the total number of workers.}
65 \textcolor{stringliteral}{    """}
66     \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \hyperlink{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}{is\_distributed}():
67         \textcolor{keywordflow}{return} 1
68     \textcolor{keywordflow}{else}:
69         \textcolor{keywordflow}{return} dist.get\_world\_size()
70 
71 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_a601345e55fc3fc3845c43efa1a04db56}\label{namespaceparlai_1_1utils_1_1distributed_a601345e55fc3fc3845c43efa1a04db56}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!override\+\_\+print@{override\+\_\+print}}
\index{override\+\_\+print@{override\+\_\+print}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{override\+\_\+print()}{override\_print()}}
{\footnotesize\ttfamily def parlai.\+utils.\+distributed.\+override\+\_\+print (\begin{DoxyParamCaption}\item[{}]{suppress = {\ttfamily False},  }\item[{}]{prefix = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Context manager to override the print to suppress or modify output. Recommended
usage is to call this with suppress=True for all non-primary workers, or call with a
prefix of rank on all workers.

>>> with override_print(prefix="rank{}".format(rank)):
...     my_computation()
:param bool suppress:
    if true, all future print statements are noops.
:param str prefix:
    if not None, this string is prefixed to all future print statements.
\end{DoxyVerb}
 

Definition at line 83 of file distributed.\+py.


\begin{DoxyCode}
83 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1utils_1_1distributed_a601345e55fc3fc3845c43efa1a04db56}{override\_print}(suppress=False, prefix=None):
84     \textcolor{stringliteral}{"""}
85 \textcolor{stringliteral}{    Context manager to override the print to suppress or modify output. Recommended}
86 \textcolor{stringliteral}{    usage is to call this with suppress=True for all non-primary workers, or call with a}
87 \textcolor{stringliteral}{    prefix of rank on all workers.}
88 \textcolor{stringliteral}{}
89 \textcolor{stringliteral}{    >>> with override\_print(prefix="rank\{\}".format(rank)):}
90 \textcolor{stringliteral}{    ...     my\_computation()}
91 \textcolor{stringliteral}{    :param bool suppress:}
92 \textcolor{stringliteral}{        if true, all future print statements are noops.}
93 \textcolor{stringliteral}{    :param str prefix:}
94 \textcolor{stringliteral}{        if not None, this string is prefixed to all future print statements.}
95 \textcolor{stringliteral}{    """}
96     builtin\_print = builtins.print
97 
98     \textcolor{keyword}{def }new\_print(*args, **kwargs):
99         \textcolor{keywordflow}{if} suppress:
100             \textcolor{comment}{# do nothing}
101             \textcolor{keywordflow}{return}
102         \textcolor{keywordflow}{elif} prefix:
103             \textcolor{keywordflow}{return} builtin\_print(prefix, *args, **kwargs)
104         \textcolor{keywordflow}{else}:
105             \textcolor{comment}{# default to normal print}
106             \textcolor{keywordflow}{return} builtin\_print(*args, **kwargs)
107 
108     \textcolor{comment}{# override the print for now}
109     builtins.print = new\_print
110     \textcolor{keywordflow}{yield}
111     \textcolor{comment}{# bring it back at the end of the context}
112     builtins.print = builtin\_print
113 
114 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_a269d3c7284127d9b287b9b61e9161de6}\label{namespaceparlai_1_1utils_1_1distributed_a269d3c7284127d9b287b9b61e9161de6}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!sync\+\_\+object@{sync\+\_\+object}}
\index{sync\+\_\+object@{sync\+\_\+object}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{sync\+\_\+object()}{sync\_object()}}
{\footnotesize\ttfamily def parlai.\+utils.\+distributed.\+sync\+\_\+object (\begin{DoxyParamCaption}\item[{}]{data,  }\item[{}]{max\+\_\+size = {\ttfamily 16384} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Sync an object among all workers.

All workers will return the same value for `data` when returning from this
method, always using the primary worker's version. Useful for ensuring control
flow decisions are made the same.

:param object data:
    The object to synchronize. Must be pickleable.
:param int max_size:
    The maximum size of this object in bytes. Large values than 255^2 are not
    supported.

:return: the synchronized data
\end{DoxyVerb}
 

Definition at line 179 of file distributed.\+py.


\begin{DoxyCode}
179 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1utils_1_1distributed_a269d3c7284127d9b287b9b61e9161de6}{sync\_object}(data, max\_size=16384):
180     \textcolor{stringliteral}{"""}
181 \textcolor{stringliteral}{    Sync an object among all workers.}
182 \textcolor{stringliteral}{}
183 \textcolor{stringliteral}{    All workers will return the same value for `data` when returning from this}
184 \textcolor{stringliteral}{    method, always using the primary worker's version. Useful for ensuring control}
185 \textcolor{stringliteral}{    flow decisions are made the same.}
186 \textcolor{stringliteral}{}
187 \textcolor{stringliteral}{    :param object data:}
188 \textcolor{stringliteral}{        The object to synchronize. Must be pickleable.}
189 \textcolor{stringliteral}{    :param int max\_size:}
190 \textcolor{stringliteral}{        The maximum size of this object in bytes. Large values than 255^2 are not}
191 \textcolor{stringliteral}{        supported.}
192 \textcolor{stringliteral}{}
193 \textcolor{stringliteral}{    :return: the synchronized data}
194 \textcolor{stringliteral}{    """}
195     \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \hyperlink{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}{is\_distributed}():
196         \textcolor{keywordflow}{return} data
197 
198     \textcolor{comment}{# prepare the buffer}
199     \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} hasattr(sync\_object, \textcolor{stringliteral}{'\_buffer'}) \textcolor{keywordflow}{or} sync\_object.\_buffer.numel() < max\_size:
200         \textcolor{comment}{# cuda is safe because distributed mode is only okay with CUDA}
201         sync\_object.\_buffer = torch.cuda.ByteTensor(max\_size)
202 
203     buffer = sync\_object.\_buffer
204 
205     \textcolor{keywordflow}{if} \hyperlink{namespaceparlai_1_1utils_1_1distributed_a9bb1dac198180590ef8c6b6c6f9fc2c4}{is\_primary\_worker}():
206         enc = pickle.dumps(data)
207         enc\_size = len(enc)
208         \textcolor{keywordflow}{if} (enc\_size + 2 > max\_size) \textcolor{keywordflow}{or} (enc\_size > 255 * 255):
209             \textcolor{comment}{# can't store the size in the first 2 bytes}
210             \textcolor{keywordflow}{raise} ValueError(\textcolor{stringliteral}{'encoded data exceeds max\_size'})
211 
212         buffer[0] = enc\_size // 255
213         buffer[1] = enc\_size % 255
214         buffer[2 : enc\_size + 2] = torch.ByteTensor(list(enc))
215 
216     dist.broadcast(buffer, 0)
217 
218     \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \hyperlink{namespaceparlai_1_1utils_1_1distributed_a9bb1dac198180590ef8c6b6c6f9fc2c4}{is\_primary\_worker}():
219         \textcolor{comment}{# deserialize the data}
220         enc\_size = buffer[0].item() * 255 + buffer[1].item()
221         \textcolor{keywordflow}{try}:
222             data = pickle.loads(bytes(buffer[2 : enc\_size + 2].tolist()))
223         \textcolor{keywordflow}{except} pickle.UnpicklingError:
224             \textcolor{keywordflow}{raise} RuntimeError(
225                 \textcolor{stringliteral}{'There was an unpickling error in sync\_object. This likely '}
226                 \textcolor{stringliteral}{'means your workers got out of syncronization (e.g. one is '}
227                 \textcolor{stringliteral}{'expecting to sync and another is not.)'}
228             )
229 
230     \textcolor{keywordflow}{return} data
231 
232 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_a47e6d93d849e487d7d3f6e1a83468073}\label{namespaceparlai_1_1utils_1_1distributed_a47e6d93d849e487d7d3f6e1a83468073}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!sync\+\_\+parameters@{sync\+\_\+parameters}}
\index{sync\+\_\+parameters@{sync\+\_\+parameters}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{sync\+\_\+parameters()}{sync\_parameters()}}
{\footnotesize\ttfamily def parlai.\+utils.\+distributed.\+sync\+\_\+parameters (\begin{DoxyParamCaption}\item[{}]{model }\end{DoxyParamCaption})}



Definition at line 233 of file distributed.\+py.


\begin{DoxyCode}
233 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1utils_1_1distributed_a47e6d93d849e487d7d3f6e1a83468073}{sync\_parameters}(model: torch.nn.Module) -> bool:
234     \textcolor{stringliteral}{"""}
235 \textcolor{stringliteral}{    Sync all parameters across all workers are the same.}
236 \textcolor{stringliteral}{}
237 \textcolor{stringliteral}{    Always returns True, or raises an AssertionError if there was a failure.}
238 \textcolor{stringliteral}{}
239 \textcolor{stringliteral}{    :param model: A pytorch model.}
240 \textcolor{stringliteral}{    :return: always True}
241 \textcolor{stringliteral}{    """}
242     \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \hyperlink{namespaceparlai_1_1utils_1_1distributed_a023acb5e3b66e1f27e21247c35661279}{is\_distributed}():
243         \textcolor{comment}{# if things aren't distributed, of course things are in sync}
244         \textcolor{keywordflow}{return} \textcolor{keyword}{True}
245 
246     \textcolor{comment}{# sync all the parameters}
247     with torch.no\_grad():
248         \textcolor{keywordflow}{for} p \textcolor{keywordflow}{in} model.parameters():
249             \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \hyperlink{namespaceparlai_1_1utils_1_1distributed_a9bb1dac198180590ef8c6b6c6f9fc2c4}{is\_primary\_worker}():
250                 \textcolor{comment}{# zero out parameters on all workers EXCEPT the primary worker}
251                 p.data.zero\_()
252             \textcolor{comment}{# sum the parameters across all workers, resulting in everyone having}
253             \textcolor{comment}{# the parameters of the primary worker}
254             dist.all\_reduce(p.data, dist.ReduceOp.SUM)
255 
256     \textcolor{comment}{# double check everything synced correctly}
257     norm2 = sum((p.data ** 2).sum().\hyperlink{namespaceprojects_1_1controllable__dialogue_1_1make__control__dataset_aa2b7207688c641dbc094ab44eca27113}{float}() \textcolor{keywordflow}{for} p \textcolor{keywordflow}{in} model.parameters()).item()
258     all\_versions = \hyperlink{namespaceparlai_1_1utils_1_1distributed_a9ef5bf0debf512ddbeb045327bfd87ef}{all\_gather\_list}(norm2)
259     \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} all(n == norm2 \textcolor{keywordflow}{for} n \textcolor{keywordflow}{in} all\_versions):
260         \textcolor{keywordflow}{raise} AssertionError(
261             \textcolor{stringliteral}{"Some models parameters were out of sync. Got the following norms: \{\}"}.
      \hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(
262                 \textcolor{stringliteral}{" "}.join(\hyperlink{namespacegenerate__task__READMEs_a5b88452ffb87b78c8c85ececebafc09f}{str}(x) \textcolor{keywordflow}{for} x \textcolor{keywordflow}{in} all\_versions)
263             )
264         )
265 
266     \textcolor{keywordflow}{return} \textcolor{keyword}{True}
267 \end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_afd854992e4cc6571b120b9e179cd4a7a}\label{namespaceparlai_1_1utils_1_1distributed_afd854992e4cc6571b120b9e179cd4a7a}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!validate\+\_\+params@{validate\+\_\+params}}
\index{validate\+\_\+params@{validate\+\_\+params}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{validate\+\_\+params()}{validate\_params()}}
{\footnotesize\ttfamily def parlai.\+utils.\+distributed.\+validate\+\_\+params (\begin{DoxyParamCaption}\item[{}]{opt }\end{DoxyParamCaption})}

\begin{DoxyVerb}Ensure sane combinations of command line parameters for distributed training.

Raises exceptions if anything is wrong, otherwise returns None.
\end{DoxyVerb}
 

Definition at line 29 of file distributed.\+py.


\begin{DoxyCode}
29 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1utils_1_1distributed_afd854992e4cc6571b120b9e179cd4a7a}{validate\_params}(opt):
30     \textcolor{stringliteral}{"""}
31 \textcolor{stringliteral}{    Ensure sane combinations of command line parameters for distributed training.}
32 \textcolor{stringliteral}{}
33 \textcolor{stringliteral}{    Raises exceptions if anything is wrong, otherwise returns None.}
34 \textcolor{stringliteral}{    """}
35     \textcolor{keywordflow}{if} torch.version.\_\_version\_\_.startswith(\textcolor{stringliteral}{'0.'}):
36         \textcolor{keywordflow}{raise} ImportError(
37             \textcolor{stringliteral}{"Please upgrade to PyTorch >=1.0; "}
38             \textcolor{stringliteral}{"visit https://pytorch.org for instructions."}
39         )
40 
41     \textcolor{keywordflow}{if} opt.get(\textcolor{stringliteral}{'no\_cuda'}, \textcolor{keyword}{False}):
42         \textcolor{keywordflow}{raise} ValueError(\textcolor{stringliteral}{'Distributed mode only makes sense when using GPUs.'})
43 
44     \textcolor{keywordflow}{if} opt.get(\textcolor{stringliteral}{'numthreads'}, 1) != 1:
45         \textcolor{keywordflow}{raise} ValueError(\textcolor{stringliteral}{'--numthreads must be 1 for distributed training.'})
46 
47     \textcolor{keywordflow}{if} \textcolor{stringliteral}{'train:stream'} \textcolor{keywordflow}{in} opt[\textcolor{stringliteral}{'datatype'}] \textcolor{keywordflow}{or} \textcolor{stringliteral}{'ordered'} \textcolor{keywordflow}{in} opt[\textcolor{stringliteral}{'datatype'}]:
48         \textcolor{keywordflow}{raise} ValueError(
49             \textcolor{stringliteral}{"You should not combine ordered streaming with distributed training "}
50             \textcolor{stringliteral}{"because all workers will have exactly the same minibatches, "}
51             \textcolor{stringliteral}{"defeating the purpose."}
52         )
53 
54 
\end{DoxyCode}


\subsection{Variable Documentation}
\mbox{\Hypertarget{namespaceparlai_1_1utils_1_1distributed_af99c57e9c67ff88f4802625436fc2c6c}\label{namespaceparlai_1_1utils_1_1distributed_af99c57e9c67ff88f4802625436fc2c6c}} 
\index{parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}!T\+O\+R\+C\+H\+\_\+\+A\+V\+A\+I\+L\+A\+B\+LE@{T\+O\+R\+C\+H\+\_\+\+A\+V\+A\+I\+L\+A\+B\+LE}}
\index{T\+O\+R\+C\+H\+\_\+\+A\+V\+A\+I\+L\+A\+B\+LE@{T\+O\+R\+C\+H\+\_\+\+A\+V\+A\+I\+L\+A\+B\+LE}!parlai\+::utils\+::distributed@{parlai\+::utils\+::distributed}}
\subsubsection{\texorpdfstring{T\+O\+R\+C\+H\+\_\+\+A\+V\+A\+I\+L\+A\+B\+LE}{TORCH\_AVAILABLE}}
{\footnotesize\ttfamily parlai.\+utils.\+distributed.\+T\+O\+R\+C\+H\+\_\+\+A\+V\+A\+I\+L\+A\+B\+LE}



Definition at line 24 of file distributed.\+py.

