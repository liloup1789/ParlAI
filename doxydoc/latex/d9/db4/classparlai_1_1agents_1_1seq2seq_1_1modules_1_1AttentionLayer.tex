\hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer}{}\section{parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer Class Reference}
\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer}\index{parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer@{parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer}}


Inheritance diagram for parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=235pt]{d6/d96/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=235pt]{d9/d2b/classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_aca18372fe25dbf5132ebcb2e63de31e3}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, attn\+\_\+type, hiddensize, embeddingsize, bidirectional=False, attn\+\_\+length=-\/1, attn\+\_\+time=\textquotesingle{}pre\textquotesingle{})
\item 
def \hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a1c840562c4d3d0a715b4d5ddb1b8bafe}{forward} (self, xes, hidden, attn\+\_\+params)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a8582ed1bb09a75d9a54e812a1b4ae648}{attention}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a38e130fe29ef52f03f78ba3c9a2ce4b9}{attn\+\_\+combine}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_ac886c3ee00cb9163fed352af8aa53e0c}{max\+\_\+length}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a5bbb3d93ee56f0a41562141beb302da7}{attn}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a2b5f04f72da768b45739b7e9f1dd6705}{attn\+\_\+v}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Computes attention between hidden and encoder states.

See arxiv.org/abs/1508.04025 for more info on each attention type.
\end{DoxyVerb}
 

Definition at line 576 of file modules.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_aca18372fe25dbf5132ebcb2e63de31e3}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_aca18372fe25dbf5132ebcb2e63de31e3}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{attn\+\_\+type,  }\item[{}]{hiddensize,  }\item[{}]{embeddingsize,  }\item[{}]{bidirectional = {\ttfamily False},  }\item[{}]{attn\+\_\+length = {\ttfamily -\/1},  }\item[{}]{attn\+\_\+time = {\ttfamily \textquotesingle{}pre\textquotesingle{}} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Initialize attention layer.
\end{DoxyVerb}
 

Definition at line 591 of file modules.\+py.


\begin{DoxyCode}
591     ):
592         \textcolor{stringliteral}{"""}
593 \textcolor{stringliteral}{        Initialize attention layer.}
594 \textcolor{stringliteral}{        """}
595         super().\_\_init\_\_()
596         self.attention = attn\_type
597 
598         \textcolor{keywordflow}{if} self.attention != \textcolor{stringliteral}{'none'}:
599             hsz = hiddensize
600             hszXdirs = hsz * (2 \textcolor{keywordflow}{if} bidirectional \textcolor{keywordflow}{else} 1)
601             \textcolor{keywordflow}{if} attn\_time == \textcolor{stringliteral}{'pre'}:
602                 \textcolor{comment}{# attention happens on the input embeddings}
603                 input\_dim = embeddingsize
604             \textcolor{keywordflow}{elif} attn\_time == \textcolor{stringliteral}{'post'}:
605                 \textcolor{comment}{# attention happens on the output of the rnn}
606                 input\_dim = hsz
607             \textcolor{keywordflow}{else}:
608                 \textcolor{keywordflow}{raise} RuntimeError(\textcolor{stringliteral}{'unsupported attention time'})
609 
610             \textcolor{comment}{# linear layer for combining applied attention weights with input}
611             self.attn\_combine = nn.Linear(hszXdirs + input\_dim, input\_dim, bias=\textcolor{keyword}{False})
612 
613             \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'local'}:
614                 \textcolor{comment}{# local attention over fixed set of output states}
615                 \textcolor{keywordflow}{if} attn\_length < 0:
616                     \textcolor{keywordflow}{raise} RuntimeError(\textcolor{stringliteral}{'Set attention length to > 0.'})
617                 self.max\_length = attn\_length
618                 \textcolor{comment}{# combines input and previous hidden output layer}
619                 self.attn = nn.Linear(hsz + input\_dim, attn\_length, bias=\textcolor{keyword}{False})
620                 \textcolor{comment}{# combines attention weights with encoder outputs}
621             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'concat'}:
622                 self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=\textcolor{keyword}{False})
623                 self.attn\_v = nn.Linear(hsz, 1, bias=\textcolor{keyword}{False})
624             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'general'}:
625                 \textcolor{comment}{# equivalent to dot if attn is identity}
626                 self.attn = nn.Linear(hsz, hszXdirs, bias=\textcolor{keyword}{False})
627 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a1c840562c4d3d0a715b4d5ddb1b8bafe}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a1c840562c4d3d0a715b4d5ddb1b8bafe}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!forward@{forward}}
\index{forward@{forward}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{xes,  }\item[{}]{hidden,  }\item[{}]{attn\+\_\+params }\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute attention over attn_params given input and hidden states.

:param xes:         input state. will be combined with applied
            attention.
:param hidden:      hidden state from model. will be used to select
            states to attend to in from the attn_params.
:param attn_params: tuple of encoder output states and a mask showing
            which input indices are nonzero.

:returns: output, attn_weights
  output is a new state of same size as input state `xes`.
  attn_weights are the weights given to each state in the
  encoder outputs.
\end{DoxyVerb}
 

Definition at line 628 of file modules.\+py.


\begin{DoxyCode}
628     \textcolor{keyword}{def }forward(self, xes, hidden, attn\_params):
629         \textcolor{stringliteral}{"""}
630 \textcolor{stringliteral}{        Compute attention over attn\_params given input and hidden states.}
631 \textcolor{stringliteral}{}
632 \textcolor{stringliteral}{        :param xes:         input state. will be combined with applied}
633 \textcolor{stringliteral}{                            attention.}
634 \textcolor{stringliteral}{        :param hidden:      hidden state from model. will be used to select}
635 \textcolor{stringliteral}{                            states to attend to in from the attn\_params.}
636 \textcolor{stringliteral}{        :param attn\_params: tuple of encoder output states and a mask showing}
637 \textcolor{stringliteral}{                            which input indices are nonzero.}
638 \textcolor{stringliteral}{}
639 \textcolor{stringliteral}{        :returns: output, attn\_weights}
640 \textcolor{stringliteral}{                  output is a new state of same size as input state `xes`.}
641 \textcolor{stringliteral}{                  attn\_weights are the weights given to each state in the}
642 \textcolor{stringliteral}{                  encoder outputs.}
643 \textcolor{stringliteral}{        """}
644         \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'none'}:
645             \textcolor{comment}{# do nothing, no attention}
646             \textcolor{keywordflow}{return} xes, \textcolor{keywordtype}{None}
647 
648         \textcolor{keywordflow}{if} \hyperlink{namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf_ad5dfae268e23f506da084a9efb72f619}{type}(hidden) == tuple:
649             \textcolor{comment}{# for lstms use the "hidden" state not the cell state}
650             hidden = hidden[0]
651         last\_hidden = hidden[-1]  \textcolor{comment}{# select hidden state from last RNN layer}
652 
653         enc\_out, attn\_mask = attn\_params
654         bsz, seqlen, hszXnumdir = enc\_out.size()
655         numlayersXnumdir = last\_hidden.size(1)
656 
657         \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'local'}:
658             \textcolor{comment}{# local attention weights aren't based on encoder states}
659             h\_merged = torch.cat((xes.squeeze(1), last\_hidden), 1)
660             attn\_weights = F.softmax(self.attn(h\_merged), dim=1)
661 
662             \textcolor{comment}{# adjust state sizes to the fixed window size}
663             \textcolor{keywordflow}{if} seqlen > self.max\_length:
664                 offset = seqlen - self.max\_length
665                 enc\_out = enc\_out.narrow(1, offset, self.max\_length)
666                 seqlen = self.max\_length
667             \textcolor{keywordflow}{if} attn\_weights.size(1) > seqlen:
668                 attn\_weights = attn\_weights.narrow(1, 0, seqlen)
669         \textcolor{keywordflow}{else}:
670             hid = last\_hidden.unsqueeze(1)
671             \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'concat'}:
672                 \textcolor{comment}{# concat hidden state and encoder outputs}
673                 hid = hid.expand(bsz, seqlen, numlayersXnumdir)
674                 h\_merged = torch.cat((enc\_out, hid), 2)
675                 \textcolor{comment}{# then do linear combination of them with activation}
676                 active = F.tanh(self.attn(h\_merged))
677                 attn\_w\_premask = self.attn\_v(active).squeeze(2)
678             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'dot'}:
679                 \textcolor{comment}{# dot product between hidden and encoder outputs}
680                 \textcolor{keywordflow}{if} numlayersXnumdir != hszXnumdir:
681                     \textcolor{comment}{# enc\_out has two directions, so double hid}
682                     hid = torch.cat([hid, hid], 2)
683                 enc\_t = enc\_out.transpose(1, 2)
684                 attn\_w\_premask = torch.bmm(hid, enc\_t).squeeze(1)
685             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'general'}:
686                 \textcolor{comment}{# before doing dot product, transform hidden state with linear}
687                 \textcolor{comment}{# same as dot if linear is identity}
688                 hid = self.attn(hid)
689                 enc\_t = enc\_out.transpose(1, 2)
690                 attn\_w\_premask = torch.bmm(hid, enc\_t).squeeze(1)
691 
692             \textcolor{comment}{# calculate activation scores, apply mask if needed}
693             \textcolor{keywordflow}{if} attn\_mask \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
694                 \textcolor{comment}{# remove activation from NULL symbols}
695                 attn\_w\_premask.masked\_fill\_((~attn\_mask), -NEAR\_INF)
696             attn\_weights = F.softmax(attn\_w\_premask, dim=1)
697 
698         \textcolor{comment}{# apply the attention weights to the encoder states}
699         attn\_applied = torch.bmm(attn\_weights.unsqueeze(1), enc\_out)
700         \textcolor{comment}{# concatenate the input and encoder states}
701         merged = torch.cat((xes.squeeze(1), attn\_applied.squeeze(1)), 1)
702         \textcolor{comment}{# combine them with a linear layer and tanh activation}
703         output = torch.tanh(self.attn\_combine(merged).unsqueeze(1))
704 
705         \textcolor{keywordflow}{return} output, attn\_weights
706 \end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a8582ed1bb09a75d9a54e812a1b4ae648}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a8582ed1bb09a75d9a54e812a1b4ae648}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!attention@{attention}}
\index{attention@{attention}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attention}{attention}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attention}



Definition at line 596 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a5bbb3d93ee56f0a41562141beb302da7}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a5bbb3d93ee56f0a41562141beb302da7}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!attn@{attn}}
\index{attn@{attn}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn}{attn}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn}



Definition at line 619 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a38e130fe29ef52f03f78ba3c9a2ce4b9}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a38e130fe29ef52f03f78ba3c9a2ce4b9}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!attn\+\_\+combine@{attn\+\_\+combine}}
\index{attn\+\_\+combine@{attn\+\_\+combine}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn\+\_\+combine}{attn\_combine}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn\+\_\+combine}



Definition at line 611 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a2b5f04f72da768b45739b7e9f1dd6705}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a2b5f04f72da768b45739b7e9f1dd6705}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!attn\+\_\+v@{attn\+\_\+v}}
\index{attn\+\_\+v@{attn\+\_\+v}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn\+\_\+v}{attn\_v}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn\+\_\+v}



Definition at line 623 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_ac886c3ee00cb9163fed352af8aa53e0c}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_ac886c3ee00cb9163fed352af8aa53e0c}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!max\+\_\+length@{max\+\_\+length}}
\index{max\+\_\+length@{max\+\_\+length}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{max\+\_\+length}{max\_length}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+max\+\_\+length}



Definition at line 617 of file modules.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/seq2seq/\hyperlink{parlai_2agents_2seq2seq_2modules_8py}{modules.\+py}\end{DoxyCompactItemize}
