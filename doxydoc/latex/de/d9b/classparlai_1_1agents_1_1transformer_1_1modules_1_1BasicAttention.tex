\hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention}{}\section{parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention Class Reference}
\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention}\index{parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention@{parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention}}


Inheritance diagram for parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=263pt]{df/d42/classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=247pt]{d2/dd8/classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_afa25129ca6ce9851053205f44f090fd6}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_ad0beb8f8b3e4514139b08555cf739d55}{dim}=1, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a04d74d5efbfdf47d36aff30331775368}{attn}=\textquotesingle{}\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_abed26e42fa2293f14cbbcf51c090794c}{cosine}\textquotesingle{}, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a98c0f263db25cf305c333e04f8a0843d}{residual}=False, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a98d54b38fac0a88d46eba7adf6ac542c}{get\+\_\+weights}=True)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_adb18b6c2564672c1820b3c72583dc0c0}{forward} (self, xs, ys, mask\+\_\+ys=None, values=None)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a669fa3f4cd988703371fc127a4943f07}{softmax}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_abed26e42fa2293f14cbbcf51c090794c}{cosine}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a04d74d5efbfdf47d36aff30331775368}{attn}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_ad0beb8f8b3e4514139b08555cf739d55}{dim}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a98d54b38fac0a88d46eba7adf6ac542c}{get\+\_\+weights}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a98c0f263db25cf305c333e04f8a0843d}{residual}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Implements simple/classical attention.
\end{DoxyVerb}
 

Definition at line 882 of file modules.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_afa25129ca6ce9851053205f44f090fd6}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_afa25129ca6ce9851053205f44f090fd6}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{dim = {\ttfamily 1},  }\item[{}]{attn = {\ttfamily \textquotesingle{}\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_abed26e42fa2293f14cbbcf51c090794c}{cosine}\textquotesingle{}},  }\item[{}]{residual = {\ttfamily False},  }\item[{}]{get\+\_\+weights = {\ttfamily True} }\end{DoxyParamCaption})}



Definition at line 887 of file modules.\+py.


\begin{DoxyCode}
887     \textcolor{keyword}{def }\_\_init\_\_(self, dim=1, attn='cosine', residual=False, get\_weights=True):
888         super().\_\_init\_\_()
889         self.softmax = nn.Softmax(dim=dim)
890         \textcolor{keywordflow}{if} attn == \textcolor{stringliteral}{'cosine'}:
891             self.cosine = nn.CosineSimilarity(dim=dim)
892         self.attn = attn
893         self.dim = dim
894         self.get\_weights = get\_weights
895         self.residual = residual
896 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_adb18b6c2564672c1820b3c72583dc0c0}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_adb18b6c2564672c1820b3c72583dc0c0}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}!forward@{forward}}
\index{forward@{forward}!parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{xs,  }\item[{}]{ys,  }\item[{}]{mask\+\_\+ys = {\ttfamily None},  }\item[{}]{values = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute attention.

Attend over ys with query xs to obtain weights, then apply weights to
values (ys if yalues is None)

Args:
    xs: B x query_len x dim (queries)
    ys: B x key_len x dim (keys)
    mask_ys: B x key_len (mask)
    values: B x value_len x dim (values); if None, default to ys
\end{DoxyVerb}
 

Definition at line 897 of file modules.\+py.


\begin{DoxyCode}
897     \textcolor{keyword}{def }forward(self, xs, ys, mask\_ys=None, values=None):
898         \textcolor{stringliteral}{"""}
899 \textcolor{stringliteral}{        Compute attention.}
900 \textcolor{stringliteral}{}
901 \textcolor{stringliteral}{        Attend over ys with query xs to obtain weights, then apply weights to}
902 \textcolor{stringliteral}{        values (ys if yalues is None)}
903 \textcolor{stringliteral}{}
904 \textcolor{stringliteral}{        Args:}
905 \textcolor{stringliteral}{            xs: B x query\_len x dim (queries)}
906 \textcolor{stringliteral}{            ys: B x key\_len x dim (keys)}
907 \textcolor{stringliteral}{            mask\_ys: B x key\_len (mask)}
908 \textcolor{stringliteral}{            values: B x value\_len x dim (values); if None, default to ys}
909 \textcolor{stringliteral}{        """}
910         bsz = xs.size(0)
911         y\_len = ys.size(1)
912         x\_len = xs.size(1)
913         \textcolor{keywordflow}{if} self.attn == \textcolor{stringliteral}{'cosine'}:
914             l1 = self.cosine(xs, ys).unsqueeze(self.dim - 1)
915         \textcolor{keywordflow}{else}:
916             l1 = torch.bmm(xs, ys.transpose(1, 2))
917             \textcolor{keywordflow}{if} self.attn == \textcolor{stringliteral}{'sqrt'}:
918                 d\_k = ys.size(-1)
919                 l1 = l1 / math.sqrt(d\_k)
920         \textcolor{keywordflow}{if} mask\_ys \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
921             attn\_mask = (mask\_ys == 0).view(bsz, 1, y\_len)
922             attn\_mask = attn\_mask.repeat(1, x\_len, 1)
923             l1.masked\_fill\_(attn\_mask, -\hyperlink{namespaceprojects_1_1controllable__dialogue_1_1make__control__dataset_aa2b7207688c641dbc094ab44eca27113}{float}(\textcolor{stringliteral}{'inf'}))
924         l2 = self.softmax(l1)
925         \textcolor{keywordflow}{if} values \textcolor{keywordflow}{is} \textcolor{keywordtype}{None}:
926             values = ys
927         lhs\_emb = torch.bmm(l2, values)
928 
929         \textcolor{comment}{# # add back the query}
930         \textcolor{keywordflow}{if} self.residual:
931             lhs\_emb = lhs\_emb.add(xs)
932 
933         \textcolor{keywordflow}{if} self.get\_weights:
934             \textcolor{keywordflow}{return} lhs\_emb.squeeze(self.dim - 1), l2
935         \textcolor{keywordflow}{else}:
936             \textcolor{keywordflow}{return} lhs\_emb.squeeze(self.dim - 1)
937 
938 
\end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a04d74d5efbfdf47d36aff30331775368}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a04d74d5efbfdf47d36aff30331775368}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}!attn@{attn}}
\index{attn@{attn}!parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}}
\subsubsection{\texorpdfstring{attn}{attn}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+attn}



Definition at line 892 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_abed26e42fa2293f14cbbcf51c090794c}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_abed26e42fa2293f14cbbcf51c090794c}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}!cosine@{cosine}}
\index{cosine@{cosine}!parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}}
\subsubsection{\texorpdfstring{cosine}{cosine}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+cosine}



Definition at line 891 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_ad0beb8f8b3e4514139b08555cf739d55}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_ad0beb8f8b3e4514139b08555cf739d55}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}!dim@{dim}}
\index{dim@{dim}!parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}}
\subsubsection{\texorpdfstring{dim}{dim}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+dim}



Definition at line 893 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a98d54b38fac0a88d46eba7adf6ac542c}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a98d54b38fac0a88d46eba7adf6ac542c}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}!get\+\_\+weights@{get\+\_\+weights}}
\index{get\+\_\+weights@{get\+\_\+weights}!parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}}
\subsubsection{\texorpdfstring{get\+\_\+weights}{get\_weights}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+get\+\_\+weights}



Definition at line 894 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a98c0f263db25cf305c333e04f8a0843d}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a98c0f263db25cf305c333e04f8a0843d}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}!residual@{residual}}
\index{residual@{residual}!parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}}
\subsubsection{\texorpdfstring{residual}{residual}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+residual}



Definition at line 895 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a669fa3f4cd988703371fc127a4943f07}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_a669fa3f4cd988703371fc127a4943f07}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}!softmax@{softmax}}
\index{softmax@{softmax}!parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention@{parlai\+::agents\+::transformer\+::modules\+::\+Basic\+Attention}}
\subsubsection{\texorpdfstring{softmax}{softmax}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+softmax}



Definition at line 889 of file modules.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/transformer/\hyperlink{parlai_2agents_2transformer_2modules_8py}{modules.\+py}\end{DoxyCompactItemize}
