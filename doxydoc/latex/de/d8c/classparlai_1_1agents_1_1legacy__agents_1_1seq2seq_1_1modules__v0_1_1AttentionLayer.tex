\hypertarget{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer}{}\section{parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer Class Reference}
\label{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer}\index{parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer@{parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer}}


Inheritance diagram for parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=211pt]{df/d1a/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=211pt]{dd/df8/classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a454f7b72defa9f2dc7e8efec1ab336eb}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, attn\+\_\+type, hidden\+\_\+size, emb\+\_\+size, bidirectional=False, attn\+\_\+length=-\/1, attn\+\_\+time=\textquotesingle{}pre\textquotesingle{})
\item 
def \hyperlink{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a6b5727eb84f3cab62adc1c647e23b98a}{forward} (self, xes, hidden, enc\+\_\+out, attn\+\_\+mask=None)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a0faa53163c14800b0deda68be978cd76}{attention}
\item 
\hyperlink{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_acd4f27aecfab6e05a026061bdabc42aa}{attn\+\_\+combine}
\item 
\hyperlink{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_ae7aed4fe106be5b39f7cc9027c5673f7}{max\+\_\+length}
\item 
\hyperlink{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_aedd5ce11eba4a46b40bc61a8bdad7e3d}{attn}
\item 
\hyperlink{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a4659f062cd7e92b2015f2cdf8558449d}{attn\+\_\+v}
\end{DoxyCompactItemize}


\subsection{Detailed Description}


Definition at line 724 of file modules\+\_\+v0.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a454f7b72defa9f2dc7e8efec1ab336eb}\label{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a454f7b72defa9f2dc7e8efec1ab336eb}} 
\index{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{attn\+\_\+type,  }\item[{}]{hidden\+\_\+size,  }\item[{}]{emb\+\_\+size,  }\item[{}]{bidirectional = {\ttfamily False},  }\item[{}]{attn\+\_\+length = {\ttfamily -\/1},  }\item[{}]{attn\+\_\+time = {\ttfamily \textquotesingle{}pre\textquotesingle{}} }\end{DoxyParamCaption})}



Definition at line 733 of file modules\+\_\+v0.\+py.


\begin{DoxyCode}
733     ):
734         super().\_\_init\_\_()
735         self.attention = attn\_type
736 
737         \textcolor{keywordflow}{if} self.attention != \textcolor{stringliteral}{'none'}:
738             hsz = hidden\_size
739             hszXdirs = hsz * (2 \textcolor{keywordflow}{if} bidirectional \textcolor{keywordflow}{else} 1)
740             \textcolor{keywordflow}{if} attn\_time == \textcolor{stringliteral}{'pre'}:
741                 \textcolor{comment}{# attention happens on the input embeddings}
742                 input\_dim = emb\_size
743             \textcolor{keywordflow}{elif} attn\_time == \textcolor{stringliteral}{'post'}:
744                 \textcolor{comment}{# attention happens on the output of the rnn}
745                 input\_dim = hsz
746             \textcolor{keywordflow}{else}:
747                 \textcolor{keywordflow}{raise} RuntimeError(\textcolor{stringliteral}{'unsupported attention time'})
748             self.attn\_combine = nn.Linear(hszXdirs + input\_dim, input\_dim, bias=\textcolor{keyword}{False})
749 
750             \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'local'}:
751                 \textcolor{comment}{# local attention over fixed set of output states}
752                 \textcolor{keywordflow}{if} attn\_length < 0:
753                     \textcolor{keywordflow}{raise} RuntimeError(\textcolor{stringliteral}{'Set attention length to > 0.'})
754                 self.max\_length = attn\_length
755                 \textcolor{comment}{# combines input and previous hidden output layer}
756                 self.attn = nn.Linear(hsz + input\_dim, attn\_length, bias=\textcolor{keyword}{False})
757                 \textcolor{comment}{# combines attention weights with encoder outputs}
758             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'concat'}:
759                 self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=\textcolor{keyword}{False})
760                 self.attn\_v = nn.Linear(hsz, 1, bias=\textcolor{keyword}{False})
761             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'general'}:
762                 \textcolor{comment}{# equivalent to dot if attn is identity}
763                 self.attn = nn.Linear(hsz, hszXdirs, bias=\textcolor{keyword}{False})
764 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a6b5727eb84f3cab62adc1c647e23b98a}\label{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a6b5727eb84f3cab62adc1c647e23b98a}} 
\index{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}!forward@{forward}}
\index{forward@{forward}!parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{xes,  }\item[{}]{hidden,  }\item[{}]{enc\+\_\+out,  }\item[{}]{attn\+\_\+mask = {\ttfamily None} }\end{DoxyParamCaption})}



Definition at line 765 of file modules\+\_\+v0.\+py.


\begin{DoxyCode}
765     \textcolor{keyword}{def }forward(self, xes, hidden, enc\_out, attn\_mask=None):
766         \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'none'}:
767             \textcolor{keywordflow}{return} xes
768 
769         \textcolor{keywordflow}{if} \hyperlink{namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf_ad5dfae268e23f506da084a9efb72f619}{type}(hidden) == tuple:
770             \textcolor{comment}{# for lstms use the "hidden" state not the cell state}
771             hidden = hidden[0]
772         last\_hidden = hidden[-1]  \textcolor{comment}{# select hidden state from last RNN layer}
773 
774         \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'local'}:
775             \textcolor{keywordflow}{if} enc\_out.size(1) > self.max\_length:
776                 offset = enc\_out.size(1) - self.max\_length
777                 enc\_out = enc\_out.narrow(1, offset, self.max\_length)
778             h\_merged = torch.cat((xes.squeeze(1), last\_hidden), 1)
779             attn\_weights = F.softmax(self.attn(h\_merged), dim=1)
780             \textcolor{keywordflow}{if} attn\_weights.size(1) > enc\_out.size(1):
781                 attn\_weights = attn\_weights.narrow(1, 0, enc\_out.size(1))
782         \textcolor{keywordflow}{else}:
783             hid = last\_hidden.unsqueeze(1)
784             \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'concat'}:
785                 hid = hid.expand(
786                     last\_hidden.size(0), enc\_out.size(1), last\_hidden.size(1)
787                 )
788                 h\_merged = torch.cat((enc\_out, hid), 2)
789                 active = F.tanh(self.attn(h\_merged))
790                 attn\_w\_premask = self.attn\_v(active).squeeze(2)
791             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'dot'}:
792                 \textcolor{keywordflow}{if} hid.size(2) != enc\_out.size(2):
793                     \textcolor{comment}{# enc\_out has two directions, so double hid}
794                     hid = torch.cat([hid, hid], 2)
795                 attn\_w\_premask = torch.bmm(hid, enc\_out.transpose(1, 2)).squeeze(1)
796             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'general'}:
797                 hid = self.attn(hid)
798                 attn\_w\_premask = torch.bmm(hid, enc\_out.transpose(1, 2)).squeeze(1)
799             \textcolor{comment}{# calculate activation scores}
800             \textcolor{keywordflow}{if} attn\_mask \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
801                 \textcolor{comment}{# remove activation from NULL symbols}
802                 attn\_w\_premask -= (1 - attn\_mask) * 1e20
803             attn\_weights = F.softmax(attn\_w\_premask, dim=1)
804 
805         attn\_applied = torch.bmm(attn\_weights.unsqueeze(1), enc\_out)
806         merged = torch.cat((xes.squeeze(1), attn\_applied.squeeze(1)), 1)
807         output = F.tanh(self.attn\_combine(merged).unsqueeze(1))
808         \textcolor{keywordflow}{return} output
809 \end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a0faa53163c14800b0deda68be978cd76}\label{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a0faa53163c14800b0deda68be978cd76}} 
\index{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}!attention@{attention}}
\index{attention@{attention}!parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attention}{attention}}
{\footnotesize\ttfamily parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+attention}



Definition at line 735 of file modules\+\_\+v0.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_aedd5ce11eba4a46b40bc61a8bdad7e3d}\label{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_aedd5ce11eba4a46b40bc61a8bdad7e3d}} 
\index{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}!attn@{attn}}
\index{attn@{attn}!parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn}{attn}}
{\footnotesize\ttfamily parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+attn}



Definition at line 756 of file modules\+\_\+v0.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_acd4f27aecfab6e05a026061bdabc42aa}\label{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_acd4f27aecfab6e05a026061bdabc42aa}} 
\index{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}!attn\+\_\+combine@{attn\+\_\+combine}}
\index{attn\+\_\+combine@{attn\+\_\+combine}!parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn\+\_\+combine}{attn\_combine}}
{\footnotesize\ttfamily parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+attn\+\_\+combine}



Definition at line 748 of file modules\+\_\+v0.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a4659f062cd7e92b2015f2cdf8558449d}\label{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_a4659f062cd7e92b2015f2cdf8558449d}} 
\index{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}!attn\+\_\+v@{attn\+\_\+v}}
\index{attn\+\_\+v@{attn\+\_\+v}!parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn\+\_\+v}{attn\_v}}
{\footnotesize\ttfamily parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+attn\+\_\+v}



Definition at line 760 of file modules\+\_\+v0.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_ae7aed4fe106be5b39f7cc9027c5673f7}\label{classparlai_1_1agents_1_1legacy__agents_1_1seq2seq_1_1modules__v0_1_1AttentionLayer_ae7aed4fe106be5b39f7cc9027c5673f7}} 
\index{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}!max\+\_\+length@{max\+\_\+length}}
\index{max\+\_\+length@{max\+\_\+length}!parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer@{parlai\+::agents\+::legacy\+\_\+agents\+::seq2seq\+::modules\+\_\+v0\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{max\+\_\+length}{max\_length}}
{\footnotesize\ttfamily parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+max\+\_\+length}



Definition at line 754 of file modules\+\_\+v0.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/legacy\+\_\+agents/seq2seq/\hyperlink{seq2seq_2modules__v0_8py}{modules\+\_\+v0.\+py}\end{DoxyCompactItemize}
