\hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train}{}\section{parlai.\+scripts.\+multiprocessing\+\_\+train Namespace Reference}
\label{namespaceparlai_1_1scripts_1_1multiprocessing__train}\index{parlai.\+scripts.\+multiprocessing\+\_\+train@{parlai.\+scripts.\+multiprocessing\+\_\+train}}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa979267c9eb44bbdfcd25a6d69a58cc4}{multiprocess\+\_\+train} (rank, opt, port=61337, rank\+\_\+offset=0, gpu=None, hostname=\textquotesingle{}localhost\textquotesingle{})
\item 
def \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}{launch\+\_\+and\+\_\+train} (opt, port)
\item 
def \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}{setup\+\_\+args} ()
\item 
def \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa7b2a133561ac5212f3ee9814a645522}{main} ()
\end{DoxyCompactItemize}


\subsection{Function Documentation}
\mbox{\Hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}\label{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}} 
\index{parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}!launch\+\_\+and\+\_\+train@{launch\+\_\+and\+\_\+train}}
\index{launch\+\_\+and\+\_\+train@{launch\+\_\+and\+\_\+train}!parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}}
\subsubsection{\texorpdfstring{launch\+\_\+and\+\_\+train()}{launch\_and\_train()}}
{\footnotesize\ttfamily def parlai.\+scripts.\+multiprocessing\+\_\+train.\+launch\+\_\+and\+\_\+train (\begin{DoxyParamCaption}\item[{}]{opt,  }\item[{}]{port }\end{DoxyParamCaption})}

\begin{DoxyVerb}Perform a fork() to many processes.
\end{DoxyVerb}
 

Definition at line 90 of file multiprocessing\+\_\+train.\+py.


\begin{DoxyCode}
90 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}{launch\_and\_train}(opt, port):
91     \textcolor{stringliteral}{"""}
92 \textcolor{stringliteral}{    Perform a fork() to many processes.}
93 \textcolor{stringliteral}{    """}
94     \textcolor{comment}{# Launch multiple subprocesses}
95     spawncontext = torch.multiprocessing.spawn(
96         multiprocess\_train,
97         \textcolor{comment}{# need to give rank offset as 1 to cover the fact that the main}
98         \textcolor{comment}{# process is rank 0, but that spawn() doesn't let you control rank}
99         (opt, port, 1),
100         nprocs=opt[\textcolor{stringliteral}{'distributed\_world\_size'}] - 1,  \textcolor{comment}{# main proc will also run loop}
101         join=\textcolor{keyword}{False},
102     )
103 
104     \textcolor{keywordflow}{try}:
105         retval = \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa979267c9eb44bbdfcd25a6d69a58cc4}{multiprocess\_train}(0, opt, port)
106         spawncontext.join()
107         \textcolor{keywordflow}{return} retval
108     \textcolor{keywordflow}{except} KeyboardInterrupt:
109         \textcolor{comment}{# tell the subprocesses to stop too}
110         \textcolor{keywordflow}{for} p \textcolor{keywordflow}{in} spawncontext.processes:
111             \textcolor{keywordflow}{if} p.is\_alive():
112                 os.kill(p.pid, signal.SIGINT)
113         \textcolor{keywordflow}{raise}
114 
115 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa7b2a133561ac5212f3ee9814a645522}\label{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa7b2a133561ac5212f3ee9814a645522}} 
\index{parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}!main@{main}}
\index{main@{main}!parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}}
\subsubsection{\texorpdfstring{main()}{main()}}
{\footnotesize\ttfamily def parlai.\+scripts.\+multiprocessing\+\_\+train.\+main (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Definition at line 123 of file multiprocessing\+\_\+train.\+py.


\begin{DoxyCode}
123 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa7b2a133561ac5212f3ee9814a645522}{main}():
124     opt = \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}{setup\_args}().parse\_args()
125     port = random.randint(32000, 48000)
126     \textcolor{keywordflow}{return} \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}{launch\_and\_train}(opt, port)
127 
128 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa979267c9eb44bbdfcd25a6d69a58cc4}\label{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa979267c9eb44bbdfcd25a6d69a58cc4}} 
\index{parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}!multiprocess\+\_\+train@{multiprocess\+\_\+train}}
\index{multiprocess\+\_\+train@{multiprocess\+\_\+train}!parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}}
\subsubsection{\texorpdfstring{multiprocess\+\_\+train()}{multiprocess\_train()}}
{\footnotesize\ttfamily def parlai.\+scripts.\+multiprocessing\+\_\+train.\+multiprocess\+\_\+train (\begin{DoxyParamCaption}\item[{}]{rank,  }\item[{}]{opt,  }\item[{}]{port = {\ttfamily 61337},  }\item[{}]{rank\+\_\+offset = {\ttfamily 0},  }\item[{}]{gpu = {\ttfamily None},  }\item[{}]{hostname = {\ttfamily \textquotesingle{}localhost\textquotesingle{}} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Subprocess which initializes distributed training, and begins training.

This should be launched n times for n GPUs; this is handled either in main
or via srun.

:param int rank: This process's rank - 1. (Starts at -1 ... n - 2). See comments.
:param opt: command line options
:param int port: A TCP port to use. This will need to be changed to run
    multiple distributed training setups on the same machine.
:param int gpu: Which GPU to use. Defaults to using rank and local devices,
    but must be manually specified when using many-hosts.
:param str hostname: Hostname of the main server.
\end{DoxyVerb}
 

Definition at line 31 of file multiprocessing\+\_\+train.\+py.


\begin{DoxyCode}
31 ):
32     \textcolor{stringliteral}{"""}
33 \textcolor{stringliteral}{    Subprocess which initializes distributed training, and begins training.}
34 \textcolor{stringliteral}{}
35 \textcolor{stringliteral}{    This should be launched n times for n GPUs; this is handled either in main}
36 \textcolor{stringliteral}{    or via srun.}
37 \textcolor{stringliteral}{}
38 \textcolor{stringliteral}{    :param int rank: This process's rank - 1. (Starts at -1 ... n - 2). See comments.}
39 \textcolor{stringliteral}{    :param opt: command line options}
40 \textcolor{stringliteral}{    :param int port: A TCP port to use. This will need to be changed to run}
41 \textcolor{stringliteral}{        multiple distributed training setups on the same machine.}
42 \textcolor{stringliteral}{    :param int gpu: Which GPU to use. Defaults to using rank and local devices,}
43 \textcolor{stringliteral}{        but must be manually specified when using many-hosts.}
44 \textcolor{stringliteral}{    :param str hostname: Hostname of the main server.}
45 \textcolor{stringliteral}{    """}
46     \textcolor{comment}{# Set per-host options}
47     opt = copy.deepcopy(opt)
48     \textcolor{comment}{# we need to manually adjust the rank differently in multiprocessing}
49     \textcolor{comment}{# and distributed train}
50     rank = rank + rank\_offset
51     opt[\textcolor{stringliteral}{'rank'}] = rank
52     \textcolor{keywordflow}{if} gpu \textcolor{keywordflow}{is} \textcolor{keywordtype}{None}:
53         \textcolor{comment}{# default assumption is local GPUs}
54         gpu = rank % torch.cuda.device\_count()
55     opt[\textcolor{stringliteral}{'gpu'}] = gpu
56     \textcolor{comment}{# make sure we don't just use whatever GPU was saved in the model file}
57     \textcolor{keywordflow}{if} \textcolor{stringliteral}{'override'} \textcolor{keywordflow}{not} \textcolor{keywordflow}{in} opt:
58         opt[\textcolor{stringliteral}{'override'}] = \{\}
59     opt[\textcolor{stringliteral}{'override'}][\textcolor{stringliteral}{'gpu'}] = gpu
60 
61     \textcolor{comment}{# Suppress output of workers except the main host.}
62     \textcolor{keywordflow}{if} opt.get(\textcolor{stringliteral}{'verbose'}) \textcolor{keywordflow}{or} rank != 0:
63         print\_prefix = \textcolor{stringliteral}{'[rank:\{:3d\}]'}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(rank)
64     \textcolor{keywordflow}{else}:
65         print\_prefix = \textcolor{keywordtype}{None}
66     suppress\_output = \textcolor{keywordflow}{not} opt.get(\textcolor{stringliteral}{'verbose'}) \textcolor{keywordflow}{and} rank != 0
67 
68     with distributed\_utils.override\_print(suppress\_output, print\_prefix):
69         \textcolor{comment}{# perform distributed setup, ensuring all hosts are ready}
70         torch.cuda.set\_device(opt[\textcolor{stringliteral}{'gpu'}])
71         dist.init\_process\_group(
72             backend=\textcolor{stringliteral}{"nccl"},
73             init\_method=\textcolor{stringliteral}{"tcp://\{\}:\{\}"}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(hostname, port),
74             world\_size=opt[\textcolor{stringliteral}{'distributed\_world\_size'}],
75             rank=rank,
76         )
77         print(\textcolor{stringliteral}{"Distributed group initialized"})
78 
79         \textcolor{comment}{# manual\_seed can be a noop without this}
80         torch.cuda.init()
81         \textcolor{comment}{# make sure all parameters will be in sync}
82         torch.manual\_seed(42)
83         \textcolor{comment}{# force a sync so that no one gets ahead, and all are seeded together}
84         distributed\_utils.sync\_object(\textcolor{keywordtype}{None})
85 
86         \textcolor{comment}{# Run the actual training}
87         \textcolor{keywordflow}{return} single\_train.TrainLoop(opt).\hyperlink{namespaceprojects_1_1mastering__the__dungeon_1_1mturk_1_1tasks_1_1MTD_1_1run_a36a5f4f6f9df0611a6818610518d2cf0}{train}()
88 
89 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}\label{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}} 
\index{parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}!setup\+\_\+args@{setup\+\_\+args}}
\index{setup\+\_\+args@{setup\+\_\+args}!parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}}
\subsubsection{\texorpdfstring{setup\+\_\+args()}{setup\_args()}}
{\footnotesize\ttfamily def parlai.\+scripts.\+multiprocessing\+\_\+train.\+setup\+\_\+args (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Definition at line 116 of file multiprocessing\+\_\+train.\+py.


\begin{DoxyCode}
116 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}{setup\_args}():
117     parser = single\_train.setup\_args()
118     parser.add\_distributed\_training\_args()
119     parser.set\_defaults(distributed\_world\_size=torch.cuda.device\_count())
120     \textcolor{keywordflow}{return} parser
121 
122 
\end{DoxyCode}
