\hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train}{}\section{parlai.\+scripts.\+multiprocessing\+\_\+train Namespace Reference}
\label{namespaceparlai_1_1scripts_1_1multiprocessing__train}\index{parlai.\+scripts.\+multiprocessing\+\_\+train@{parlai.\+scripts.\+multiprocessing\+\_\+train}}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa979267c9eb44bbdfcd25a6d69a58cc4}{multiprocess\+\_\+train} (rank, opt, port=61337, rank\+\_\+offset=0, gpu=None, hostname=\textquotesingle{}localhost\textquotesingle{})
\item 
def \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}{launch\+\_\+and\+\_\+train} (opt, port)
\item 
def \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}{setup\+\_\+args} ()
\item 
def \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa7b2a133561ac5212f3ee9814a645522}{main} ()
\end{DoxyCompactItemize}


\subsection{Function Documentation}
\mbox{\Hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}\label{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}} 
\index{parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}!launch\+\_\+and\+\_\+train@{launch\+\_\+and\+\_\+train}}
\index{launch\+\_\+and\+\_\+train@{launch\+\_\+and\+\_\+train}!parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}}
\subsubsection{\texorpdfstring{launch\+\_\+and\+\_\+train()}{launch\_and\_train()}}
{\footnotesize\ttfamily def parlai.\+scripts.\+multiprocessing\+\_\+train.\+launch\+\_\+and\+\_\+train (\begin{DoxyParamCaption}\item[{}]{opt,  }\item[{}]{port }\end{DoxyParamCaption})}

\begin{DoxyVerb}Perform a fork() to many processes.
\end{DoxyVerb}
 

Definition at line 97 of file multiprocessing\+\_\+train.\+py.


\begin{DoxyCode}
97 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}{launch\_and\_train}(opt, port):
98     \textcolor{stringliteral}{"""}
99 \textcolor{stringliteral}{    Perform a fork() to many processes.}
100 \textcolor{stringliteral}{    """}
101     \textcolor{comment}{# Launch multiple subprocesses}
102     spawncontext = torch.multiprocessing.spawn(
103         multiprocess\_train,
104         \textcolor{comment}{# need to give rank offset as 1 to cover the fact that the main}
105         \textcolor{comment}{# process is rank 0, but that spawn() doesn't let you control rank}
106         (opt, port, 1),
107         nprocs=opt[\textcolor{stringliteral}{'distributed\_world\_size'}] - 1,  \textcolor{comment}{# main proc will also run loop}
108         join=\textcolor{keyword}{False},
109     )
110 
111     \textcolor{keywordflow}{try}:
112         retval = \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa979267c9eb44bbdfcd25a6d69a58cc4}{multiprocess\_train}(0, opt, port)
113         spawncontext.join()
114         \textcolor{keywordflow}{return} retval
115     \textcolor{keywordflow}{except} KeyboardInterrupt:
116         \textcolor{comment}{# tell the subprocesses to stop too}
117         \textcolor{keywordflow}{for} p \textcolor{keywordflow}{in} spawncontext.processes:
118             \textcolor{keywordflow}{if} p.is\_alive():
119                 os.kill(p.pid, signal.SIGINT)
120         \textcolor{keywordflow}{raise}
121 
122 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa7b2a133561ac5212f3ee9814a645522}\label{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa7b2a133561ac5212f3ee9814a645522}} 
\index{parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}!main@{main}}
\index{main@{main}!parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}}
\subsubsection{\texorpdfstring{main()}{main()}}
{\footnotesize\ttfamily def parlai.\+scripts.\+multiprocessing\+\_\+train.\+main (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Definition at line 130 of file multiprocessing\+\_\+train.\+py.


\begin{DoxyCode}
130 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa7b2a133561ac5212f3ee9814a645522}{main}():
131     opt = \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}{setup\_args}().parse\_args()
132     port = random.randint(32000, 48000)
133     \textcolor{keywordflow}{return} \hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a5dc8df166f1c025b54f7420a1ab2f812}{launch\_and\_train}(opt, port)
134 
135 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa979267c9eb44bbdfcd25a6d69a58cc4}\label{namespaceparlai_1_1scripts_1_1multiprocessing__train_aa979267c9eb44bbdfcd25a6d69a58cc4}} 
\index{parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}!multiprocess\+\_\+train@{multiprocess\+\_\+train}}
\index{multiprocess\+\_\+train@{multiprocess\+\_\+train}!parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}}
\subsubsection{\texorpdfstring{multiprocess\+\_\+train()}{multiprocess\_train()}}
{\footnotesize\ttfamily def parlai.\+scripts.\+multiprocessing\+\_\+train.\+multiprocess\+\_\+train (\begin{DoxyParamCaption}\item[{}]{rank,  }\item[{}]{opt,  }\item[{}]{port = {\ttfamily 61337},  }\item[{}]{rank\+\_\+offset = {\ttfamily 0},  }\item[{}]{gpu = {\ttfamily None},  }\item[{}]{hostname = {\ttfamily \textquotesingle{}localhost\textquotesingle{}} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Subprocess which initializes distributed training, and begins training.

This should be launched n times for n GPUs; this is handled either in main
or via srun.

:param int rank: This process's rank - 1. (Starts at -1 ... n - 2). See comments.
:param opt: command line options
:param int port: A TCP port to use. This will need to be changed to run
    multiple distributed training setups on the same machine.
:param int gpu: Which GPU to use. Defaults to using rank and local devices,
    but must be manually specified when using many-hosts.
:param str hostname: Hostname of the main server.
\end{DoxyVerb}
 

Definition at line 38 of file multiprocessing\+\_\+train.\+py.


\begin{DoxyCode}
38 ):
39     \textcolor{stringliteral}{"""}
40 \textcolor{stringliteral}{    Subprocess which initializes distributed training, and begins training.}
41 \textcolor{stringliteral}{}
42 \textcolor{stringliteral}{    This should be launched n times for n GPUs; this is handled either in main}
43 \textcolor{stringliteral}{    or via srun.}
44 \textcolor{stringliteral}{}
45 \textcolor{stringliteral}{    :param int rank: This process's rank - 1. (Starts at -1 ... n - 2). See comments.}
46 \textcolor{stringliteral}{    :param opt: command line options}
47 \textcolor{stringliteral}{    :param int port: A TCP port to use. This will need to be changed to run}
48 \textcolor{stringliteral}{        multiple distributed training setups on the same machine.}
49 \textcolor{stringliteral}{    :param int gpu: Which GPU to use. Defaults to using rank and local devices,}
50 \textcolor{stringliteral}{        but must be manually specified when using many-hosts.}
51 \textcolor{stringliteral}{    :param str hostname: Hostname of the main server.}
52 \textcolor{stringliteral}{    """}
53     \textcolor{comment}{# Set per-host options}
54     opt = copy.deepcopy(opt)
55     \textcolor{comment}{# we need to manually adjust the rank differently in multiprocessing}
56     \textcolor{comment}{# and distributed train}
57     rank = rank + rank\_offset
58     opt[\textcolor{stringliteral}{'rank'}] = rank
59     \textcolor{keywordflow}{if} gpu \textcolor{keywordflow}{is} \textcolor{keywordtype}{None}:
60         \textcolor{comment}{# default assumption is local GPUs}
61         gpu = rank % torch.cuda.device\_count()
62     opt[\textcolor{stringliteral}{'gpu'}] = gpu
63     \textcolor{comment}{# make sure we don't just use whatever GPU was saved in the model file}
64     \textcolor{keywordflow}{if} \textcolor{stringliteral}{'override'} \textcolor{keywordflow}{not} \textcolor{keywordflow}{in} opt:
65         opt[\textcolor{stringliteral}{'override'}] = \{\}
66     opt[\textcolor{stringliteral}{'override'}][\textcolor{stringliteral}{'gpu'}] = gpu
67 
68     \textcolor{comment}{# Suppress output of workers except the main host.}
69     \textcolor{keywordflow}{if} opt.get(\textcolor{stringliteral}{'verbose'}) \textcolor{keywordflow}{or} rank != 0:
70         print\_prefix = \textcolor{stringliteral}{'[rank:\{:3d\}]'}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(rank)
71     \textcolor{keywordflow}{else}:
72         print\_prefix = \textcolor{keywordtype}{None}
73     suppress\_output = \textcolor{keywordflow}{not} opt.get(\textcolor{stringliteral}{'verbose'}) \textcolor{keywordflow}{and} rank != 0
74 
75     with distributed\_utils.override\_print(suppress\_output, print\_prefix):
76         \textcolor{comment}{# perform distributed setup, ensuring all hosts are ready}
77         torch.cuda.set\_device(opt[\textcolor{stringliteral}{'gpu'}])
78         dist.init\_process\_group(
79             backend=\textcolor{stringliteral}{"nccl"},
80             init\_method=\textcolor{stringliteral}{"tcp://\{\}:\{\}"}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(hostname, port),
81             world\_size=opt[\textcolor{stringliteral}{'distributed\_world\_size'}],
82             rank=rank,
83         )
84         print(\textcolor{stringliteral}{"Distributed group initialized"})
85 
86         \textcolor{comment}{# manual\_seed can be a noop without this}
87         torch.cuda.init()
88         \textcolor{comment}{# make sure all parameters will be in sync}
89         torch.manual\_seed(42)
90         \textcolor{comment}{# force a sync so that no one gets ahead, and all are seeded together}
91         distributed\_utils.sync\_object(\textcolor{keywordtype}{None})
92 
93         \textcolor{comment}{# Run the actual training}
94         \textcolor{keywordflow}{return} single\_train.TrainLoop(opt).\hyperlink{namespaceprojects_1_1mastering__the__dungeon_1_1mturk_1_1tasks_1_1MTD_1_1run_a36a5f4f6f9df0611a6818610518d2cf0}{train}()
95 
96 
\end{DoxyCode}
\mbox{\Hypertarget{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}\label{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}} 
\index{parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}!setup\+\_\+args@{setup\+\_\+args}}
\index{setup\+\_\+args@{setup\+\_\+args}!parlai\+::scripts\+::multiprocessing\+\_\+train@{parlai\+::scripts\+::multiprocessing\+\_\+train}}
\subsubsection{\texorpdfstring{setup\+\_\+args()}{setup\_args()}}
{\footnotesize\ttfamily def parlai.\+scripts.\+multiprocessing\+\_\+train.\+setup\+\_\+args (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Definition at line 123 of file multiprocessing\+\_\+train.\+py.


\begin{DoxyCode}
123 \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1scripts_1_1multiprocessing__train_a1ee26bddeb470040cfbceb5ee7a9fa08}{setup\_args}():
124     parser = single\_train.setup\_args()
125     parser.add\_distributed\_training\_args()
126     parser.set\_defaults(distributed\_world\_size=torch.cuda.device\_count())
127     \textcolor{keywordflow}{return} parser
128 
129 
\end{DoxyCode}
