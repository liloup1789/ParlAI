\hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule}{}\section{parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module Class Reference}
\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule}\index{parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module@{parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module}}


Inheritance diagram for parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=263pt]{d4/d08/classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=263pt]{d0/d15/classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acf6f1b0dd5025b20c330d08bd2a13755}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, opt, dict, \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}{null\+\_\+idx})
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acd213717123d2f2f24e6f03270c3db14}{get\+\_\+encoder} (self, opt, dict, \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}{null\+\_\+idx}, reduction\+\_\+type)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a21343cb181ee148326166a8e32e9e3d6}{attend} (self, attention\+\_\+layer, queries, keys, values, mask)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a54ca3cb24e2a0fbbfa7b3f145c7558ed}{encode} (self, ctxt\+\_\+tokens, cand\+\_\+tokens)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a52218a8c0594ff4d5bf8e666dc38f508}{score} (self, ctxt\+\_\+rep, ctxt\+\_\+rep\+\_\+mask, ctxt\+\_\+pos, cand\+\_\+embed)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9e769210d947e88dc3e4555583b9c82e}{forward} (self, ctxt\+\_\+tokens=None, cand\+\_\+tokens=None, ctxt\+\_\+rep=None, ctxt\+\_\+rep\+\_\+mask=None, ctxt\+\_\+pos=None, cand\+\_\+rep=None)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}{null\+\_\+idx}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9a5e58bfbd75ec89e10b8e4bcbcd4ab6}{encoder\+\_\+ctxt}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af70c94c4e4073e804904b983a683c0a7}{encoder\+\_\+cand}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a3eea152c32cd33d1e6b0ca680887d0e7}{type}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8440bd98b6ee31c0540fdbc2e9efe987}{n\+\_\+codes}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a2efc9276c4ab6d1533b7ab3ceef3ac3a}{attention\+\_\+type}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af29f6ff1656451ec4c35b4cc9ea0f5dc}{attention\+\_\+keys}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acc2312ac770d343991de882c05da56e6}{attention\+\_\+num\+\_\+heads}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a80cf22447d874c72d3e9b64a64cf5747}{codes\+\_\+attention\+\_\+type}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ae30ed526a2a4442222b6572a6fa41604}{codes\+\_\+attention\+\_\+num\+\_\+heads}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ac8d8955e72761c0775aeefea5ce80c3c}{codes}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a6740a4e891b964a41484a793ae0e6e6d}{code\+\_\+attention}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8fab02ef33d1e1ee413e4bcbe25aa03b}{attention}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Poly-encoder model.

See https://arxiv.org/abs/1905.01969 for more details
\end{DoxyVerb}
 

Definition at line 211 of file polyencoder.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acf6f1b0dd5025b20c330d08bd2a13755}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acf6f1b0dd5025b20c330d08bd2a13755}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{opt,  }\item[{}]{dict,  }\item[{}]{null\+\_\+idx }\end{DoxyParamCaption})}



Definition at line 218 of file polyencoder.\+py.


\begin{DoxyCode}
218     \textcolor{keyword}{def }\_\_init\_\_(self, opt, dict, null\_idx):
219         super(PolyEncoderModule, self).\_\_init\_\_()
220         self.null\_idx = null\_idx
221         self.encoder\_ctxt = self.get\_encoder(opt, dict, null\_idx, \textcolor{stringliteral}{'none\_with\_pos\_embs'})
222         self.encoder\_cand = self.get\_encoder(opt, dict, null\_idx, opt[\textcolor{stringliteral}{'reduction\_type'}])
223 
224         self.type = opt[\textcolor{stringliteral}{'polyencoder\_type'}]
225         self.n\_codes = opt[\textcolor{stringliteral}{'poly\_n\_codes'}]
226         self.attention\_type = opt[\textcolor{stringliteral}{'poly\_attention\_type'}]
227         self.attention\_keys = opt.get(\textcolor{stringliteral}{'polyencoder\_attention\_keys'}, \textcolor{stringliteral}{'context'})
228         self.attention\_num\_heads = opt[\textcolor{stringliteral}{'poly\_attention\_num\_heads'}]
229         self.codes\_attention\_type = opt[\textcolor{stringliteral}{'codes\_attention\_type'}]
230         self.codes\_attention\_num\_heads = opt[\textcolor{stringliteral}{'codes\_attention\_num\_heads'}]
231         embed\_dim = opt[\textcolor{stringliteral}{'embedding\_size'}]
232 
233         \textcolor{comment}{# In case it's a polyencoder with code.}
234         \textcolor{keywordflow}{if} self.type == \textcolor{stringliteral}{'codes'}:
235             \textcolor{comment}{# experimentally it seems that random with size = 1 was good.}
236             codes = torch.empty(self.n\_codes, embed\_dim)
237             codes = torch.nn.init.uniform\_(codes)
238             self.codes = torch.nn.Parameter(codes)
239 
240             \textcolor{comment}{# The attention for the codes.}
241             \textcolor{keywordflow}{if} self.codes\_attention\_type == \textcolor{stringliteral}{'multihead'}:
242                 self.code\_attention = MultiHeadAttention(
243                     self.codes\_attention\_num\_heads, embed\_dim, opt[\textcolor{stringliteral}{'dropout'}]
244                 )
245             \textcolor{keywordflow}{elif} self.codes\_attention\_type == \textcolor{stringliteral}{'sqrt'}:
246                 self.code\_attention = PolyBasicAttention(
247                     self.type, self.n\_codes, dim=2, attn=\textcolor{stringliteral}{'sqrt'}, get\_weights=\textcolor{keyword}{False}
248                 )
249             \textcolor{keywordflow}{elif} self.codes\_attention\_type == \textcolor{stringliteral}{'basic'}:
250                 self.code\_attention = PolyBasicAttention(
251                     self.type, self.n\_codes, dim=2, attn=\textcolor{stringliteral}{'basic'}, get\_weights=\textcolor{keyword}{False}
252                 )
253 
254         \textcolor{comment}{# The final attention (the one that takes the candidate as key)}
255         \textcolor{keywordflow}{if} self.attention\_type == \textcolor{stringliteral}{'multihead'}:
256             self.attention = MultiHeadAttention(
257                 self.attention\_num\_heads, opt[\textcolor{stringliteral}{'embedding\_size'}], opt[\textcolor{stringliteral}{'dropout'}]
258             )
259         \textcolor{keywordflow}{else}:
260             self.attention = PolyBasicAttention(
261                 self.type,
262                 self.n\_codes,
263                 dim=2,
264                 attn=self.attention\_type,
265                 get\_weights=\textcolor{keyword}{False},
266             )
267 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a21343cb181ee148326166a8e32e9e3d6}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a21343cb181ee148326166a8e32e9e3d6}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attend@{attend}}
\index{attend@{attend}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attend()}{attend()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attend (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{attention\+\_\+layer,  }\item[{}]{queries,  }\item[{}]{keys,  }\item[{}]{values,  }\item[{}]{mask }\end{DoxyParamCaption})}

\begin{DoxyVerb}Apply attention.

:param attention_layer:
    nn.Module attention layer to use for the attention
:param queries:
    the queries for attention
:param keys:
    the keys for attention
:param values:
    the values for attention
:param mask:
    mask for the attention keys

:return:
    the result of applying attention to the values, with weights computed
    wrt to the queries and keys.
\end{DoxyVerb}
 

Definition at line 310 of file polyencoder.\+py.


\begin{DoxyCode}
310     \textcolor{keyword}{def }attend(self, attention\_layer, queries, keys, values, mask):
311         \textcolor{stringliteral}{"""}
312 \textcolor{stringliteral}{        Apply attention.}
313 \textcolor{stringliteral}{}
314 \textcolor{stringliteral}{        :param attention\_layer:}
315 \textcolor{stringliteral}{            nn.Module attention layer to use for the attention}
316 \textcolor{stringliteral}{        :param queries:}
317 \textcolor{stringliteral}{            the queries for attention}
318 \textcolor{stringliteral}{        :param keys:}
319 \textcolor{stringliteral}{            the keys for attention}
320 \textcolor{stringliteral}{        :param values:}
321 \textcolor{stringliteral}{            the values for attention}
322 \textcolor{stringliteral}{        :param mask:}
323 \textcolor{stringliteral}{            mask for the attention keys}
324 \textcolor{stringliteral}{}
325 \textcolor{stringliteral}{        :return:}
326 \textcolor{stringliteral}{            the result of applying attention to the values, with weights computed}
327 \textcolor{stringliteral}{            wrt to the queries and keys.}
328 \textcolor{stringliteral}{        """}
329         \textcolor{keywordflow}{if} keys \textcolor{keywordflow}{is} \textcolor{keywordtype}{None}:
330             keys = values
331         \textcolor{keywordflow}{if} isinstance(attention\_layer, PolyBasicAttention):
332             \textcolor{keywordflow}{return} attention\_layer(queries, keys, mask\_ys=mask, values=values)
333         \textcolor{keywordflow}{elif} isinstance(attention\_layer, MultiHeadAttention):
334             \textcolor{keywordflow}{return} attention\_layer(queries, keys, values, mask)
335         \textcolor{keywordflow}{else}:
336             \textcolor{keywordflow}{raise} Exception(\textcolor{stringliteral}{'Unrecognized type of attention'})
337 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a54ca3cb24e2a0fbbfa7b3f145c7558ed}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a54ca3cb24e2a0fbbfa7b3f145c7558ed}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!encode@{encode}}
\index{encode@{encode}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{encode()}{encode()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+encode (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{ctxt\+\_\+tokens,  }\item[{}]{cand\+\_\+tokens }\end{DoxyParamCaption})}

\begin{DoxyVerb}Encode a text sequence.

:param ctxt_tokens:
    2D long tensor, batchsize x sent_len
:param cand_tokens:
    3D long tensor, batchsize x num_cands x sent_len
    Note this will actually view it as a 2D tensor
:return:
    (ctxt_rep, ctxt_mask, ctxt_pos, cand_rep)
    - ctxt_rep 3D float tensor, batchsize x n_codes x dim
    - ctxt_mask byte:  batchsize x n_codes (all 1 in case
    of polyencoder with code. Which are the vectors to use
    in the ctxt_rep)
    - ctxt_pos 3D float tensor, batchsize x sent_len x dim
    - cand_rep (3D float tensor) batchsize x num_cands x dim
\end{DoxyVerb}
 

Definition at line 338 of file polyencoder.\+py.


\begin{DoxyCode}
338     \textcolor{keyword}{def }encode(self, ctxt\_tokens, cand\_tokens):
339         \textcolor{stringliteral}{"""}
340 \textcolor{stringliteral}{        Encode a text sequence.}
341 \textcolor{stringliteral}{}
342 \textcolor{stringliteral}{        :param ctxt\_tokens:}
343 \textcolor{stringliteral}{            2D long tensor, batchsize x sent\_len}
344 \textcolor{stringliteral}{        :param cand\_tokens:}
345 \textcolor{stringliteral}{            3D long tensor, batchsize x num\_cands x sent\_len}
346 \textcolor{stringliteral}{            Note this will actually view it as a 2D tensor}
347 \textcolor{stringliteral}{        :return:}
348 \textcolor{stringliteral}{            (ctxt\_rep, ctxt\_mask, ctxt\_pos, cand\_rep)}
349 \textcolor{stringliteral}{            - ctxt\_rep 3D float tensor, batchsize x n\_codes x dim}
350 \textcolor{stringliteral}{            - ctxt\_mask byte:  batchsize x n\_codes (all 1 in case}
351 \textcolor{stringliteral}{            of polyencoder with code. Which are the vectors to use}
352 \textcolor{stringliteral}{            in the ctxt\_rep)}
353 \textcolor{stringliteral}{            - ctxt\_pos 3D float tensor, batchsize x sent\_len x dim}
354 \textcolor{stringliteral}{            - cand\_rep (3D float tensor) batchsize x num\_cands x dim}
355 \textcolor{stringliteral}{        """}
356         cand\_embed = \textcolor{keywordtype}{None}
357         ctxt\_rep = \textcolor{keywordtype}{None}
358         ctxt\_rep\_mask = \textcolor{keywordtype}{None}
359         ctxt\_pos = \textcolor{keywordtype}{None}
360         \textcolor{keywordflow}{if} cand\_tokens \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
361             \textcolor{keyword}{assert} len(cand\_tokens.shape) == 3
362             bsz = cand\_tokens.size(0)
363             num\_cands = cand\_tokens.size(1)
364             cand\_embed = self.encoder\_cand(cand\_tokens.view(bsz * num\_cands, -1))
365             cand\_embed = cand\_embed.view(bsz, num\_cands, -1)
366 
367         \textcolor{keywordflow}{if} ctxt\_tokens \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
368             \textcolor{keyword}{assert} len(ctxt\_tokens.shape) == 2
369             bsz = ctxt\_tokens.size(0)
370             \textcolor{comment}{# get context\_representation. Now that depends on the cases.}
371             ctxt\_out, ctxt\_mask, ctxt\_pos = self.encoder\_ctxt(ctxt\_tokens)
372             att\_keys = ctxt\_out \textcolor{keywordflow}{if} self.attention\_keys == \textcolor{stringliteral}{'context'} \textcolor{keywordflow}{else} ctxt\_pos
373             dim = ctxt\_out.size(2)
374 
375             \textcolor{keywordflow}{if} self.type == \textcolor{stringliteral}{'codes'}:
376                 ctxt\_rep = self.attend(
377                     self.code\_attention,
378                     queries=self.codes.\hyperlink{namespacerepeat}{repeat}(bsz, 1, 1),
379                     keys=att\_keys,
380                     values=ctxt\_out,
381                     mask=ctxt\_mask,
382                 )
383                 ctxt\_pos = \textcolor{keywordtype}{None}  \textcolor{comment}{# we don't need this anymore}
384                 ctxt\_rep\_mask = ctxt\_rep.new\_ones(bsz, self.n\_codes).byte()
385 
386             \textcolor{keywordflow}{elif} self.type == \textcolor{stringliteral}{'n\_first'}:
387                 \textcolor{comment}{# Expand the output if it is not long enough}
388                 \textcolor{keywordflow}{if} ctxt\_out.size(1) < self.n\_codes:
389                     difference = self.n\_codes - ctxt\_out.size(1)
390                     extra\_rep = ctxt\_out.new\_zeros(bsz, difference, dim)
391                     ctxt\_rep = torch.cat([ctxt\_out, extra\_rep], dim=1)
392                     ctxt\_pos = torch.cat([ctxt\_pos, extra\_rep], dim=1)
393                     extra\_mask = ctxt\_mask.new\_zeros(bsz, difference)
394                     ctxt\_rep\_mask = torch.cat([ctxt\_mask, extra\_mask], dim=1)
395                 \textcolor{keywordflow}{else}:
396                     ctxt\_rep = ctxt\_out[:, 0 : self.n\_codes, :]
397                     ctxt\_pos = ctxt\_pos[:, 0 : self.n\_codes, :]
398                     ctxt\_rep\_mask = ctxt\_mask[:, 0 : self.n\_codes]
399 
400         \textcolor{keywordflow}{return} ctxt\_rep, ctxt\_rep\_mask, ctxt\_pos, cand\_embed
401 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9e769210d947e88dc3e4555583b9c82e}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9e769210d947e88dc3e4555583b9c82e}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!forward@{forward}}
\index{forward@{forward}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{ctxt\+\_\+tokens = {\ttfamily None},  }\item[{}]{cand\+\_\+tokens = {\ttfamily None},  }\item[{}]{ctxt\+\_\+rep = {\ttfamily None},  }\item[{}]{ctxt\+\_\+rep\+\_\+mask = {\ttfamily None},  }\item[{}]{ctxt\+\_\+pos = {\ttfamily None},  }\item[{}]{cand\+\_\+rep = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Forward pass of the model.

Due to a limitation of parlai, we have to have one single model
in the agent. And because we want to be able to use data-parallel,
we need to have one single forward() method.
Therefore the operation_type can be either 'encode' or 'score'.

:param ctxt_tokens:
    tokenized contexts
:param cand_tokens:
    tokenized candidates
:param ctxt_rep:
    (bsz x num_codes x hsz)
    encoded representation of the context. If self.type == 'codes', these
    are the context codes. Otherwise, they are the outputs from the
    encoder
:param ctxt_rep_mask:
    mask for ctxt rep
:param ctxt_pos:
    position embeddings for the ctxt_rep. If self.type == 'codes', these
    are None, as their use is earlier in the pipeline.
:param cand_rep:
    encoded representation of the candidates
\end{DoxyVerb}
 

Definition at line 434 of file polyencoder.\+py.


\begin{DoxyCode}
434     ):
435         \textcolor{stringliteral}{"""}
436 \textcolor{stringliteral}{        Forward pass of the model.}
437 \textcolor{stringliteral}{}
438 \textcolor{stringliteral}{        Due to a limitation of parlai, we have to have one single model}
439 \textcolor{stringliteral}{        in the agent. And because we want to be able to use data-parallel,}
440 \textcolor{stringliteral}{        we need to have one single forward() method.}
441 \textcolor{stringliteral}{        Therefore the operation\_type can be either 'encode' or 'score'.}
442 \textcolor{stringliteral}{}
443 \textcolor{stringliteral}{        :param ctxt\_tokens:}
444 \textcolor{stringliteral}{            tokenized contexts}
445 \textcolor{stringliteral}{        :param cand\_tokens:}
446 \textcolor{stringliteral}{            tokenized candidates}
447 \textcolor{stringliteral}{        :param ctxt\_rep:}
448 \textcolor{stringliteral}{            (bsz x num\_codes x hsz)}
449 \textcolor{stringliteral}{            encoded representation of the context. If self.type == 'codes', these}
450 \textcolor{stringliteral}{            are the context codes. Otherwise, they are the outputs from the}
451 \textcolor{stringliteral}{            encoder}
452 \textcolor{stringliteral}{        :param ctxt\_rep\_mask:}
453 \textcolor{stringliteral}{            mask for ctxt rep}
454 \textcolor{stringliteral}{        :param ctxt\_pos:}
455 \textcolor{stringliteral}{            position embeddings for the ctxt\_rep. If self.type == 'codes', these}
456 \textcolor{stringliteral}{            are None, as their use is earlier in the pipeline.}
457 \textcolor{stringliteral}{        :param cand\_rep:}
458 \textcolor{stringliteral}{            encoded representation of the candidates}
459 \textcolor{stringliteral}{        """}
460         \textcolor{keywordflow}{if} ctxt\_tokens \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None} \textcolor{keywordflow}{or} cand\_tokens \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
461             \textcolor{keywordflow}{return} self.encode(ctxt\_tokens, cand\_tokens)
462         \textcolor{keywordflow}{elif} (
463             ctxt\_rep \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None} \textcolor{keywordflow}{and} ctxt\_rep\_mask \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None} \textcolor{keywordflow}{and} cand\_rep \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}
464         ):
465             \textcolor{comment}{# ctxt\_pos can be none, if we are using codes (not first M)}
466             \textcolor{keywordflow}{return} self.score(ctxt\_rep, ctxt\_rep\_mask, ctxt\_pos, cand\_rep)
467         \textcolor{keywordflow}{raise} Exception(\textcolor{stringliteral}{'Unsupported operation'})
468 
469 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acd213717123d2f2f24e6f03270c3db14}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acd213717123d2f2f24e6f03270c3db14}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!get\+\_\+encoder@{get\+\_\+encoder}}
\index{get\+\_\+encoder@{get\+\_\+encoder}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{get\+\_\+encoder()}{get\_encoder()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+get\+\_\+encoder (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{opt,  }\item[{}]{dict,  }\item[{}]{null\+\_\+idx,  }\item[{}]{reduction\+\_\+type }\end{DoxyParamCaption})}

\begin{DoxyVerb}Return encoder, given options.

:param opt:
    opt dict
:param dict:
    dictionary agent
:param null_idx:
    null/pad index into dict
:reduction_type:
    reduction type for the encoder

:return:
    a TransformerEncoder, initialized correctly
\end{DoxyVerb}
 

Definition at line 268 of file polyencoder.\+py.


\begin{DoxyCode}
268     \textcolor{keyword}{def }get\_encoder(self, opt, dict, null\_idx, reduction\_type):
269         \textcolor{stringliteral}{"""}
270 \textcolor{stringliteral}{        Return encoder, given options.}
271 \textcolor{stringliteral}{}
272 \textcolor{stringliteral}{        :param opt:}
273 \textcolor{stringliteral}{            opt dict}
274 \textcolor{stringliteral}{        :param dict:}
275 \textcolor{stringliteral}{            dictionary agent}
276 \textcolor{stringliteral}{        :param null\_idx:}
277 \textcolor{stringliteral}{            null/pad index into dict}
278 \textcolor{stringliteral}{        :reduction\_type:}
279 \textcolor{stringliteral}{            reduction type for the encoder}
280 \textcolor{stringliteral}{}
281 \textcolor{stringliteral}{        :return:}
282 \textcolor{stringliteral}{            a TransformerEncoder, initialized correctly}
283 \textcolor{stringliteral}{        """}
284         n\_positions = \hyperlink{namespaceparlai_1_1agents_1_1transformer_1_1modules_ab67607512c597ddd54f2b60a1a1eaf4c}{get\_n\_positions\_from\_options}(opt)
285         embeddings = torch.nn.Embedding(
286             len(dict), opt[\textcolor{stringliteral}{'embedding\_size'}], padding\_idx=null\_idx
287         )
288         torch.nn.init.normal\_(embeddings.weight, 0, opt[\textcolor{stringliteral}{'embedding\_size'}] ** -0.5)
289         \textcolor{keywordflow}{return} TransformerEncoder(
290             n\_heads=opt[\textcolor{stringliteral}{'n\_heads'}],
291             n\_layers=opt[\textcolor{stringliteral}{'n\_layers'}],
292             embedding\_size=opt[\textcolor{stringliteral}{'embedding\_size'}],
293             ffn\_size=opt[\textcolor{stringliteral}{'ffn\_size'}],
294             vocabulary\_size=len(dict),
295             embedding=embeddings,
296             dropout=opt[\textcolor{stringliteral}{'dropout'}],
297             attention\_dropout=opt[\textcolor{stringliteral}{'attention\_dropout'}],
298             relu\_dropout=opt[\textcolor{stringliteral}{'relu\_dropout'}],
299             padding\_idx=null\_idx,
300             learn\_positional\_embeddings=opt[\textcolor{stringliteral}{'learn\_positional\_embeddings'}],
301             embeddings\_scale=opt[\textcolor{stringliteral}{'embeddings\_scale'}],
302             reduction\_type=reduction\_type,
303             n\_positions=n\_positions,
304             n\_segments=2,
305             activation=opt[\textcolor{stringliteral}{'activation'}],
306             variant=opt[\textcolor{stringliteral}{'variant'}],
307             output\_scaling=opt[\textcolor{stringliteral}{'output\_scaling'}],
308         )
309 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a52218a8c0594ff4d5bf8e666dc38f508}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a52218a8c0594ff4d5bf8e666dc38f508}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!score@{score}}
\index{score@{score}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{score()}{score()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+score (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{ctxt\+\_\+rep,  }\item[{}]{ctxt\+\_\+rep\+\_\+mask,  }\item[{}]{ctxt\+\_\+pos,  }\item[{}]{cand\+\_\+embed }\end{DoxyParamCaption})}

\begin{DoxyVerb}Score the candidates.

:param ctxt_rep:
    3D float tensor, bsz x ctxt_len x dim
:param ctxt_rep_mask:
    2D byte tensor, bsz x ctxt_len, in case there are some elements
    of the ctxt that we should not take into account.
:param ctx_pos: 3D float tensor, bsz x sent_len x dim
:param cand_embed: 3D float tensor, bsz x num_cands x dim

:return: scores, 2D float tensor: bsz x num_cands
\end{DoxyVerb}
 

Definition at line 402 of file polyencoder.\+py.


\begin{DoxyCode}
402     \textcolor{keyword}{def }score(self, ctxt\_rep, ctxt\_rep\_mask, ctxt\_pos, cand\_embed):
403         \textcolor{stringliteral}{"""}
404 \textcolor{stringliteral}{        Score the candidates.}
405 \textcolor{stringliteral}{}
406 \textcolor{stringliteral}{        :param ctxt\_rep:}
407 \textcolor{stringliteral}{            3D float tensor, bsz x ctxt\_len x dim}
408 \textcolor{stringliteral}{        :param ctxt\_rep\_mask:}
409 \textcolor{stringliteral}{            2D byte tensor, bsz x ctxt\_len, in case there are some elements}
410 \textcolor{stringliteral}{            of the ctxt that we should not take into account.}
411 \textcolor{stringliteral}{        :param ctx\_pos: 3D float tensor, bsz x sent\_len x dim}
412 \textcolor{stringliteral}{        :param cand\_embed: 3D float tensor, bsz x num\_cands x dim}
413 \textcolor{stringliteral}{}
414 \textcolor{stringliteral}{        :return: scores, 2D float tensor: bsz x num\_cands}
415 \textcolor{stringliteral}{        """}
416         \textcolor{comment}{# Attention keys determined by self.attention\_keys}
417         \textcolor{comment}{# 'context' == use context final rep; otherwise use context position embs}
418         keys = ctxt\_rep \textcolor{keywordflow}{if} self.attention\_keys == \textcolor{stringliteral}{'context'} \textcolor{keywordflow}{else} ctxt\_pos
419         \textcolor{comment}{# reduces the context representation to a 3D tensor bsz x num\_cands x dim}
420         ctxt\_final\_rep = self.attend(
421             self.attention, cand\_embed, keys, ctxt\_rep, ctxt\_rep\_mask
422         )
423         scores = torch.sum(ctxt\_final\_rep * cand\_embed, 2)
424         \textcolor{keywordflow}{return} scores
425 
\end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8fab02ef33d1e1ee413e4bcbe25aa03b}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8fab02ef33d1e1ee413e4bcbe25aa03b}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attention@{attention}}
\index{attention@{attention}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attention}{attention}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attention}



Definition at line 256 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af29f6ff1656451ec4c35b4cc9ea0f5dc}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af29f6ff1656451ec4c35b4cc9ea0f5dc}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attention\+\_\+keys@{attention\+\_\+keys}}
\index{attention\+\_\+keys@{attention\+\_\+keys}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attention\+\_\+keys}{attention\_keys}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attention\+\_\+keys}



Definition at line 227 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acc2312ac770d343991de882c05da56e6}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acc2312ac770d343991de882c05da56e6}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attention\+\_\+num\+\_\+heads@{attention\+\_\+num\+\_\+heads}}
\index{attention\+\_\+num\+\_\+heads@{attention\+\_\+num\+\_\+heads}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attention\+\_\+num\+\_\+heads}{attention\_num\_heads}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attention\+\_\+num\+\_\+heads}



Definition at line 228 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a2efc9276c4ab6d1533b7ab3ceef3ac3a}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a2efc9276c4ab6d1533b7ab3ceef3ac3a}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attention\+\_\+type@{attention\+\_\+type}}
\index{attention\+\_\+type@{attention\+\_\+type}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attention\+\_\+type}{attention\_type}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attention\+\_\+type}



Definition at line 226 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a6740a4e891b964a41484a793ae0e6e6d}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a6740a4e891b964a41484a793ae0e6e6d}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!code\+\_\+attention@{code\+\_\+attention}}
\index{code\+\_\+attention@{code\+\_\+attention}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{code\+\_\+attention}{code\_attention}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+code\+\_\+attention}



Definition at line 242 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ac8d8955e72761c0775aeefea5ce80c3c}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ac8d8955e72761c0775aeefea5ce80c3c}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!codes@{codes}}
\index{codes@{codes}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{codes}{codes}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+codes}



Definition at line 238 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ae30ed526a2a4442222b6572a6fa41604}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ae30ed526a2a4442222b6572a6fa41604}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!codes\+\_\+attention\+\_\+num\+\_\+heads@{codes\+\_\+attention\+\_\+num\+\_\+heads}}
\index{codes\+\_\+attention\+\_\+num\+\_\+heads@{codes\+\_\+attention\+\_\+num\+\_\+heads}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{codes\+\_\+attention\+\_\+num\+\_\+heads}{codes\_attention\_num\_heads}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+codes\+\_\+attention\+\_\+num\+\_\+heads}



Definition at line 230 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a80cf22447d874c72d3e9b64a64cf5747}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a80cf22447d874c72d3e9b64a64cf5747}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!codes\+\_\+attention\+\_\+type@{codes\+\_\+attention\+\_\+type}}
\index{codes\+\_\+attention\+\_\+type@{codes\+\_\+attention\+\_\+type}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{codes\+\_\+attention\+\_\+type}{codes\_attention\_type}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+codes\+\_\+attention\+\_\+type}



Definition at line 229 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af70c94c4e4073e804904b983a683c0a7}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af70c94c4e4073e804904b983a683c0a7}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!encoder\+\_\+cand@{encoder\+\_\+cand}}
\index{encoder\+\_\+cand@{encoder\+\_\+cand}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{encoder\+\_\+cand}{encoder\_cand}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+encoder\+\_\+cand}



Definition at line 222 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9a5e58bfbd75ec89e10b8e4bcbcd4ab6}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9a5e58bfbd75ec89e10b8e4bcbcd4ab6}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!encoder\+\_\+ctxt@{encoder\+\_\+ctxt}}
\index{encoder\+\_\+ctxt@{encoder\+\_\+ctxt}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{encoder\+\_\+ctxt}{encoder\_ctxt}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+encoder\+\_\+ctxt}



Definition at line 221 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8440bd98b6ee31c0540fdbc2e9efe987}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8440bd98b6ee31c0540fdbc2e9efe987}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!n\+\_\+codes@{n\+\_\+codes}}
\index{n\+\_\+codes@{n\+\_\+codes}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{n\+\_\+codes}{n\_codes}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+n\+\_\+codes}



Definition at line 225 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!null\+\_\+idx@{null\+\_\+idx}}
\index{null\+\_\+idx@{null\+\_\+idx}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{null\+\_\+idx}{null\_idx}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+null\+\_\+idx}



Definition at line 220 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a3eea152c32cd33d1e6b0ca680887d0e7}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a3eea152c32cd33d1e6b0ca680887d0e7}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!type@{type}}
\index{type@{type}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{type}{type}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+type}



Definition at line 224 of file polyencoder.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/transformer/\hyperlink{polyencoder_8py}{polyencoder.\+py}\end{DoxyCompactItemize}
