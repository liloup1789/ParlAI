\hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule}{}\section{parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module Class Reference}
\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule}\index{parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module@{parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module}}


Inheritance diagram for parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=263pt]{d4/d08/classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=263pt]{d0/d15/classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acf6f1b0dd5025b20c330d08bd2a13755}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, opt, dict, \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}{null\+\_\+idx})
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acd213717123d2f2f24e6f03270c3db14}{get\+\_\+encoder} (self, opt, dict, \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}{null\+\_\+idx}, reduction\+\_\+type)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a21343cb181ee148326166a8e32e9e3d6}{attend} (self, attention\+\_\+layer, queries, keys, values, mask)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a54ca3cb24e2a0fbbfa7b3f145c7558ed}{encode} (self, ctxt\+\_\+tokens, cand\+\_\+tokens)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a52218a8c0594ff4d5bf8e666dc38f508}{score} (self, ctxt\+\_\+rep, ctxt\+\_\+rep\+\_\+mask, ctxt\+\_\+pos, cand\+\_\+embed)
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9e769210d947e88dc3e4555583b9c82e}{forward} (self, ctxt\+\_\+tokens=None, cand\+\_\+tokens=None, ctxt\+\_\+rep=None, ctxt\+\_\+rep\+\_\+mask=None, ctxt\+\_\+pos=None, cand\+\_\+rep=None)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}{null\+\_\+idx}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9a5e58bfbd75ec89e10b8e4bcbcd4ab6}{encoder\+\_\+ctxt}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af70c94c4e4073e804904b983a683c0a7}{encoder\+\_\+cand}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a3eea152c32cd33d1e6b0ca680887d0e7}{type}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8440bd98b6ee31c0540fdbc2e9efe987}{n\+\_\+codes}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a2efc9276c4ab6d1533b7ab3ceef3ac3a}{attention\+\_\+type}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af29f6ff1656451ec4c35b4cc9ea0f5dc}{attention\+\_\+keys}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acc2312ac770d343991de882c05da56e6}{attention\+\_\+num\+\_\+heads}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a80cf22447d874c72d3e9b64a64cf5747}{codes\+\_\+attention\+\_\+type}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ae30ed526a2a4442222b6572a6fa41604}{codes\+\_\+attention\+\_\+num\+\_\+heads}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ac8d8955e72761c0775aeefea5ce80c3c}{codes}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a6740a4e891b964a41484a793ae0e6e6d}{code\+\_\+attention}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8fab02ef33d1e1ee413e4bcbe25aa03b}{attention}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Poly-encoder model.

See https://arxiv.org/abs/1905.01969 for more details
\end{DoxyVerb}
 

Definition at line 210 of file polyencoder.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acf6f1b0dd5025b20c330d08bd2a13755}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acf6f1b0dd5025b20c330d08bd2a13755}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{opt,  }\item[{}]{dict,  }\item[{}]{null\+\_\+idx }\end{DoxyParamCaption})}



Definition at line 217 of file polyencoder.\+py.


\begin{DoxyCode}
217     \textcolor{keyword}{def }\_\_init\_\_(self, opt, dict, null\_idx):
218         super(PolyEncoderModule, self).\_\_init\_\_()
219         self.null\_idx = null\_idx
220         self.encoder\_ctxt = self.get\_encoder(opt, dict, null\_idx, \textcolor{stringliteral}{'none\_with\_pos\_embs'})
221         self.encoder\_cand = self.get\_encoder(opt, dict, null\_idx, opt[\textcolor{stringliteral}{'reduction\_type'}])
222 
223         self.type = opt[\textcolor{stringliteral}{'polyencoder\_type'}]
224         self.n\_codes = opt[\textcolor{stringliteral}{'poly\_n\_codes'}]
225         self.attention\_type = opt[\textcolor{stringliteral}{'poly\_attention\_type'}]
226         self.attention\_keys = opt.get(\textcolor{stringliteral}{'polyencoder\_attention\_keys'}, \textcolor{stringliteral}{'context'})
227         self.attention\_num\_heads = opt[\textcolor{stringliteral}{'poly\_attention\_num\_heads'}]
228         self.codes\_attention\_type = opt[\textcolor{stringliteral}{'codes\_attention\_type'}]
229         self.codes\_attention\_num\_heads = opt[\textcolor{stringliteral}{'codes\_attention\_num\_heads'}]
230         embed\_dim = opt[\textcolor{stringliteral}{'embedding\_size'}]
231 
232         \textcolor{comment}{# In case it's a polyencoder with code.}
233         \textcolor{keywordflow}{if} self.type == \textcolor{stringliteral}{'codes'}:
234             \textcolor{comment}{# experimentally it seems that random with size = 1 was good.}
235             codes = torch.empty(self.n\_codes, embed\_dim)
236             codes = torch.nn.init.uniform\_(codes)
237             self.codes = torch.nn.Parameter(codes)
238 
239             \textcolor{comment}{# The attention for the codes.}
240             \textcolor{keywordflow}{if} self.codes\_attention\_type == \textcolor{stringliteral}{'multihead'}:
241                 self.code\_attention = MultiHeadAttention(
242                     self.codes\_attention\_num\_heads, embed\_dim, opt[\textcolor{stringliteral}{'dropout'}]
243                 )
244             \textcolor{keywordflow}{elif} self.codes\_attention\_type == \textcolor{stringliteral}{'sqrt'}:
245                 self.code\_attention = PolyBasicAttention(
246                     self.type, self.n\_codes, dim=2, attn=\textcolor{stringliteral}{'sqrt'}, get\_weights=\textcolor{keyword}{False}
247                 )
248             \textcolor{keywordflow}{elif} self.codes\_attention\_type == \textcolor{stringliteral}{'basic'}:
249                 self.code\_attention = PolyBasicAttention(
250                     self.type, self.n\_codes, dim=2, attn=\textcolor{stringliteral}{'basic'}, get\_weights=\textcolor{keyword}{False}
251                 )
252 
253         \textcolor{comment}{# The final attention (the one that takes the candidate as key)}
254         \textcolor{keywordflow}{if} self.attention\_type == \textcolor{stringliteral}{'multihead'}:
255             self.attention = MultiHeadAttention(
256                 self.attention\_num\_heads, opt[\textcolor{stringliteral}{'embedding\_size'}], opt[\textcolor{stringliteral}{'dropout'}]
257             )
258         \textcolor{keywordflow}{else}:
259             self.attention = PolyBasicAttention(
260                 self.type,
261                 self.n\_codes,
262                 dim=2,
263                 attn=self.attention\_type,
264                 get\_weights=\textcolor{keyword}{False},
265             )
266 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a21343cb181ee148326166a8e32e9e3d6}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a21343cb181ee148326166a8e32e9e3d6}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attend@{attend}}
\index{attend@{attend}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attend()}{attend()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attend (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{attention\+\_\+layer,  }\item[{}]{queries,  }\item[{}]{keys,  }\item[{}]{values,  }\item[{}]{mask }\end{DoxyParamCaption})}

\begin{DoxyVerb}Apply attention.

:param attention_layer:
    nn.Module attention layer to use for the attention
:param queries:
    the queries for attention
:param keys:
    the keys for attention
:param values:
    the values for attention
:param mask:
    mask for the attention keys

:return:
    the result of applying attention to the values, with weights computed
    wrt to the queries and keys.
\end{DoxyVerb}
 

Definition at line 309 of file polyencoder.\+py.


\begin{DoxyCode}
309     \textcolor{keyword}{def }attend(self, attention\_layer, queries, keys, values, mask):
310         \textcolor{stringliteral}{"""}
311 \textcolor{stringliteral}{        Apply attention.}
312 \textcolor{stringliteral}{}
313 \textcolor{stringliteral}{        :param attention\_layer:}
314 \textcolor{stringliteral}{            nn.Module attention layer to use for the attention}
315 \textcolor{stringliteral}{        :param queries:}
316 \textcolor{stringliteral}{            the queries for attention}
317 \textcolor{stringliteral}{        :param keys:}
318 \textcolor{stringliteral}{            the keys for attention}
319 \textcolor{stringliteral}{        :param values:}
320 \textcolor{stringliteral}{            the values for attention}
321 \textcolor{stringliteral}{        :param mask:}
322 \textcolor{stringliteral}{            mask for the attention keys}
323 \textcolor{stringliteral}{}
324 \textcolor{stringliteral}{        :return:}
325 \textcolor{stringliteral}{            the result of applying attention to the values, with weights computed}
326 \textcolor{stringliteral}{            wrt to the queries and keys.}
327 \textcolor{stringliteral}{        """}
328         \textcolor{keywordflow}{if} keys \textcolor{keywordflow}{is} \textcolor{keywordtype}{None}:
329             keys = values
330         \textcolor{keywordflow}{if} isinstance(attention\_layer, PolyBasicAttention):
331             \textcolor{keywordflow}{return} attention\_layer(queries, keys, mask\_ys=mask, values=values)
332         \textcolor{keywordflow}{elif} isinstance(attention\_layer, MultiHeadAttention):
333             \textcolor{keywordflow}{return} attention\_layer(queries, keys, values, mask)
334         \textcolor{keywordflow}{else}:
335             \textcolor{keywordflow}{raise} Exception(\textcolor{stringliteral}{'Unrecognized type of attention'})
336 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a54ca3cb24e2a0fbbfa7b3f145c7558ed}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a54ca3cb24e2a0fbbfa7b3f145c7558ed}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!encode@{encode}}
\index{encode@{encode}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{encode()}{encode()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+encode (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{ctxt\+\_\+tokens,  }\item[{}]{cand\+\_\+tokens }\end{DoxyParamCaption})}

\begin{DoxyVerb}Encode a text sequence.

:param ctxt_tokens:
    2D long tensor, batchsize x sent_len
:param cand_tokens:
    3D long tensor, batchsize x num_cands x sent_len
    Note this will actually view it as a 2D tensor
:return:
    (ctxt_rep, ctxt_mask, ctxt_pos, cand_rep)
    - ctxt_rep 3D float tensor, batchsize x n_codes x dim
    - ctxt_mask byte:  batchsize x n_codes (all 1 in case
    of polyencoder with code. Which are the vectors to use
    in the ctxt_rep)
    - ctxt_pos 3D float tensor, batchsize x sent_len x dim
    - cand_rep (3D float tensor) batchsize x num_cands x dim
\end{DoxyVerb}
 

Definition at line 337 of file polyencoder.\+py.


\begin{DoxyCode}
337     \textcolor{keyword}{def }encode(self, ctxt\_tokens, cand\_tokens):
338         \textcolor{stringliteral}{"""}
339 \textcolor{stringliteral}{        Encode a text sequence.}
340 \textcolor{stringliteral}{}
341 \textcolor{stringliteral}{        :param ctxt\_tokens:}
342 \textcolor{stringliteral}{            2D long tensor, batchsize x sent\_len}
343 \textcolor{stringliteral}{        :param cand\_tokens:}
344 \textcolor{stringliteral}{            3D long tensor, batchsize x num\_cands x sent\_len}
345 \textcolor{stringliteral}{            Note this will actually view it as a 2D tensor}
346 \textcolor{stringliteral}{        :return:}
347 \textcolor{stringliteral}{            (ctxt\_rep, ctxt\_mask, ctxt\_pos, cand\_rep)}
348 \textcolor{stringliteral}{            - ctxt\_rep 3D float tensor, batchsize x n\_codes x dim}
349 \textcolor{stringliteral}{            - ctxt\_mask byte:  batchsize x n\_codes (all 1 in case}
350 \textcolor{stringliteral}{            of polyencoder with code. Which are the vectors to use}
351 \textcolor{stringliteral}{            in the ctxt\_rep)}
352 \textcolor{stringliteral}{            - ctxt\_pos 3D float tensor, batchsize x sent\_len x dim}
353 \textcolor{stringliteral}{            - cand\_rep (3D float tensor) batchsize x num\_cands x dim}
354 \textcolor{stringliteral}{        """}
355         cand\_embed = \textcolor{keywordtype}{None}
356         ctxt\_rep = \textcolor{keywordtype}{None}
357         ctxt\_rep\_mask = \textcolor{keywordtype}{None}
358         ctxt\_pos = \textcolor{keywordtype}{None}
359         \textcolor{keywordflow}{if} cand\_tokens \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
360             \textcolor{keyword}{assert} len(cand\_tokens.shape) == 3
361             bsz = cand\_tokens.size(0)
362             num\_cands = cand\_tokens.size(1)
363             cand\_embed = self.encoder\_cand(cand\_tokens.view(bsz * num\_cands, -1))
364             cand\_embed = cand\_embed.view(bsz, num\_cands, -1)
365 
366         \textcolor{keywordflow}{if} ctxt\_tokens \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
367             \textcolor{keyword}{assert} len(ctxt\_tokens.shape) == 2
368             bsz = ctxt\_tokens.size(0)
369             \textcolor{comment}{# get context\_representation. Now that depends on the cases.}
370             ctxt\_out, ctxt\_mask, ctxt\_pos = self.encoder\_ctxt(ctxt\_tokens)
371             att\_keys = ctxt\_out \textcolor{keywordflow}{if} self.attention\_keys == \textcolor{stringliteral}{'context'} \textcolor{keywordflow}{else} ctxt\_pos
372             dim = ctxt\_out.size(2)
373 
374             \textcolor{keywordflow}{if} self.type == \textcolor{stringliteral}{'codes'}:
375                 ctxt\_rep = self.attend(
376                     self.code\_attention,
377                     queries=self.codes.\hyperlink{namespacerepeat}{repeat}(bsz, 1, 1),
378                     keys=att\_keys,
379                     values=ctxt\_out,
380                     mask=ctxt\_mask,
381                 )
382                 ctxt\_pos = \textcolor{keywordtype}{None}  \textcolor{comment}{# we don't need this anymore}
383                 ctxt\_rep\_mask = ctxt\_rep.new\_ones(bsz, self.n\_codes).byte()
384 
385             \textcolor{keywordflow}{elif} self.type == \textcolor{stringliteral}{'n\_first'}:
386                 \textcolor{comment}{# Expand the output if it is not long enough}
387                 \textcolor{keywordflow}{if} ctxt\_out.size(1) < self.n\_codes:
388                     difference = self.n\_codes - ctxt\_out.size(1)
389                     extra\_rep = ctxt\_out.new\_zeros(bsz, difference, dim)
390                     ctxt\_rep = torch.cat([ctxt\_out, extra\_rep], dim=1)
391                     ctxt\_pos = torch.cat([ctxt\_pos, extra\_rep], dim=1)
392                     extra\_mask = ctxt\_mask.new\_zeros(bsz, difference)
393                     ctxt\_rep\_mask = torch.cat([ctxt\_mask, extra\_mask], dim=1)
394                 \textcolor{keywordflow}{else}:
395                     ctxt\_rep = ctxt\_out[:, 0 : self.n\_codes, :]
396                     ctxt\_pos = ctxt\_pos[:, 0 : self.n\_codes, :]
397                     ctxt\_rep\_mask = ctxt\_mask[:, 0 : self.n\_codes]
398 
399         \textcolor{keywordflow}{return} ctxt\_rep, ctxt\_rep\_mask, ctxt\_pos, cand\_embed
400 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9e769210d947e88dc3e4555583b9c82e}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9e769210d947e88dc3e4555583b9c82e}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!forward@{forward}}
\index{forward@{forward}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{ctxt\+\_\+tokens = {\ttfamily None},  }\item[{}]{cand\+\_\+tokens = {\ttfamily None},  }\item[{}]{ctxt\+\_\+rep = {\ttfamily None},  }\item[{}]{ctxt\+\_\+rep\+\_\+mask = {\ttfamily None},  }\item[{}]{ctxt\+\_\+pos = {\ttfamily None},  }\item[{}]{cand\+\_\+rep = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Forward pass of the model.

Due to a limitation of parlai, we have to have one single model
in the agent. And because we want to be able to use data-parallel,
we need to have one single forward() method.
Therefore the operation_type can be either 'encode' or 'score'.

:param ctxt_tokens:
    tokenized contexts
:param cand_tokens:
    tokenized candidates
:param ctxt_rep:
    (bsz x num_codes x hsz)
    encoded representation of the context. If self.type == 'codes', these
    are the context codes. Otherwise, they are the outputs from the
    encoder
:param ctxt_rep_mask:
    mask for ctxt rep
:param ctxt_pos:
    position embeddings for the ctxt_rep. If self.type == 'codes', these
    are None, as their use is earlier in the pipeline.
:param cand_rep:
    encoded representation of the candidates
\end{DoxyVerb}
 

Definition at line 433 of file polyencoder.\+py.


\begin{DoxyCode}
433     ):
434         \textcolor{stringliteral}{"""}
435 \textcolor{stringliteral}{        Forward pass of the model.}
436 \textcolor{stringliteral}{}
437 \textcolor{stringliteral}{        Due to a limitation of parlai, we have to have one single model}
438 \textcolor{stringliteral}{        in the agent. And because we want to be able to use data-parallel,}
439 \textcolor{stringliteral}{        we need to have one single forward() method.}
440 \textcolor{stringliteral}{        Therefore the operation\_type can be either 'encode' or 'score'.}
441 \textcolor{stringliteral}{}
442 \textcolor{stringliteral}{        :param ctxt\_tokens:}
443 \textcolor{stringliteral}{            tokenized contexts}
444 \textcolor{stringliteral}{        :param cand\_tokens:}
445 \textcolor{stringliteral}{            tokenized candidates}
446 \textcolor{stringliteral}{        :param ctxt\_rep:}
447 \textcolor{stringliteral}{            (bsz x num\_codes x hsz)}
448 \textcolor{stringliteral}{            encoded representation of the context. If self.type == 'codes', these}
449 \textcolor{stringliteral}{            are the context codes. Otherwise, they are the outputs from the}
450 \textcolor{stringliteral}{            encoder}
451 \textcolor{stringliteral}{        :param ctxt\_rep\_mask:}
452 \textcolor{stringliteral}{            mask for ctxt rep}
453 \textcolor{stringliteral}{        :param ctxt\_pos:}
454 \textcolor{stringliteral}{            position embeddings for the ctxt\_rep. If self.type == 'codes', these}
455 \textcolor{stringliteral}{            are None, as their use is earlier in the pipeline.}
456 \textcolor{stringliteral}{        :param cand\_rep:}
457 \textcolor{stringliteral}{            encoded representation of the candidates}
458 \textcolor{stringliteral}{        """}
459         \textcolor{keywordflow}{if} ctxt\_tokens \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None} \textcolor{keywordflow}{or} cand\_tokens \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
460             \textcolor{keywordflow}{return} self.encode(ctxt\_tokens, cand\_tokens)
461         \textcolor{keywordflow}{elif} (
462             ctxt\_rep \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None} \textcolor{keywordflow}{and} ctxt\_rep\_mask \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None} \textcolor{keywordflow}{and} cand\_rep \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}
463         ):
464             \textcolor{comment}{# ctxt\_pos can be none, if we are using codes (not first M)}
465             \textcolor{keywordflow}{return} self.score(ctxt\_rep, ctxt\_rep\_mask, ctxt\_pos, cand\_rep)
466         \textcolor{keywordflow}{raise} Exception(\textcolor{stringliteral}{'Unsupported operation'})
467 
468 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acd213717123d2f2f24e6f03270c3db14}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acd213717123d2f2f24e6f03270c3db14}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!get\+\_\+encoder@{get\+\_\+encoder}}
\index{get\+\_\+encoder@{get\+\_\+encoder}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{get\+\_\+encoder()}{get\_encoder()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+get\+\_\+encoder (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{opt,  }\item[{}]{dict,  }\item[{}]{null\+\_\+idx,  }\item[{}]{reduction\+\_\+type }\end{DoxyParamCaption})}

\begin{DoxyVerb}Return encoder, given options.

:param opt:
    opt dict
:param dict:
    dictionary agent
:param null_idx:
    null/pad index into dict
:reduction_type:
    reduction type for the encoder

:return:
    a TransformerEncoder, initialized correctly
\end{DoxyVerb}
 

Definition at line 267 of file polyencoder.\+py.


\begin{DoxyCode}
267     \textcolor{keyword}{def }get\_encoder(self, opt, dict, null\_idx, reduction\_type):
268         \textcolor{stringliteral}{"""}
269 \textcolor{stringliteral}{        Return encoder, given options.}
270 \textcolor{stringliteral}{}
271 \textcolor{stringliteral}{        :param opt:}
272 \textcolor{stringliteral}{            opt dict}
273 \textcolor{stringliteral}{        :param dict:}
274 \textcolor{stringliteral}{            dictionary agent}
275 \textcolor{stringliteral}{        :param null\_idx:}
276 \textcolor{stringliteral}{            null/pad index into dict}
277 \textcolor{stringliteral}{        :reduction\_type:}
278 \textcolor{stringliteral}{            reduction type for the encoder}
279 \textcolor{stringliteral}{}
280 \textcolor{stringliteral}{        :return:}
281 \textcolor{stringliteral}{            a TransformerEncoder, initialized correctly}
282 \textcolor{stringliteral}{        """}
283         n\_positions = \hyperlink{namespaceparlai_1_1agents_1_1transformer_1_1modules_ab67607512c597ddd54f2b60a1a1eaf4c}{get\_n\_positions\_from\_options}(opt)
284         embeddings = torch.nn.Embedding(
285             len(dict), opt[\textcolor{stringliteral}{'embedding\_size'}], padding\_idx=null\_idx
286         )
287         torch.nn.init.normal\_(embeddings.weight, 0, opt[\textcolor{stringliteral}{'embedding\_size'}] ** -0.5)
288         \textcolor{keywordflow}{return} TransformerEncoder(
289             n\_heads=opt[\textcolor{stringliteral}{'n\_heads'}],
290             n\_layers=opt[\textcolor{stringliteral}{'n\_layers'}],
291             embedding\_size=opt[\textcolor{stringliteral}{'embedding\_size'}],
292             ffn\_size=opt[\textcolor{stringliteral}{'ffn\_size'}],
293             vocabulary\_size=len(dict),
294             embedding=embeddings,
295             dropout=opt[\textcolor{stringliteral}{'dropout'}],
296             attention\_dropout=opt[\textcolor{stringliteral}{'attention\_dropout'}],
297             relu\_dropout=opt[\textcolor{stringliteral}{'relu\_dropout'}],
298             padding\_idx=null\_idx,
299             learn\_positional\_embeddings=opt[\textcolor{stringliteral}{'learn\_positional\_embeddings'}],
300             embeddings\_scale=opt[\textcolor{stringliteral}{'embeddings\_scale'}],
301             reduction\_type=reduction\_type,
302             n\_positions=n\_positions,
303             n\_segments=2,
304             activation=opt[\textcolor{stringliteral}{'activation'}],
305             variant=opt[\textcolor{stringliteral}{'variant'}],
306             output\_scaling=opt[\textcolor{stringliteral}{'output\_scaling'}],
307         )
308 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a52218a8c0594ff4d5bf8e666dc38f508}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a52218a8c0594ff4d5bf8e666dc38f508}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!score@{score}}
\index{score@{score}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{score()}{score()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+score (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{ctxt\+\_\+rep,  }\item[{}]{ctxt\+\_\+rep\+\_\+mask,  }\item[{}]{ctxt\+\_\+pos,  }\item[{}]{cand\+\_\+embed }\end{DoxyParamCaption})}

\begin{DoxyVerb}Score the candidates.

:param ctxt_rep:
    3D float tensor, bsz x ctxt_len x dim
:param ctxt_rep_mask:
    2D byte tensor, bsz x ctxt_len, in case there are some elements
    of the ctxt that we should not take into account.
:param ctx_pos: 3D float tensor, bsz x sent_len x dim
:param cand_embed: 3D float tensor, bsz x num_cands x dim

:return: scores, 2D float tensor: bsz x num_cands
\end{DoxyVerb}
 

Definition at line 401 of file polyencoder.\+py.


\begin{DoxyCode}
401     \textcolor{keyword}{def }score(self, ctxt\_rep, ctxt\_rep\_mask, ctxt\_pos, cand\_embed):
402         \textcolor{stringliteral}{"""}
403 \textcolor{stringliteral}{        Score the candidates.}
404 \textcolor{stringliteral}{}
405 \textcolor{stringliteral}{        :param ctxt\_rep:}
406 \textcolor{stringliteral}{            3D float tensor, bsz x ctxt\_len x dim}
407 \textcolor{stringliteral}{        :param ctxt\_rep\_mask:}
408 \textcolor{stringliteral}{            2D byte tensor, bsz x ctxt\_len, in case there are some elements}
409 \textcolor{stringliteral}{            of the ctxt that we should not take into account.}
410 \textcolor{stringliteral}{        :param ctx\_pos: 3D float tensor, bsz x sent\_len x dim}
411 \textcolor{stringliteral}{        :param cand\_embed: 3D float tensor, bsz x num\_cands x dim}
412 \textcolor{stringliteral}{}
413 \textcolor{stringliteral}{        :return: scores, 2D float tensor: bsz x num\_cands}
414 \textcolor{stringliteral}{        """}
415         \textcolor{comment}{# Attention keys determined by self.attention\_keys}
416         \textcolor{comment}{# 'context' == use context final rep; otherwise use context position embs}
417         keys = ctxt\_rep \textcolor{keywordflow}{if} self.attention\_keys == \textcolor{stringliteral}{'context'} \textcolor{keywordflow}{else} ctxt\_pos
418         \textcolor{comment}{# reduces the context representation to a 3D tensor bsz x num\_cands x dim}
419         ctxt\_final\_rep = self.attend(
420             self.attention, cand\_embed, keys, ctxt\_rep, ctxt\_rep\_mask
421         )
422         scores = torch.sum(ctxt\_final\_rep * cand\_embed, 2)
423         \textcolor{keywordflow}{return} scores
424 
\end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8fab02ef33d1e1ee413e4bcbe25aa03b}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8fab02ef33d1e1ee413e4bcbe25aa03b}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attention@{attention}}
\index{attention@{attention}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attention}{attention}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attention}



Definition at line 255 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af29f6ff1656451ec4c35b4cc9ea0f5dc}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af29f6ff1656451ec4c35b4cc9ea0f5dc}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attention\+\_\+keys@{attention\+\_\+keys}}
\index{attention\+\_\+keys@{attention\+\_\+keys}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attention\+\_\+keys}{attention\_keys}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attention\+\_\+keys}



Definition at line 226 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acc2312ac770d343991de882c05da56e6}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_acc2312ac770d343991de882c05da56e6}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attention\+\_\+num\+\_\+heads@{attention\+\_\+num\+\_\+heads}}
\index{attention\+\_\+num\+\_\+heads@{attention\+\_\+num\+\_\+heads}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attention\+\_\+num\+\_\+heads}{attention\_num\_heads}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attention\+\_\+num\+\_\+heads}



Definition at line 227 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a2efc9276c4ab6d1533b7ab3ceef3ac3a}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a2efc9276c4ab6d1533b7ab3ceef3ac3a}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!attention\+\_\+type@{attention\+\_\+type}}
\index{attention\+\_\+type@{attention\+\_\+type}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{attention\+\_\+type}{attention\_type}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+attention\+\_\+type}



Definition at line 225 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a6740a4e891b964a41484a793ae0e6e6d}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a6740a4e891b964a41484a793ae0e6e6d}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!code\+\_\+attention@{code\+\_\+attention}}
\index{code\+\_\+attention@{code\+\_\+attention}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{code\+\_\+attention}{code\_attention}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+code\+\_\+attention}



Definition at line 241 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ac8d8955e72761c0775aeefea5ce80c3c}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ac8d8955e72761c0775aeefea5ce80c3c}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!codes@{codes}}
\index{codes@{codes}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{codes}{codes}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+codes}



Definition at line 237 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ae30ed526a2a4442222b6572a6fa41604}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_ae30ed526a2a4442222b6572a6fa41604}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!codes\+\_\+attention\+\_\+num\+\_\+heads@{codes\+\_\+attention\+\_\+num\+\_\+heads}}
\index{codes\+\_\+attention\+\_\+num\+\_\+heads@{codes\+\_\+attention\+\_\+num\+\_\+heads}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{codes\+\_\+attention\+\_\+num\+\_\+heads}{codes\_attention\_num\_heads}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+codes\+\_\+attention\+\_\+num\+\_\+heads}



Definition at line 229 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a80cf22447d874c72d3e9b64a64cf5747}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a80cf22447d874c72d3e9b64a64cf5747}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!codes\+\_\+attention\+\_\+type@{codes\+\_\+attention\+\_\+type}}
\index{codes\+\_\+attention\+\_\+type@{codes\+\_\+attention\+\_\+type}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{codes\+\_\+attention\+\_\+type}{codes\_attention\_type}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+codes\+\_\+attention\+\_\+type}



Definition at line 228 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af70c94c4e4073e804904b983a683c0a7}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_af70c94c4e4073e804904b983a683c0a7}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!encoder\+\_\+cand@{encoder\+\_\+cand}}
\index{encoder\+\_\+cand@{encoder\+\_\+cand}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{encoder\+\_\+cand}{encoder\_cand}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+encoder\+\_\+cand}



Definition at line 221 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9a5e58bfbd75ec89e10b8e4bcbcd4ab6}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a9a5e58bfbd75ec89e10b8e4bcbcd4ab6}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!encoder\+\_\+ctxt@{encoder\+\_\+ctxt}}
\index{encoder\+\_\+ctxt@{encoder\+\_\+ctxt}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{encoder\+\_\+ctxt}{encoder\_ctxt}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+encoder\+\_\+ctxt}



Definition at line 220 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8440bd98b6ee31c0540fdbc2e9efe987}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a8440bd98b6ee31c0540fdbc2e9efe987}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!n\+\_\+codes@{n\+\_\+codes}}
\index{n\+\_\+codes@{n\+\_\+codes}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{n\+\_\+codes}{n\_codes}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+n\+\_\+codes}



Definition at line 224 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a7a029490f5786bfa0fb2755e716118e4}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!null\+\_\+idx@{null\+\_\+idx}}
\index{null\+\_\+idx@{null\+\_\+idx}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{null\+\_\+idx}{null\_idx}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+null\+\_\+idx}



Definition at line 219 of file polyencoder.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a3eea152c32cd33d1e6b0ca680887d0e7}\label{classparlai_1_1agents_1_1transformer_1_1polyencoder_1_1PolyEncoderModule_a3eea152c32cd33d1e6b0ca680887d0e7}} 
\index{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}!type@{type}}
\index{type@{type}!parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module@{parlai\+::agents\+::transformer\+::polyencoder\+::\+Poly\+Encoder\+Module}}
\subsubsection{\texorpdfstring{type}{type}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+type}



Definition at line 223 of file polyencoder.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/transformer/\hyperlink{polyencoder_8py}{polyencoder.\+py}\end{DoxyCompactItemize}
