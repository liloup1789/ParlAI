\hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder}{}\section{parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder Class Reference}
\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder}\index{parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder@{parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder}}


Inheritance diagram for parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=247pt]{d4/d8d/classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=247pt]{d7/d21/classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_ac9356241c0dd44bf980a155124cbde59}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_af83edca868bacf80329be8606ffc7efb}{n\+\_\+heads}, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a20983026d2ed3edd48698ed08d666287}{n\+\_\+layers}, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_abb275df1d00ad62deb2424266f1563d6}{embedding\+\_\+size}, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a90c40d660300194a42a18dc38f7e9fb2}{ffn\+\_\+size}, vocabulary\+\_\+size, embedding=None, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a0d41303200949e8406daf8426ce7cbb3}{dropout}=0.\+0, attention\+\_\+dropout=0.\+0, relu\+\_\+dropout=0.\+0, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a45a356a825e02ed79cdc43cf7f5dc8af}{embeddings\+\_\+scale}=True, learn\+\_\+positional\+\_\+embeddings=False, padding\+\_\+idx=None, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a91210271cf67a77c0ab81f03f29bf70d}{n\+\_\+positions}=1024, n\+\_\+segments=0, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a84349da2592ce07a34257a3995dd2254}{variant}=\textquotesingle{}aiayn\textquotesingle{}, \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_aced85fffc20320f29be295f94e686a63}{activation}=\textquotesingle{}relu\textquotesingle{})
\item 
def \hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a19b2e9eecef1384bcfa9f772cac24a20}{forward} (self, input, encoder\+\_\+state, incr\+\_\+state=None)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_abb275df1d00ad62deb2424266f1563d6}{embedding\+\_\+size}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a90c40d660300194a42a18dc38f7e9fb2}{ffn\+\_\+size}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a20983026d2ed3edd48698ed08d666287}{n\+\_\+layers}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_af83edca868bacf80329be8606ffc7efb}{n\+\_\+heads}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_ab29b2b4f3a44d3069d208c61ddba6f21}{dim}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_aced85fffc20320f29be295f94e686a63}{activation}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a84349da2592ce07a34257a3995dd2254}{variant}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a45a356a825e02ed79cdc43cf7f5dc8af}{embeddings\+\_\+scale}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a0d41303200949e8406daf8426ce7cbb3}{dropout}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a91210271cf67a77c0ab81f03f29bf70d}{n\+\_\+positions}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a18e16e542264d07a8c70f0c5b0d2e5ce}{out\+\_\+dim}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_aa6d97e0eeed7b3cc11e4b60fb4a86167}{embeddings}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a4a930028ffdff0380897228bc156fe74}{norm\+\_\+embeddings}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a253da75d3014b859206169b2d692ab07}{position\+\_\+embeddings}
\item 
\hyperlink{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a5007810b22cdfddf768bdd94c233f87e}{layers}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Transformer Decoder layer.

:param int n_heads: the number of multihead attention heads.
:param int n_layers: number of transformer layers.
:param int embedding_size: the embedding sizes. Must be a multiple of n_heads.
:param int ffn_size: the size of the hidden layer in the FFN
:param embedding: an embedding matrix for the bottom layer of the transformer.
    If none, one is created for this encoder.
:param float dropout: Dropout used around embeddings and before layer
    layer normalizations. This is used in Vaswani 2017 and works well on
    large datasets.
:param float attention_dropout: Dropout performed after the multhead attention
    softmax. This is not used in Vaswani 2017.
:param float relu_attention: Dropout used after the ReLU in the FFN. Not used
    in Vaswani 2017, but used in Tensor2Tensor.
:param int padding_idx: Reserved padding index in the embeddings matrix.
:param bool learn_positional_embeddings: If off, sinusoidal embeddings are
    used. If on, position embeddings are learned from scratch.
:param bool embeddings_scale: Scale embeddings relative to their dimensionality.
    Found useful in fairseq.
:param int n_positions: Size of the position embeddings matrix.
\end{DoxyVerb}
 

Definition at line 593 of file modules.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_ac9356241c0dd44bf980a155124cbde59}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_ac9356241c0dd44bf980a155124cbde59}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{n\+\_\+heads,  }\item[{}]{n\+\_\+layers,  }\item[{}]{embedding\+\_\+size,  }\item[{}]{ffn\+\_\+size,  }\item[{}]{vocabulary\+\_\+size,  }\item[{}]{embedding = {\ttfamily None},  }\item[{}]{dropout = {\ttfamily 0.0},  }\item[{}]{attention\+\_\+dropout = {\ttfamily 0.0},  }\item[{}]{relu\+\_\+dropout = {\ttfamily 0.0},  }\item[{}]{embeddings\+\_\+scale = {\ttfamily True},  }\item[{}]{learn\+\_\+positional\+\_\+embeddings = {\ttfamily False},  }\item[{}]{padding\+\_\+idx = {\ttfamily None},  }\item[{}]{n\+\_\+positions = {\ttfamily 1024},  }\item[{}]{n\+\_\+segments = {\ttfamily 0},  }\item[{}]{variant = {\ttfamily \textquotesingle{}aiayn\textquotesingle{}},  }\item[{}]{activation = {\ttfamily \textquotesingle{}relu\textquotesingle{}} }\end{DoxyParamCaption})}



Definition at line 636 of file modules.\+py.


\begin{DoxyCode}
636     ):
637         super().\_\_init\_\_()
638         self.embedding\_size = embedding\_size
639         self.ffn\_size = ffn\_size
640         self.n\_layers = n\_layers
641         self.n\_heads = n\_heads
642         self.dim = embedding\_size
643         self.activation = activation
644         self.variant = variant
645         self.embeddings\_scale = embeddings\_scale
646         self.dropout = nn.Dropout(p=dropout)  \textcolor{comment}{# --dropout}
647 
648         self.n\_positions = n\_positions
649         self.out\_dim = embedding\_size
650         \textcolor{keyword}{assert} (
651             embedding\_size % n\_heads == 0
652         ), \textcolor{stringliteral}{'Transformer embedding size must be a multiple of n\_heads'}
653 
654         self.embeddings = embedding
655 
656         \textcolor{keywordflow}{if} self.variant == \textcolor{stringliteral}{'xlm'}:
657             self.norm\_embeddings = LayerNorm(self.dim, eps=LAYER\_NORM\_EPS)
658         \textcolor{keywordflow}{elif} self.variant == \textcolor{stringliteral}{'aiayn'}:
659             \textcolor{keywordflow}{pass}
660         \textcolor{keywordflow}{else}:
661             \textcolor{keywordflow}{raise} ValueError(\textcolor{stringliteral}{"Can't handle --variant \{\}"}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(self.variant))
662 
663         \textcolor{comment}{# create the positional embeddings}
664         self.position\_embeddings = nn.Embedding(n\_positions, embedding\_size)
665         \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} learn\_positional\_embeddings:
666             \hyperlink{namespaceparlai_1_1agents_1_1transformer_1_1modules_a0b86437e6e9682fa3100e9cadcaae259}{create\_position\_codes}(
667                 n\_positions, embedding\_size, out=self.position\_embeddings.weight
668             )
669         \textcolor{keywordflow}{else}:
670             nn.init.normal\_(self.position\_embeddings.weight, 0, embedding\_size ** -0.5)
671 
672         \textcolor{comment}{# build the model}
673         self.layers = nn.ModuleList()
674         \textcolor{keywordflow}{for} \_ \textcolor{keywordflow}{in} range(self.n\_layers):
675             self.layers.append(
676                 TransformerDecoderLayer(
677                     n\_heads,
678                     embedding\_size,
679                     ffn\_size,
680                     attention\_dropout=attention\_dropout,
681                     relu\_dropout=relu\_dropout,
682                     dropout=dropout,
683                     activation=activation,
684                     variant=variant,
685                 )
686             )
687 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a19b2e9eecef1384bcfa9f772cac24a20}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a19b2e9eecef1384bcfa9f772cac24a20}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!forward@{forward}}
\index{forward@{forward}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{input,  }\item[{}]{encoder\+\_\+state,  }\item[{}]{incr\+\_\+state = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Forward pass.

:param LongTensor[batch,seqlen] input:
    The decoder inputs (partial or full decoded token IDs).
:param encoder_state:
    Output from the encoder module forward pass.
:param incr_state:
    The incremental state: a dictionary whose keys index the layers and whose
    values contain the incremental state for each layer.
\end{DoxyVerb}
 

Definition at line 688 of file modules.\+py.


\begin{DoxyCode}
688     \textcolor{keyword}{def }forward(self, input, encoder\_state, incr\_state=None):
689         \textcolor{stringliteral}{"""}
690 \textcolor{stringliteral}{        Forward pass.}
691 \textcolor{stringliteral}{}
692 \textcolor{stringliteral}{        :param LongTensor[batch,seqlen] input:}
693 \textcolor{stringliteral}{            The decoder inputs (partial or full decoded token IDs).}
694 \textcolor{stringliteral}{        :param encoder\_state:}
695 \textcolor{stringliteral}{            Output from the encoder module forward pass.}
696 \textcolor{stringliteral}{        :param incr\_state:}
697 \textcolor{stringliteral}{            The incremental state: a dictionary whose keys index the layers and whose}
698 \textcolor{stringliteral}{            values contain the incremental state for each layer.}
699 \textcolor{stringliteral}{        """}
700         encoder\_output, encoder\_mask = encoder\_state
701 
702         seq\_len = input.size(1)
703         positions = input.new(seq\_len).long()
704         positions = torch.arange(seq\_len, out=positions).unsqueeze(0)
705 
706         \textcolor{keywordflow}{if} incr\_state \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
707             \textcolor{comment}{# We're doing incremental decoding, so select only the most recent position}
708             input = input[:, -1:]
709             \textcolor{keywordflow}{if} positions \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
710                 positions = positions[:, -1:]
711         \textcolor{keywordflow}{else}:
712             incr\_state = \{idx: \{\} \textcolor{keywordflow}{for} idx \textcolor{keywordflow}{in} range(len(self.layers))\}
713 
714         tensor = self.embeddings(input)
715         \textcolor{keywordflow}{if} self.embeddings\_scale:
716             tensor = tensor * np.sqrt(self.dim)
717         \textcolor{keywordflow}{if} self.variant == \textcolor{stringliteral}{'xlm'}:
718             tensor = \_normalize(tensor, self.norm\_embeddings)
719         \textcolor{keywordflow}{if} positions.max().item() > self.n\_positions:
720             \hyperlink{namespaceparlai_1_1utils_1_1misc_a884a3aefa90581f53bc592fa6a78dc43}{warn\_once}(
721                 \textcolor{stringliteral}{'You are inputting a sequence of \{x\} length, but only have '}
722                 \textcolor{stringliteral}{'--n-positions \{y\}. Set --truncate or increase --n-positions'}.
      \hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(
723                     x=positions.max().item(), y=self.n\_positions
724                 )
725             )
726         tensor = tensor + self.position\_embeddings(positions).expand\_as(tensor)
727         tensor = self.dropout(tensor)  \textcolor{comment}{# --dropout}
728 
729         new\_incr\_state = \{\}
730         \textcolor{keywordflow}{for} idx, layer \textcolor{keywordflow}{in} enumerate(self.layers):
731             tensor, new\_incr\_state[idx] = layer(
732                 x=tensor,
733                 encoder\_output=encoder\_output,
734                 encoder\_mask=encoder\_mask,
735                 incr\_state=incr\_state[idx],
736             )
737 
738         \textcolor{keywordflow}{return} tensor, new\_incr\_state
739 
740 
\end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_aced85fffc20320f29be295f94e686a63}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_aced85fffc20320f29be295f94e686a63}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!activation@{activation}}
\index{activation@{activation}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{activation}{activation}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+activation}



Definition at line 643 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_ab29b2b4f3a44d3069d208c61ddba6f21}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_ab29b2b4f3a44d3069d208c61ddba6f21}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!dim@{dim}}
\index{dim@{dim}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{dim}{dim}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+dim}



Definition at line 642 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a0d41303200949e8406daf8426ce7cbb3}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a0d41303200949e8406daf8426ce7cbb3}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!dropout@{dropout}}
\index{dropout@{dropout}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{dropout}{dropout}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+dropout}



Definition at line 646 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_abb275df1d00ad62deb2424266f1563d6}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_abb275df1d00ad62deb2424266f1563d6}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!embedding\+\_\+size@{embedding\+\_\+size}}
\index{embedding\+\_\+size@{embedding\+\_\+size}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{embedding\+\_\+size}{embedding\_size}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+embedding\+\_\+size}



Definition at line 638 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_aa6d97e0eeed7b3cc11e4b60fb4a86167}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_aa6d97e0eeed7b3cc11e4b60fb4a86167}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!embeddings@{embeddings}}
\index{embeddings@{embeddings}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{embeddings}{embeddings}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+embeddings}



Definition at line 654 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a45a356a825e02ed79cdc43cf7f5dc8af}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a45a356a825e02ed79cdc43cf7f5dc8af}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!embeddings\+\_\+scale@{embeddings\+\_\+scale}}
\index{embeddings\+\_\+scale@{embeddings\+\_\+scale}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{embeddings\+\_\+scale}{embeddings\_scale}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+embeddings\+\_\+scale}



Definition at line 645 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a90c40d660300194a42a18dc38f7e9fb2}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a90c40d660300194a42a18dc38f7e9fb2}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!ffn\+\_\+size@{ffn\+\_\+size}}
\index{ffn\+\_\+size@{ffn\+\_\+size}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{ffn\+\_\+size}{ffn\_size}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+ffn\+\_\+size}



Definition at line 639 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a5007810b22cdfddf768bdd94c233f87e}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a5007810b22cdfddf768bdd94c233f87e}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!layers@{layers}}
\index{layers@{layers}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{layers}{layers}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+layers}



Definition at line 673 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_af83edca868bacf80329be8606ffc7efb}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_af83edca868bacf80329be8606ffc7efb}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!n\+\_\+heads@{n\+\_\+heads}}
\index{n\+\_\+heads@{n\+\_\+heads}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{n\+\_\+heads}{n\_heads}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+n\+\_\+heads}



Definition at line 641 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a20983026d2ed3edd48698ed08d666287}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a20983026d2ed3edd48698ed08d666287}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!n\+\_\+layers@{n\+\_\+layers}}
\index{n\+\_\+layers@{n\+\_\+layers}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{n\+\_\+layers}{n\_layers}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+n\+\_\+layers}



Definition at line 640 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a91210271cf67a77c0ab81f03f29bf70d}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a91210271cf67a77c0ab81f03f29bf70d}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!n\+\_\+positions@{n\+\_\+positions}}
\index{n\+\_\+positions@{n\+\_\+positions}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{n\+\_\+positions}{n\_positions}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+n\+\_\+positions}



Definition at line 648 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a4a930028ffdff0380897228bc156fe74}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a4a930028ffdff0380897228bc156fe74}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!norm\+\_\+embeddings@{norm\+\_\+embeddings}}
\index{norm\+\_\+embeddings@{norm\+\_\+embeddings}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{norm\+\_\+embeddings}{norm\_embeddings}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+norm\+\_\+embeddings}



Definition at line 657 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a18e16e542264d07a8c70f0c5b0d2e5ce}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a18e16e542264d07a8c70f0c5b0d2e5ce}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!out\+\_\+dim@{out\+\_\+dim}}
\index{out\+\_\+dim@{out\+\_\+dim}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{out\+\_\+dim}{out\_dim}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+out\+\_\+dim}



Definition at line 649 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a253da75d3014b859206169b2d692ab07}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a253da75d3014b859206169b2d692ab07}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!position\+\_\+embeddings@{position\+\_\+embeddings}}
\index{position\+\_\+embeddings@{position\+\_\+embeddings}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{position\+\_\+embeddings}{position\_embeddings}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+position\+\_\+embeddings}



Definition at line 664 of file modules.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a84349da2592ce07a34257a3995dd2254}\label{classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_a84349da2592ce07a34257a3995dd2254}} 
\index{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}!variant@{variant}}
\index{variant@{variant}!parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder@{parlai\+::agents\+::transformer\+::modules\+::\+Transformer\+Decoder}}
\subsubsection{\texorpdfstring{variant}{variant}}
{\footnotesize\ttfamily parlai.\+agents.\+transformer.\+modules.\+Transformer\+Decoder.\+variant}



Definition at line 644 of file modules.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/transformer/\hyperlink{parlai_2agents_2transformer_2modules_8py}{modules.\+py}\end{DoxyCompactItemize}
