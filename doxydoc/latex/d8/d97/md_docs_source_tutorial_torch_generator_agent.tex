{\bfseries Authors}\+: Eric Smith

{\ttfamily \hyperlink{classparlai_1_1core_1_1torch__generator__agent_1_1TorchGeneratorAgent}{parlai.\+core.\+torch\+\_\+generator\+\_\+agent.\+Torch\+Generator\+Agent}} is an abstract parent class that provides functionality for building autoregressive generative models. Extending {\ttfamily Torch\+Generator\+Agent} requires your model conform to a strict interface, but then provides you rich functionality like beam search and sampling.

\subsection*{Example Models}

Two major models in Parl\+AI inherit from {\ttfamily Torch\+Generator\+Agent}\+: \hyperlink{namespaceseq2seq}{seq2seq} and transformer. You can try the transformer with the example below\+:


\begin{DoxyCode}
python examples/train\_model -m transformer/generator -t convai2 -mf /tmp/testtransformer \(\backslash\)
  --beam-size 5 -bs 16
\end{DoxyCode}


\subsection*{Creating a Model}

In order to write a generative model, your agent should extend {\ttfamily Torch\+Generator\+Agent}. This parent class implements {\ttfamily train\+\_\+step} and {\ttfamily eval\+\_\+step}, so you only need to implement your model and instantiate it through {\ttfamily build\+\_\+model}. {\ttfamily Torch\+Generator\+Agent} will take care of many common generator features, such as forced decoding, beam search, n-\/gram beam blocking, top-\/k and top-\/p/nucleus sampling, etc.

Additionally, your model should implement the {\ttfamily Torch\+Generator\+Model} interface\+: see the tutorial below for an example of this.

\subsection*{Tutorial}

This tutorial will walk you through creating a simple generative model, found at {\ttfamily \hyperlink{namespaceparlai_1_1agents_1_1examples_1_1seq2seq}{parlai.\+agents.\+examples.\+seq2seq}}, that consists of a 1-\/layer-\/\+L\+S\+TM encoder and decoder.

\subsubsection*{Extending {\ttfamily Torch\+Generator\+Agent}}

Creating a generative model in Parl\+AI consists of subclassing {\ttfamily Torch\+Generator\+Agent} and subclassing {\ttfamily Torch\+Generator\+Model}. A minimal subclass of {\ttfamily Torch\+Generator\+Agent} only needs to implement {\ttfamily build\+\_\+model()}, but if you want to specify any command-\/line arguments, you\textquotesingle{}ll need to add {\ttfamily \hyperlink{namespaceparlai_1_1agents_1_1drqa_1_1config_a62fdd5554f1da6be0cba185271058320}{add\+\_\+cmdline\+\_\+args()}} as well. Our implementation below first adds flags for {\ttfamily Torch\+Generator\+Agent} and then adds a {\ttfamily -\/-\/hidden-\/size} flag for the hidden dimension of the L\+S\+T\+Ms of the encoder and decoder.

In {\ttfamily build\+\_\+model()}, we instantiate our example model (defined below) by passing in the agent\textquotesingle{}s dict (set by {\ttfamily Torch\+Agent}) and the hidden size. We add lines to optionally copy pre-\/existing token embeddings into the model\textquotesingle{}s embedding table.

Altogether, our example agent is defined as follows\+:


\begin{DoxyCode}
import parlai.core.torch\_generator\_agent as tga


class Seq2seqAgent(tga.TorchGeneratorAgent):

    @classmethod
    def add\_cmdline\_args(cls, argparser):
        super(Seq2seqAgent, cls).add\_cmdline\_args(argparser)
        group = argparser.add\_argument\_group('Example TGA Agent')
        group.add\_argument(
            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'
        )

    def build\_model(self):
        model = ExampleModel(self.dict, self.opt['hidden\_size'])
        if self.opt['embedding\_type'] != 'random':
            self.\_copy\_embeddings(
                model.embeddings.weight, self.opt['embedding\_type']
            )
        return model
\end{DoxyCode}


\subsubsection*{Extending {\ttfamily Torch\+Generator\+Model}}

We now subclass {\ttfamily Torch\+Generator\+Model} to create {\ttfamily Example\+Model}. We initialize this by first calling {\ttfamily super().\+\_\+\+\_\+init\+\_\+\+\_\+()} and passing in dictionary tokens for padding, start, end, and U\+N\+Ks; we then create an embedding lookup table with {\ttfamily nn.\+Embedding} and instantiate the encoder and decoder, described in the following sections.


\begin{DoxyCode}
import torch.nn as nn
import torch.nn.functional as F

class ExampleModel(tga.TorchGeneratorModel):

    def \_\_init\_\_(self, dictionary, esz=256, hidden\_size=1024):
        super().\_\_init\_\_(
            padding\_idx=dictionary[dictionary.null\_token],
            start\_idx=dictionary[dictionary.start\_token],
            end\_idx=dictionary[dictionary.end\_token],
            unknown\_idx=dictionary[dictionary.unk\_token],
        )
        self.embeddings = nn.Embedding(len(dictionary), esz)
        self.encoder = Encoder(self.embeddings, hidden\_size)
        self.decoder = Decoder(self.embeddings, hidden\_size)
\end{DoxyCode}


We next define a function to project the output of the decoder back into the token space\+:


\begin{DoxyCode}
def output(self, decoder\_output):
    return F.linear(decoder\_output, self.embeddings.weight)
\end{DoxyCode}


Lastly, we define two functions to reindex the latent states of the encoder and decoder. For the encoder, the indices that we pass in index the samples in the batch, and for the decoder, the indices index the candidates that we want to retain for the next step of decoding (for instance, in beam search). We reindex the encoder at the very beginning of beam search and when ranking candidates during eval, and we reindex the decoder after each step of decoding. Since our encoder and decoder both are based on L\+S\+T\+Ms, these encoder/decoder states are the hidden and cell states\+: 
\begin{DoxyCode}
def reorder\_encoder\_states(self, encoder\_states, indices):
    h, c = encoder\_states
    return h[:, indices, :], c[:, indices, :]

def reorder\_decoder\_incremental\_state(self, incr\_state, indices):
    h, c = incr\_state
    return h[:, indices, :], c[:, indices, :]
\end{DoxyCode}


\subsubsection*{Creating the encoder}

The encoder is straightfoward\+: it contains an embedding layer and an L\+S\+TM, and a forward pass through the encoder consists of passing the sequences of input tokens through both of them sequentially. The final hidden state is returned.


\begin{DoxyCode}
class Encoder(nn.Module):

    def \_\_init\_\_(self, embeddings, hidden\_size):
        super().\_\_init\_\_()
        \_vocab\_size, esz = embeddings.weight.shape
        self.embeddings = embeddings
        self.lstm = nn.LSTM(
            input\_size=esz, hidden\_size=hidden\_size, num\_layers=1, batch\_first=True
        )

    def forward(self, input\_tokens):
        embedded = self.embeddings(input\_tokens)
        \_output, hidden = self.lstm(embedded)
        return hidden
\end{DoxyCode}


\subsubsection*{Creating the decoder}

The decoder is initialized in the same way as the encoder, but now the forward pass reflects the fact that the input tokens need to be passed through the embedder and L\+S\+TM one token at a time rather than all at once. If this is the first pass through the decoder, we pass a tuple {\ttfamily encoder\+\_\+state} to the L\+S\+TM that consists of the initial hidden and cell state, as taken from the output of the encoder. If this is a subsequent pass through the decoder, the L\+S\+TM will have given us the current values of the hidden and cell states, so we pass that back in to the L\+S\+TM, after potentially having reindexed the states with {\ttfamily Example\+Model().reorder\+\_\+decoder\+\_\+incremental\+\_\+state()}.


\begin{DoxyCode}
class Decoder(nn.Module):

    def \_\_init\_\_(self, embeddings, hidden\_size):
        super().\_\_init\_\_()
        \_vocab\_size, self.esz = embeddings.weight.shape
        self.embeddings = embeddings
        self.lstm = nn.LSTM(
            input\_size=self.esz, hidden\_size=hidden\_size, num\_layers=1, batch\_first=True
        )

    def forward(self, input, encoder\_state, incr\_state=None):
        embedded = self.embeddings(input)
        if incr\_state is None:
            state = encoder\_state
        else:
            state = incr\_state
        output, incr\_state = self.lstm(embedded, state)
        return output, incr\_state
\end{DoxyCode}


\subsubsection*{Training}

The full code for the agent can be seen \href{https://github.com/facebookresearch/ParlAI/tree/master/parlai/agents/examples/seq2seq.py}{\tt here}. To call training\+:


\begin{DoxyCode}
python examples/train\_model.py -m examples/seq2seq \(\backslash\)
    -mf /tmp/example\_model \(\backslash\)
    -t convai2 -bs 32 -eps 2 --truncate 128
\end{DoxyCode}


You should get a perplexity of around 140 and a token accuracy of around 28\% on the Conv\+A\+I2 validation/test set. 