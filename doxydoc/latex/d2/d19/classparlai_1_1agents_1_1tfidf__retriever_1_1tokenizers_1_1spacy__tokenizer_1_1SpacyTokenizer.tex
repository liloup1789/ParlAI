\hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer}{}\section{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer Class Reference}
\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer}\index{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer@{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer}}


Inheritance diagram for parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=231pt]{da/d55/classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=231pt]{d1/d0f/classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a091dbfa93db9f2dfdaae3779c24aecb0}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, kwargs)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a479b83909bafee8db3f60c9d8fe69635}{tokenize} (self, text)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a6d3d9c1c8632f0b1ec559b3bbef9fe60}{annotators}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a7124470b153c9ed25f1c7a1404dd8e1d}{nlp}
\end{DoxyCompactItemize}


\subsection{Detailed Description}


Definition at line 18 of file spacy\+\_\+tokenizer.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a091dbfa93db9f2dfdaae3779c24aecb0}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a091dbfa93db9f2dfdaae3779c24aecb0}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{kwargs }\end{DoxyParamCaption})}

\begin{DoxyVerb}Args:
    annotators: set that can include pos, lemma, and ner.
    model: spaCy model to use (either path, or keyword like 'en').
\end{DoxyVerb}
 

Definition at line 19 of file spacy\+\_\+tokenizer.\+py.


\begin{DoxyCode}
19     \textcolor{keyword}{def }\_\_init\_\_(self, **kwargs):
20         \textcolor{stringliteral}{"""}
21 \textcolor{stringliteral}{        Args:}
22 \textcolor{stringliteral}{            annotators: set that can include pos, lemma, and ner.}
23 \textcolor{stringliteral}{            model: spaCy model to use (either path, or keyword like 'en').}
24 \textcolor{stringliteral}{        """}
25         model = kwargs.get(\textcolor{stringliteral}{'model'}, \textcolor{stringliteral}{'en'})
26         self.annotators = copy.deepcopy(kwargs.get(\textcolor{stringliteral}{'annotators'}, set()))
27         nlp\_kwargs = \{\textcolor{stringliteral}{'parser'}: \textcolor{keyword}{False}\}
28         \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \{\textcolor{stringliteral}{'lemma'}, \textcolor{stringliteral}{'pos'}, \textcolor{stringliteral}{'ner'}\} & self.annotators:
29             nlp\_kwargs[\textcolor{stringliteral}{'tagger'}] = \textcolor{keyword}{False}
30         \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \{\textcolor{stringliteral}{'ner'}\} & self.annotators:
31             nlp\_kwargs[\textcolor{stringliteral}{'entity'}] = \textcolor{keyword}{False}
32         self.nlp = spacy.load(model, **nlp\_kwargs)
33 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a479b83909bafee8db3f60c9d8fe69635}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a479b83909bafee8db3f60c9d8fe69635}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}!tokenize@{tokenize}}
\index{tokenize@{tokenize}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}}
\subsubsection{\texorpdfstring{tokenize()}{tokenize()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+tokenize (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{text }\end{DoxyParamCaption})}



Definition at line 34 of file spacy\+\_\+tokenizer.\+py.


\begin{DoxyCode}
34     \textcolor{keyword}{def }\hyperlink{namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf_a1fdb457e98eb4e4c26047e229686a616}{tokenize}(self, text):
35         \textcolor{comment}{# We don't treat new lines as tokens.}
36         clean\_text = text.replace(\textcolor{stringliteral}{'\(\backslash\)n'}, \textcolor{stringliteral}{' '})
37         tokens = self.nlp.tokenizer(clean\_text)
38         \textcolor{keywordflow}{if} \{\textcolor{stringliteral}{'lemma'}, \textcolor{stringliteral}{'pos'}, \textcolor{stringliteral}{'ner'}\} & self.annotators:
39             self.nlp.tagger(tokens)
40         \textcolor{keywordflow}{if} \{\textcolor{stringliteral}{'ner'}\} & self.annotators:
41             self.nlp.entity(tokens)
42 
43         data = []
44         \textcolor{keywordflow}{for} i \textcolor{keywordflow}{in} range(len(tokens)):
45             \textcolor{comment}{# Get whitespace}
46             start\_ws = tokens[i].idx
47             \textcolor{keywordflow}{if} i + 1 < len(tokens):
48                 end\_ws = tokens[i + 1].idx
49             \textcolor{keywordflow}{else}:
50                 end\_ws = tokens[i].idx + len(tokens[i].text)
51 
52             data.append(
53                 (
54                     tokens[i].text,
55                     text[start\_ws:end\_ws],
56                     (tokens[i].idx, tokens[i].idx + len(tokens[i].text)),
57                     tokens[i].tag\_,
58                     tokens[i].lemma\_,
59                     tokens[i].ent\_type\_,
60                 )
61             )
62 
63         \textcolor{comment}{# Set special option for non-entity tag: '' vs 'O' in spaCy}
64         \textcolor{keywordflow}{return} Tokens(data, self.annotators, opts=\{\textcolor{stringliteral}{'non\_ent'}: \textcolor{stringliteral}{''}\})
65 \end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a6d3d9c1c8632f0b1ec559b3bbef9fe60}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a6d3d9c1c8632f0b1ec559b3bbef9fe60}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}!annotators@{annotators}}
\index{annotators@{annotators}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}}
\subsubsection{\texorpdfstring{annotators}{annotators}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+annotators}



Definition at line 26 of file spacy\+\_\+tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a7124470b153c9ed25f1c7a1404dd8e1d}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a7124470b153c9ed25f1c7a1404dd8e1d}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}!nlp@{nlp}}
\index{nlp@{nlp}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}}
\subsubsection{\texorpdfstring{nlp}{nlp}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+nlp}



Definition at line 32 of file spacy\+\_\+tokenizer.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/tfidf\+\_\+retriever/tokenizers/\hyperlink{spacy__tokenizer_8py}{spacy\+\_\+tokenizer.\+py}\end{DoxyCompactItemize}
