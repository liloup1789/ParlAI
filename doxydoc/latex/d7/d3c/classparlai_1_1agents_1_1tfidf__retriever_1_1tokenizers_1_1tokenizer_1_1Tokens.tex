\hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens}{}\section{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens Class Reference}
\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens}\index{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens@{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens}}


Inheritance diagram for parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=231pt]{d3/d75/classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=231pt]{db/dbb/classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a302d05e18dc97ba20cdfb879f3442b88}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ad5068b92d72245d43fd75dbb4c070a27}{data}, \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a394834203475385444a933440dcdd026}{annotators}, \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ada7339fd7da0200a4260970771f9bfe9}{opts}=None)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_aa4d7fdac5660d51ef4b59712142c5ef4}{\+\_\+\+\_\+len\+\_\+\+\_\+} (self)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a8e68a443796116164126c7216f5963d3}{slice} (self, i=None, j=None)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a1bea40b1ea8e1772fb8ae6c2157ee819}{untokenize} (self)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a81b6d47a051c87c7f885a6f7db5ac891}{words} (self, uncased=False)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ad64d5292cef77a2a02193ba836875ba7}{offsets} (self)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a5aa226b8190b5e72dbe0515caf0a21d6}{pos} (self)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_acc0353a15e5585dae3bd08943b9abe0c}{lemmas} (self)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ab5d74ebfda5597b48f1ddc5f607292de}{entities} (self)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ae66797d00194acc3c67b42c7cf17dabf}{ngrams} (self, n=1, uncased=False, filter\+\_\+fn=None, as\+\_\+strings=True)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_af76da989bcd6090398f0e63b8c30de22}{entity\+\_\+groups} (self)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ad5068b92d72245d43fd75dbb4c070a27}{data}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a394834203475385444a933440dcdd026}{annotators}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ada7339fd7da0200a4260970771f9bfe9}{opts}
\end{DoxyCompactItemize}
\subsection*{Static Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a95137955c2cf11d2d28434ece03adf7a}{T\+E\+XT}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a319701df20f2f209c615adbab8c445f4}{T\+E\+X\+T\+\_\+\+WS}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a53956921f62121488f2ff918f0692bf9}{S\+P\+AN}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a10bd510e7cf4ee6c633ce7fec8aadbaf}{P\+OS}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ae82009092f014a3458777aeb5427ccf4}{L\+E\+M\+MA}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a730dffdd2e5c859fb396c707830a6633}{N\+ER}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}A class to represent a list of tokenized text.
\end{DoxyVerb}
 

Definition at line 14 of file tokenizer.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a302d05e18dc97ba20cdfb879f3442b88}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a302d05e18dc97ba20cdfb879f3442b88}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{data,  }\item[{}]{annotators,  }\item[{}]{opts = {\ttfamily None} }\end{DoxyParamCaption})}



Definition at line 26 of file tokenizer.\+py.


\begin{DoxyCode}
26     \textcolor{keyword}{def }\_\_init\_\_(self, data, annotators, opts=None):
27         self.data = data
28         self.annotators = annotators
29         self.opts = opts \textcolor{keywordflow}{or} \{\}
30 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_aa4d7fdac5660d51ef4b59712142c5ef4}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_aa4d7fdac5660d51ef4b59712142c5ef4}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!\+\_\+\+\_\+len\+\_\+\+\_\+@{\+\_\+\+\_\+len\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+len\+\_\+\+\_\+@{\+\_\+\+\_\+len\+\_\+\+\_\+}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+len\+\_\+\+\_\+()}{\_\_len\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+\_\+\+\_\+len\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}The number of tokens.
\end{DoxyVerb}
 

Definition at line 31 of file tokenizer.\+py.


\begin{DoxyCode}
31     \textcolor{keyword}{def }\_\_len\_\_(self):
32         \textcolor{stringliteral}{"""}
33 \textcolor{stringliteral}{        The number of tokens.}
34 \textcolor{stringliteral}{        """}
35         \textcolor{keywordflow}{return} len(self.data)
36 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ab5d74ebfda5597b48f1ddc5f607292de}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ab5d74ebfda5597b48f1ddc5f607292de}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!entities@{entities}}
\index{entities@{entities}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{entities()}{entities()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+entities (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Returns a list of named-entity-recognition tags of each token.

Returns None if this annotation was not included.
\end{DoxyVerb}
 

Definition at line 89 of file tokenizer.\+py.


\begin{DoxyCode}
89     \textcolor{keyword}{def }entities(self):
90         \textcolor{stringliteral}{"""}
91 \textcolor{stringliteral}{        Returns a list of named-entity-recognition tags of each token.}
92 \textcolor{stringliteral}{}
93 \textcolor{stringliteral}{        Returns None if this annotation was not included.}
94 \textcolor{stringliteral}{        """}
95         \textcolor{keywordflow}{if} \textcolor{stringliteral}{'ner'} \textcolor{keywordflow}{not} \textcolor{keywordflow}{in} self.annotators:
96             \textcolor{keywordflow}{return} \textcolor{keywordtype}{None}
97         \textcolor{keywordflow}{return} [t[self.NER] \textcolor{keywordflow}{for} t \textcolor{keywordflow}{in} self.data]
98 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_af76da989bcd6090398f0e63b8c30de22}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_af76da989bcd6090398f0e63b8c30de22}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!entity\+\_\+groups@{entity\+\_\+groups}}
\index{entity\+\_\+groups@{entity\+\_\+groups}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{entity\+\_\+groups()}{entity\_groups()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+entity\+\_\+groups (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Group consecutive entity tokens with the same NER tag.
\end{DoxyVerb}
 

Definition at line 130 of file tokenizer.\+py.


\begin{DoxyCode}
130     \textcolor{keyword}{def }entity\_groups(self):
131         \textcolor{stringliteral}{"""}
132 \textcolor{stringliteral}{        Group consecutive entity tokens with the same NER tag.}
133 \textcolor{stringliteral}{        """}
134         entities = self.entities()
135         \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} entities:
136             \textcolor{keywordflow}{return} \textcolor{keywordtype}{None}
137         non\_ent = self.opts.get(\textcolor{stringliteral}{'non\_ent'}, \textcolor{stringliteral}{'O'})
138         groups = []
139         idx = 0
140         \textcolor{keywordflow}{while} idx < len(entities):
141             ner\_tag = entities[idx]
142             \textcolor{comment}{# Check for entity tag}
143             \textcolor{keywordflow}{if} ner\_tag != non\_ent:
144                 \textcolor{comment}{# Chomp the sequence}
145                 start = idx
146                 \textcolor{keywordflow}{while} idx < len(entities) \textcolor{keywordflow}{and} entities[idx] == ner\_tag:
147                     idx += 1
148                 groups.append((self.slice(start, idx).untokenize(), ner\_tag))
149             \textcolor{keywordflow}{else}:
150                 idx += 1
151         \textcolor{keywordflow}{return} groups
152 
153 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_acc0353a15e5585dae3bd08943b9abe0c}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_acc0353a15e5585dae3bd08943b9abe0c}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!lemmas@{lemmas}}
\index{lemmas@{lemmas}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{lemmas()}{lemmas()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+lemmas (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Returns a list of the lemmatized text of each token.

Returns None if this annotation was not included.
\end{DoxyVerb}
 

Definition at line 79 of file tokenizer.\+py.


\begin{DoxyCode}
79     \textcolor{keyword}{def }lemmas(self):
80         \textcolor{stringliteral}{"""}
81 \textcolor{stringliteral}{        Returns a list of the lemmatized text of each token.}
82 \textcolor{stringliteral}{}
83 \textcolor{stringliteral}{        Returns None if this annotation was not included.}
84 \textcolor{stringliteral}{        """}
85         \textcolor{keywordflow}{if} \textcolor{stringliteral}{'lemma'} \textcolor{keywordflow}{not} \textcolor{keywordflow}{in} self.annotators:
86             \textcolor{keywordflow}{return} \textcolor{keywordtype}{None}
87         \textcolor{keywordflow}{return} [t[self.LEMMA] \textcolor{keywordflow}{for} t \textcolor{keywordflow}{in} self.data]
88 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ae66797d00194acc3c67b42c7cf17dabf}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ae66797d00194acc3c67b42c7cf17dabf}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!ngrams@{ngrams}}
\index{ngrams@{ngrams}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{ngrams()}{ngrams()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+ngrams (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{n = {\ttfamily 1},  }\item[{}]{uncased = {\ttfamily False},  }\item[{}]{filter\+\_\+fn = {\ttfamily None},  }\item[{}]{as\+\_\+strings = {\ttfamily True} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Returns a list of all ngrams from length 1 to n.

Args:
    n: upper limit of ngram length
    uncased: lower cases text
    filter_fn: user function that takes in an ngram list and returns
      True or False to keep or not keep the ngram
    as_string: return the ngram as a string vs list
\end{DoxyVerb}
 

Definition at line 99 of file tokenizer.\+py.


\begin{DoxyCode}
99     \textcolor{keyword}{def }ngrams(self, n=1, uncased=False, filter\_fn=None, as\_strings=True):
100         \textcolor{stringliteral}{"""}
101 \textcolor{stringliteral}{        Returns a list of all ngrams from length 1 to n.}
102 \textcolor{stringliteral}{}
103 \textcolor{stringliteral}{        Args:}
104 \textcolor{stringliteral}{            n: upper limit of ngram length}
105 \textcolor{stringliteral}{            uncased: lower cases text}
106 \textcolor{stringliteral}{            filter\_fn: user function that takes in an ngram list and returns}
107 \textcolor{stringliteral}{              True or False to keep or not keep the ngram}
108 \textcolor{stringliteral}{            as\_string: return the ngram as a string vs list}
109 \textcolor{stringliteral}{        """}
110 
111         \textcolor{keyword}{def }\_skip(gram):
112             \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} filter\_fn:
113                 \textcolor{keywordflow}{return} \textcolor{keyword}{False}
114             \textcolor{keywordflow}{return} filter\_fn(gram)
115 
116         words = self.words(uncased)
117         ngrams = [
118             (s, e + 1)
119             \textcolor{keywordflow}{for} s \textcolor{keywordflow}{in} range(len(words))
120             \textcolor{keywordflow}{for} e \textcolor{keywordflow}{in} range(s, min(s + n, len(words)))
121             \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} \_skip(words[s : e + 1])
122         ]
123 
124         \textcolor{comment}{# Concatenate into strings}
125         \textcolor{keywordflow}{if} as\_strings:
126             ngrams = [\textcolor{stringliteral}{'\{\}'}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(\textcolor{stringliteral}{' '}.join(words[s:e])) \textcolor{keywordflow}{for} (s, e) \textcolor{keywordflow}{in} ngrams]
127 
128         \textcolor{keywordflow}{return} ngrams
129 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ad64d5292cef77a2a02193ba836875ba7}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ad64d5292cef77a2a02193ba836875ba7}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!offsets@{offsets}}
\index{offsets@{offsets}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{offsets()}{offsets()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+offsets (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Returns a list of [start, end) character offsets of each token.
\end{DoxyVerb}
 

Definition at line 63 of file tokenizer.\+py.


\begin{DoxyCode}
63     \textcolor{keyword}{def }offsets(self):
64         \textcolor{stringliteral}{"""}
65 \textcolor{stringliteral}{        Returns a list of [start, end) character offsets of each token.}
66 \textcolor{stringliteral}{        """}
67         \textcolor{keywordflow}{return} [t[self.SPAN] \textcolor{keywordflow}{for} t \textcolor{keywordflow}{in} self.data]
68 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a5aa226b8190b5e72dbe0515caf0a21d6}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a5aa226b8190b5e72dbe0515caf0a21d6}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!pos@{pos}}
\index{pos@{pos}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{pos()}{pos()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+pos (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Returns a list of part-of-speech tags of each token.

Returns None if this annotation was not included.
\end{DoxyVerb}
 

Definition at line 69 of file tokenizer.\+py.


\begin{DoxyCode}
69     \textcolor{keyword}{def }pos(self):
70         \textcolor{stringliteral}{"""}
71 \textcolor{stringliteral}{        Returns a list of part-of-speech tags of each token.}
72 \textcolor{stringliteral}{}
73 \textcolor{stringliteral}{        Returns None if this annotation was not included.}
74 \textcolor{stringliteral}{        """}
75         \textcolor{keywordflow}{if} \textcolor{stringliteral}{'pos'} \textcolor{keywordflow}{not} \textcolor{keywordflow}{in} self.annotators:
76             \textcolor{keywordflow}{return} \textcolor{keywordtype}{None}
77         \textcolor{keywordflow}{return} [t[self.POS] \textcolor{keywordflow}{for} t \textcolor{keywordflow}{in} self.data]
78 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a8e68a443796116164126c7216f5963d3}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a8e68a443796116164126c7216f5963d3}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!slice@{slice}}
\index{slice@{slice}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{slice()}{slice()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+slice (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{i = {\ttfamily None},  }\item[{}]{j = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Return a view of the list of tokens from [i, j).
\end{DoxyVerb}
 

Definition at line 37 of file tokenizer.\+py.


\begin{DoxyCode}
37     \textcolor{keyword}{def }slice(self, i=None, j=None):
38         \textcolor{stringliteral}{"""}
39 \textcolor{stringliteral}{        Return a view of the list of tokens from [i, j).}
40 \textcolor{stringliteral}{        """}
41         new\_tokens = copy.copy(self)
42         new\_tokens.data = self.data[i:j]
43         \textcolor{keywordflow}{return} new\_tokens
44 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a1bea40b1ea8e1772fb8ae6c2157ee819}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a1bea40b1ea8e1772fb8ae6c2157ee819}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!untokenize@{untokenize}}
\index{untokenize@{untokenize}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{untokenize()}{untokenize()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+untokenize (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Returns the original text (with whitespace reinserted).
\end{DoxyVerb}
 

Definition at line 45 of file tokenizer.\+py.


\begin{DoxyCode}
45     \textcolor{keyword}{def }untokenize(self):
46         \textcolor{stringliteral}{"""}
47 \textcolor{stringliteral}{        Returns the original text (with whitespace reinserted).}
48 \textcolor{stringliteral}{        """}
49         \textcolor{keywordflow}{return} \textcolor{stringliteral}{''}.join([t[self.TEXT\_WS] \textcolor{keywordflow}{for} t \textcolor{keywordflow}{in} self.data]).strip()
50 
\end{DoxyCode}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a81b6d47a051c87c7f885a6f7db5ac891}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a81b6d47a051c87c7f885a6f7db5ac891}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!words@{words}}
\index{words@{words}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{words()}{words()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+words (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{uncased = {\ttfamily False} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Returns a list of the text of each token.

Args:
    uncased: lower cases text
\end{DoxyVerb}
 

Definition at line 51 of file tokenizer.\+py.


\begin{DoxyCode}
51     \textcolor{keyword}{def }words(self, uncased=False):
52         \textcolor{stringliteral}{"""}
53 \textcolor{stringliteral}{        Returns a list of the text of each token.}
54 \textcolor{stringliteral}{}
55 \textcolor{stringliteral}{        Args:}
56 \textcolor{stringliteral}{            uncased: lower cases text}
57 \textcolor{stringliteral}{        """}
58         \textcolor{keywordflow}{if} uncased:
59             \textcolor{keywordflow}{return} [t[self.TEXT].lower() \textcolor{keywordflow}{for} t \textcolor{keywordflow}{in} self.data]
60         \textcolor{keywordflow}{else}:
61             \textcolor{keywordflow}{return} [t[self.TEXT] \textcolor{keywordflow}{for} t \textcolor{keywordflow}{in} self.data]
62 
\end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a394834203475385444a933440dcdd026}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a394834203475385444a933440dcdd026}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!annotators@{annotators}}
\index{annotators@{annotators}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{annotators}{annotators}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+annotators}



Definition at line 28 of file tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ad5068b92d72245d43fd75dbb4c070a27}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ad5068b92d72245d43fd75dbb4c070a27}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!data@{data}}
\index{data@{data}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{data}{data}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+data}



Definition at line 27 of file tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ae82009092f014a3458777aeb5427ccf4}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ae82009092f014a3458777aeb5427ccf4}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!L\+E\+M\+MA@{L\+E\+M\+MA}}
\index{L\+E\+M\+MA@{L\+E\+M\+MA}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{L\+E\+M\+MA}{LEMMA}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+L\+E\+M\+MA\hspace{0.3cm}{\ttfamily [static]}}



Definition at line 23 of file tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a730dffdd2e5c859fb396c707830a6633}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a730dffdd2e5c859fb396c707830a6633}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!N\+ER@{N\+ER}}
\index{N\+ER@{N\+ER}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{N\+ER}{NER}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+N\+ER\hspace{0.3cm}{\ttfamily [static]}}



Definition at line 24 of file tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ada7339fd7da0200a4260970771f9bfe9}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_ada7339fd7da0200a4260970771f9bfe9}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!opts@{opts}}
\index{opts@{opts}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{opts}{opts}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+opts}



Definition at line 29 of file tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a10bd510e7cf4ee6c633ce7fec8aadbaf}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a10bd510e7cf4ee6c633ce7fec8aadbaf}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!P\+OS@{P\+OS}}
\index{P\+OS@{P\+OS}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{P\+OS}{POS}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+P\+OS\hspace{0.3cm}{\ttfamily [static]}}



Definition at line 22 of file tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a53956921f62121488f2ff918f0692bf9}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a53956921f62121488f2ff918f0692bf9}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!S\+P\+AN@{S\+P\+AN}}
\index{S\+P\+AN@{S\+P\+AN}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{S\+P\+AN}{SPAN}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+S\+P\+AN\hspace{0.3cm}{\ttfamily [static]}}



Definition at line 21 of file tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a95137955c2cf11d2d28434ece03adf7a}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a95137955c2cf11d2d28434ece03adf7a}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!T\+E\+XT@{T\+E\+XT}}
\index{T\+E\+XT@{T\+E\+XT}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{T\+E\+XT}{TEXT}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+T\+E\+XT\hspace{0.3cm}{\ttfamily [static]}}



Definition at line 19 of file tokenizer.\+py.

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a319701df20f2f209c615adbab8c445f4}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1tokenizer_1_1Tokens_a319701df20f2f209c615adbab8c445f4}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}!T\+E\+X\+T\+\_\+\+WS@{T\+E\+X\+T\+\_\+\+WS}}
\index{T\+E\+X\+T\+\_\+\+WS@{T\+E\+X\+T\+\_\+\+WS}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::tokenizer\+::\+Tokens}}
\subsubsection{\texorpdfstring{T\+E\+X\+T\+\_\+\+WS}{TEXT\_WS}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+T\+E\+X\+T\+\_\+\+WS\hspace{0.3cm}{\ttfamily [static]}}



Definition at line 20 of file tokenizer.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/tfidf\+\_\+retriever/tokenizers/\hyperlink{tokenizer_8py}{tokenizer.\+py}\end{DoxyCompactItemize}
