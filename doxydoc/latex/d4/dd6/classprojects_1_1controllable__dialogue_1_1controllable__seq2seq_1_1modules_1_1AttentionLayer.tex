\hypertarget{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer}{}\section{projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer Class Reference}
\label{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer}\index{projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer@{projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer}}


Inheritance diagram for projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=244pt]{d7/dc9/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=244pt]{d7/dcf/classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_ae4cb3ab12da870aa7fda064ea2f9a082}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, attn\+\_\+type, hiddensize, embeddingsize, bidirectional=False, attn\+\_\+length=-\/1, attn\+\_\+time=\textquotesingle{}pre\textquotesingle{})
\item 
def \hyperlink{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a18edcb2fb56e109d42db47ba411811b9}{forward} (self, xes, hidden, attn\+\_\+params)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_afe3902f442288aaef94b75b15b6547b7}{attention}
\item 
\hyperlink{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a93efcb1437c5bbf7e8f99c843a6ed086}{attn\+\_\+combine}
\item 
\hyperlink{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a4e117b37868e62d4348fdead83f6b5cc}{max\+\_\+length}
\item 
\hyperlink{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a0cedce597de9cf52d627f566998f6f85}{attn}
\item 
\hyperlink{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_ab1fefce0c25d815745a8d6c5bfd98b18}{attn\+\_\+v}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Computes attention between hidden and encoder states.

See arxiv.org/abs/1508.04025 for more info on each attention type.
\end{DoxyVerb}
 

Definition at line 815 of file modules.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_ae4cb3ab12da870aa7fda064ea2f9a082}\label{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_ae4cb3ab12da870aa7fda064ea2f9a082}} 
\index{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{attn\+\_\+type,  }\item[{}]{hiddensize,  }\item[{}]{embeddingsize,  }\item[{}]{bidirectional = {\ttfamily False},  }\item[{}]{attn\+\_\+length = {\ttfamily -\/1},  }\item[{}]{attn\+\_\+time = {\ttfamily \textquotesingle{}pre\textquotesingle{}} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Initialize attention layer.
\end{DoxyVerb}
 

Definition at line 830 of file modules.\+py.


\begin{DoxyCode}
830     ):
831         \textcolor{stringliteral}{"""}
832 \textcolor{stringliteral}{        Initialize attention layer.}
833 \textcolor{stringliteral}{        """}
834         super().\_\_init\_\_()
835         self.attention = attn\_type
836 
837         \textcolor{keywordflow}{if} self.attention != \textcolor{stringliteral}{'none'}:
838             hsz = hiddensize
839             hszXdirs = hsz * (2 \textcolor{keywordflow}{if} bidirectional \textcolor{keywordflow}{else} 1)
840             \textcolor{keywordflow}{if} attn\_time == \textcolor{stringliteral}{'pre'}:
841                 \textcolor{comment}{# attention happens on the input embeddings}
842                 input\_dim = embeddingsize
843             \textcolor{keywordflow}{elif} attn\_time == \textcolor{stringliteral}{'post'}:
844                 \textcolor{comment}{# attention happens on the output of the rnn}
845                 input\_dim = hsz
846             \textcolor{keywordflow}{else}:
847                 \textcolor{keywordflow}{raise} RuntimeError(\textcolor{stringliteral}{'unsupported attention time'})
848 
849             \textcolor{comment}{# linear layer for combining applied attention weights with input}
850             self.attn\_combine = nn.Linear(hszXdirs + input\_dim, input\_dim, bias=\textcolor{keyword}{False})
851 
852             \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'local'}:
853                 \textcolor{comment}{# local attention over fixed set of output states}
854                 \textcolor{keywordflow}{if} attn\_length < 0:
855                     \textcolor{keywordflow}{raise} RuntimeError(\textcolor{stringliteral}{'Set attention length to > 0.'})
856                 self.max\_length = attn\_length
857                 \textcolor{comment}{# combines input and previous hidden output layer}
858                 self.attn = nn.Linear(hsz + input\_dim, attn\_length, bias=\textcolor{keyword}{False})
859                 \textcolor{comment}{# combines attention weights with encoder outputs}
860             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'concat'}:
861                 self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=\textcolor{keyword}{False})
862                 self.attn\_v = nn.Linear(hsz, 1, bias=\textcolor{keyword}{False})
863             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'general'}:
864                 \textcolor{comment}{# equivalent to dot if attn is identity}
865                 self.attn = nn.Linear(hsz, hszXdirs, bias=\textcolor{keyword}{False})
866 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a18edcb2fb56e109d42db47ba411811b9}\label{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a18edcb2fb56e109d42db47ba411811b9}} 
\index{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}!forward@{forward}}
\index{forward@{forward}!projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{xes,  }\item[{}]{hidden,  }\item[{}]{attn\+\_\+params }\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute attention over attn_params given input and hidden states.

:param xes:         input state. will be combined with applied
            attention.
:param hidden:      hidden state from model. will be used to select
            states to attend to in from the attn_params.
:param attn_params: tuple of encoder output states and a mask showing
            which input indices are nonzero.

:returns: output, attn_weights
  output is a new state of same size as input state `xes`.
  attn_weights are the weights given to each state in the
  encoder outputs.
\end{DoxyVerb}
 

Definition at line 867 of file modules.\+py.


\begin{DoxyCode}
867     \textcolor{keyword}{def }forward(self, xes, hidden, attn\_params):
868         \textcolor{stringliteral}{"""}
869 \textcolor{stringliteral}{        Compute attention over attn\_params given input and hidden states.}
870 \textcolor{stringliteral}{}
871 \textcolor{stringliteral}{        :param xes:         input state. will be combined with applied}
872 \textcolor{stringliteral}{                            attention.}
873 \textcolor{stringliteral}{        :param hidden:      hidden state from model. will be used to select}
874 \textcolor{stringliteral}{                            states to attend to in from the attn\_params.}
875 \textcolor{stringliteral}{        :param attn\_params: tuple of encoder output states and a mask showing}
876 \textcolor{stringliteral}{                            which input indices are nonzero.}
877 \textcolor{stringliteral}{}
878 \textcolor{stringliteral}{        :returns: output, attn\_weights}
879 \textcolor{stringliteral}{                  output is a new state of same size as input state `xes`.}
880 \textcolor{stringliteral}{                  attn\_weights are the weights given to each state in the}
881 \textcolor{stringliteral}{                  encoder outputs.}
882 \textcolor{stringliteral}{        """}
883         \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'none'}:
884             \textcolor{comment}{# do nothing, no attention}
885             \textcolor{keywordflow}{return} xes, \textcolor{keywordtype}{None}
886 
887         \textcolor{keywordflow}{if} \hyperlink{namespaceparlai_1_1agents_1_1tfidf__retriever_1_1build__tfidf_ad5dfae268e23f506da084a9efb72f619}{type}(hidden) == tuple:
888             \textcolor{comment}{# for lstms use the "hidden" state not the cell state}
889             hidden = hidden[0]
890         last\_hidden = hidden[-1]  \textcolor{comment}{# select hidden state from last RNN layer}
891 
892         enc\_out, attn\_mask = attn\_params
893         bsz, seqlen, hszXnumdir = enc\_out.size()
894         numlayersXnumdir = last\_hidden.size(1)
895 
896         \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'local'}:
897             \textcolor{comment}{# local attention weights aren't based on encoder states}
898             h\_merged = torch.cat((xes.squeeze(1), last\_hidden), 1)
899             attn\_weights = F.softmax(self.attn(h\_merged), dim=1)
900 
901             \textcolor{comment}{# adjust state sizes to the fixed window size}
902             \textcolor{keywordflow}{if} seqlen > self.max\_length:
903                 offset = seqlen - self.max\_length
904                 enc\_out = enc\_out.narrow(1, offset, self.max\_length)
905                 seqlen = self.max\_length
906             \textcolor{keywordflow}{if} attn\_weights.size(1) > seqlen:
907                 attn\_weights = attn\_weights.narrow(1, 0, seqlen)
908         \textcolor{keywordflow}{else}:
909             hid = last\_hidden.unsqueeze(1)
910             \textcolor{keywordflow}{if} self.attention == \textcolor{stringliteral}{'concat'}:
911                 \textcolor{comment}{# concat hidden state and encoder outputs}
912                 hid = hid.expand(bsz, seqlen, numlayersXnumdir)
913                 h\_merged = torch.cat((enc\_out, hid), 2)
914                 \textcolor{comment}{# then do linear combination of them with activation}
915                 active = F.tanh(self.attn(h\_merged))
916                 attn\_w\_premask = self.attn\_v(active).squeeze(2)
917             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'dot'}:
918                 \textcolor{comment}{# dot product between hidden and encoder outputs}
919                 \textcolor{keywordflow}{if} numlayersXnumdir != hszXnumdir:
920                     \textcolor{comment}{# enc\_out has two directions, so double hid}
921                     hid = torch.cat([hid, hid], 2)
922                 enc\_t = enc\_out.transpose(1, 2)
923                 attn\_w\_premask = torch.bmm(hid, enc\_t).squeeze(1)
924             \textcolor{keywordflow}{elif} self.attention == \textcolor{stringliteral}{'general'}:
925                 \textcolor{comment}{# before doing dot product, transform hidden state with linear}
926                 \textcolor{comment}{# same as dot if linear is identity}
927                 hid = self.attn(hid)
928                 enc\_t = enc\_out.transpose(1, 2)
929                 attn\_w\_premask = torch.bmm(hid, enc\_t).squeeze(1)
930 
931             \textcolor{comment}{# calculate activation scores, apply mask if needed}
932             \textcolor{keywordflow}{if} attn\_mask \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
933                 \textcolor{comment}{# remove activation from NULL symbols}
934                 attn\_w\_premask.masked\_fill\_(~attn\_mask, -NEAR\_INF)
935             attn\_weights = F.softmax(attn\_w\_premask, dim=1)
936 
937         \textcolor{comment}{# apply the attention weights to the encoder states}
938         attn\_applied = torch.bmm(attn\_weights.unsqueeze(1), enc\_out)
939         \textcolor{comment}{# concatenate the input and encoder states}
940         merged = torch.cat((xes.squeeze(1), attn\_applied.squeeze(1)), 1)
941         \textcolor{comment}{# combine them with a linear layer and tanh activation}
942         output = torch.tanh(self.attn\_combine(merged).unsqueeze(1))
943 
944         \textcolor{keywordflow}{return} output, attn\_weights
945 \end{DoxyCode}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_afe3902f442288aaef94b75b15b6547b7}\label{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_afe3902f442288aaef94b75b15b6547b7}} 
\index{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}!attention@{attention}}
\index{attention@{attention}!projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attention}{attention}}
{\footnotesize\ttfamily projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+attention}



Definition at line 835 of file modules.\+py.

\mbox{\Hypertarget{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a0cedce597de9cf52d627f566998f6f85}\label{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a0cedce597de9cf52d627f566998f6f85}} 
\index{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}!attn@{attn}}
\index{attn@{attn}!projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn}{attn}}
{\footnotesize\ttfamily projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+attn}



Definition at line 858 of file modules.\+py.

\mbox{\Hypertarget{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a93efcb1437c5bbf7e8f99c843a6ed086}\label{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a93efcb1437c5bbf7e8f99c843a6ed086}} 
\index{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}!attn\+\_\+combine@{attn\+\_\+combine}}
\index{attn\+\_\+combine@{attn\+\_\+combine}!projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn\+\_\+combine}{attn\_combine}}
{\footnotesize\ttfamily projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+attn\+\_\+combine}



Definition at line 850 of file modules.\+py.

\mbox{\Hypertarget{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_ab1fefce0c25d815745a8d6c5bfd98b18}\label{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_ab1fefce0c25d815745a8d6c5bfd98b18}} 
\index{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}!attn\+\_\+v@{attn\+\_\+v}}
\index{attn\+\_\+v@{attn\+\_\+v}!projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn\+\_\+v}{attn\_v}}
{\footnotesize\ttfamily projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+attn\+\_\+v}



Definition at line 862 of file modules.\+py.

\mbox{\Hypertarget{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a4e117b37868e62d4348fdead83f6b5cc}\label{classprojects_1_1controllable__dialogue_1_1controllable__seq2seq_1_1modules_1_1AttentionLayer_a4e117b37868e62d4348fdead83f6b5cc}} 
\index{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}!max\+\_\+length@{max\+\_\+length}}
\index{max\+\_\+length@{max\+\_\+length}!projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer@{projects\+::controllable\+\_\+dialogue\+::controllable\+\_\+seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{max\+\_\+length}{max\_length}}
{\footnotesize\ttfamily projects.\+controllable\+\_\+dialogue.\+controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+max\+\_\+length}



Definition at line 856 of file modules.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
projects/controllable\+\_\+dialogue/controllable\+\_\+seq2seq/\hyperlink{projects_2controllable__dialogue_2controllable__seq2seq_2modules_8py}{modules.\+py}\end{DoxyCompactItemize}
