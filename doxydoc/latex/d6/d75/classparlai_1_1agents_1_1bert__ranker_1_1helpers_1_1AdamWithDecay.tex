\hypertarget{classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay}{}\section{parlai.\+agents.\+bert\+\_\+ranker.\+helpers.\+Adam\+With\+Decay Class Reference}
\label{classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay}\index{parlai.\+agents.\+bert\+\_\+ranker.\+helpers.\+Adam\+With\+Decay@{parlai.\+agents.\+bert\+\_\+ranker.\+helpers.\+Adam\+With\+Decay}}


Inheritance diagram for parlai.\+agents.\+bert\+\_\+ranker.\+helpers.\+Adam\+With\+Decay\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=241pt]{d1/d2f/classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+bert\+\_\+ranker.\+helpers.\+Adam\+With\+Decay\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=241pt]{d2/da4/classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay_a341310a9a32859bdbdf4ca8bce7bb239}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, params, lr, b1=0.\+9, b2=0.\+999, e=1e-\/6, weight\+\_\+decay=0.\+01, max\+\_\+grad\+\_\+norm=1.\+0)
\item 
def \hyperlink{classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay_aeb41188377c1a61b4638228abd6c3e64}{step} (self, closure=None)
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Adam with decay; mirror's HF's implementation.

:param lr:
    learning rate
:param b1:
    Adams b1. Default: 0.9
:param b2:
    Adams b2. Default: 0.999
:param e:
    Adams epsilon. Default: 1e-6
:param weight_decay:
    Weight decay. Default: 0.01
:param max_grad_norm:
    Maximum norm for the gradients (-1 means no clipping).  Default: 1.0
\end{DoxyVerb}
 

Definition at line 242 of file helpers.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay_a341310a9a32859bdbdf4ca8bce7bb239}\label{classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay_a341310a9a32859bdbdf4ca8bce7bb239}} 
\index{parlai\+::agents\+::bert\+\_\+ranker\+::helpers\+::\+Adam\+With\+Decay@{parlai\+::agents\+::bert\+\_\+ranker\+::helpers\+::\+Adam\+With\+Decay}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::bert\+\_\+ranker\+::helpers\+::\+Adam\+With\+Decay@{parlai\+::agents\+::bert\+\_\+ranker\+::helpers\+::\+Adam\+With\+Decay}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+bert\+\_\+ranker.\+helpers.\+Adam\+With\+Decay.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{params,  }\item[{}]{lr,  }\item[{}]{b1 = {\ttfamily 0.9},  }\item[{}]{b2 = {\ttfamily 0.999},  }\item[{}]{e = {\ttfamily 1e-\/6},  }\item[{}]{weight\+\_\+decay = {\ttfamily 0.01},  }\item[{}]{max\+\_\+grad\+\_\+norm = {\ttfamily 1.0} }\end{DoxyParamCaption})}



Definition at line 262 of file helpers.\+py.


\begin{DoxyCode}
262     ):
263         \textcolor{keywordflow}{if} lr < 0.0:
264             \textcolor{keywordflow}{raise} ValueError(\textcolor{stringliteral}{'Invalid learning rate: \{\} - should be >= 0.0'}.
      \hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(lr))
265         \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} 0.0 <= b1 < 1.0:
266             \textcolor{keywordflow}{raise} ValueError(
267                 \textcolor{stringliteral}{'Invalid b1 parameter: \{\} - should be in [0.0, 1.0['}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(b1)
268             )
269         \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} 0.0 <= b2 < 1.0:
270             \textcolor{keywordflow}{raise} ValueError(
271                 \textcolor{stringliteral}{'Invalid b2 parameter: \{\} - should be in [0.0, 1.0['}.\hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(b2)
272             )
273         \textcolor{keywordflow}{if} \textcolor{keywordflow}{not} e >= 0.0:
274             \textcolor{keywordflow}{raise} ValueError(\textcolor{stringliteral}{'Invalid epsilon value: \{\} - should be >= 0.0'}.
      \hyperlink{namespaceparlai_1_1chat__service_1_1services_1_1messenger_1_1shared__utils_a32e2e2022b824fbaf80c747160b52a76}{format}(e))
275         defaults = dict(
276             lr=lr,
277             b1=b1,
278             b2=b2,
279             e=e,
280             weight\_decay=weight\_decay,
281             max\_grad\_norm=max\_grad\_norm,
282         )
283         super(AdamWithDecay, self).\_\_init\_\_(params, defaults)
284 
\end{DoxyCode}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay_aeb41188377c1a61b4638228abd6c3e64}\label{classparlai_1_1agents_1_1bert__ranker_1_1helpers_1_1AdamWithDecay_aeb41188377c1a61b4638228abd6c3e64}} 
\index{parlai\+::agents\+::bert\+\_\+ranker\+::helpers\+::\+Adam\+With\+Decay@{parlai\+::agents\+::bert\+\_\+ranker\+::helpers\+::\+Adam\+With\+Decay}!step@{step}}
\index{step@{step}!parlai\+::agents\+::bert\+\_\+ranker\+::helpers\+::\+Adam\+With\+Decay@{parlai\+::agents\+::bert\+\_\+ranker\+::helpers\+::\+Adam\+With\+Decay}}
\subsubsection{\texorpdfstring{step()}{step()}}
{\footnotesize\ttfamily def parlai.\+agents.\+bert\+\_\+ranker.\+helpers.\+Adam\+With\+Decay.\+step (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{closure = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Perform a single optimization step.

:param closure:
    A closure that reevaluates the model and returns the loss.
\end{DoxyVerb}
 

Definition at line 285 of file helpers.\+py.


\begin{DoxyCode}
285     \textcolor{keyword}{def }step(self, closure=None):
286         \textcolor{stringliteral}{"""}
287 \textcolor{stringliteral}{        Perform a single optimization step.}
288 \textcolor{stringliteral}{}
289 \textcolor{stringliteral}{        :param closure:}
290 \textcolor{stringliteral}{            A closure that reevaluates the model and returns the loss.}
291 \textcolor{stringliteral}{        """}
292         loss = \textcolor{keywordtype}{None}
293         \textcolor{keywordflow}{if} closure \textcolor{keywordflow}{is} \textcolor{keywordflow}{not} \textcolor{keywordtype}{None}:
294             loss = closure()
295 
296         \textcolor{keywordflow}{for} group \textcolor{keywordflow}{in} self.param\_groups:
297             \textcolor{keywordflow}{for} p \textcolor{keywordflow}{in} group[\textcolor{stringliteral}{'params'}]:
298                 \textcolor{keywordflow}{if} p.grad \textcolor{keywordflow}{is} \textcolor{keywordtype}{None}:
299                     \textcolor{keywordflow}{continue}
300                 grad = p.grad.data
301                 \textcolor{keywordflow}{if} grad.is\_sparse:
302                     \textcolor{keywordflow}{raise} RuntimeError(
303                         \textcolor{stringliteral}{'Adam does not support sparse gradients, please '}
304                         \textcolor{stringliteral}{'consider SparseAdam instead'}
305                     )
306 
307                 state = self.state[p]
308 
309                 \textcolor{comment}{# State initialization}
310                 \textcolor{keywordflow}{if} len(state) == 0:
311                     \textcolor{comment}{# Exponential moving average of gradient values}
312                     state[\textcolor{stringliteral}{'next\_m'}] = torch.zeros\_like(p.data)
313                     \textcolor{comment}{# Exponential moving average of squared gradient values}
314                     state[\textcolor{stringliteral}{'next\_v'}] = torch.zeros\_like(p.data)
315 
316                 next\_m, next\_v = state[\textcolor{stringliteral}{'next\_m'}], state[\textcolor{stringliteral}{'next\_v'}]
317                 beta1, beta2 = group[\textcolor{stringliteral}{'b1'}], group[\textcolor{stringliteral}{'b2'}]
318 
319                 \textcolor{comment}{# Add grad clipping}
320                 \textcolor{keywordflow}{if} group[\textcolor{stringliteral}{'max\_grad\_norm'}] > 0:
321                     clip\_grad\_norm\_(p, group[\textcolor{stringliteral}{'max\_grad\_norm'}])
322 
323                 \textcolor{comment}{# Decay the first and second moment running average coefficient}
324                 \textcolor{comment}{# In-place operations to update the averages at the same time}
325                 next\_m.mul\_(beta1).add\_(1 - beta1, grad)
326                 next\_v.mul\_(beta2).addcmul\_(1 - beta2, grad, grad)
327                 update = next\_m / (next\_v.sqrt() + group[\textcolor{stringliteral}{'e'}])
328 
329                 \textcolor{comment}{# Just adding the square of the weights to the loss function is *not*}
330                 \textcolor{comment}{# the correct way of using L2 regularization/weight decay with Adam,}
331                 \textcolor{comment}{# since that will interact with the m and v parameters in strange ways.}
332                 \textcolor{comment}{#}
333                 \textcolor{comment}{# Instead we want to decay the weights in a manner that doesn't interact}
334                 \textcolor{comment}{# with the m/v parameters. This is equivalent to adding the square}
335                 \textcolor{comment}{# of the weights to the loss with plain (non-momentum) SGD.}
336                 \textcolor{keywordflow}{if} group[\textcolor{stringliteral}{'weight\_decay'}] > 0.0:
337                     update += group[\textcolor{stringliteral}{'weight\_decay'}] * p.data
338                 lr = group[\textcolor{stringliteral}{'lr'}]
339 
340                 update\_with\_lr = lr * update
341                 p.data.add\_(-update\_with\_lr)
342         \textcolor{keywordflow}{return} loss
343 \end{DoxyCode}


The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/bert\+\_\+ranker/\hyperlink{helpers_8py}{helpers.\+py}\end{DoxyCompactItemize}
